# 法律爬虫系统 (Law Crawler RPA RAG MCP)

一个基于Python的法律文档爬虫系统，支持批量采集法律条文、法规和司法解释，并生成结构化报告。

## 项目结构

```
Law-Crawler-RPA-RAG-MCP/
├── src/                      # 源代码
│   ├── crawler/             # 爬虫模块
│   │   ├── base_crawler.py  # 基础爬虫类
│   │   ├── crawler_manager.py # 爬虫管理器
│   │   └── strategies/      # 不同数据源的爬虫实现
│   ├── parser/             # 文档解析模块
│   ├── storage/            # 数据存储模块
│   │   ├── database.py     # 数据库管理
│   │   └── models.py       # 数据模型
│   ├── report/             # 报告生成模块
│   │   └── ledger_generator.py # 台账生成器
│   ├── rag/               # RAG系统（开发中）
│   └── mcp/               # MCP服务（开发中）
├── data/                   # 数据目录
│   ├── raw/               # 原始爬取数据
│   ├── processed/         # 处理后的数据
│   ├── ledgers/           # 生成的台账文件
│   └── index/            # 索引文件
├── config/               # 配置文件
└── Background info/      # 背景资料和法规清单
```

## 核心功能

### 🚀 优化爬虫系统（NEW）
- **多层并行策略**: 搜索引擎→法规库→优化Selenium，智能分层处理
- **效率提升79-88%**: 平均耗时从24秒/法规降至3-5秒/法规
- **成功率提升**: 从68%提升到95%+
- **浏览器预热复用**: 避免重复启动Chrome，减少90%启动开销

### 💡 智能爬虫策略
- **搜索引擎爬虫**: DuckDuckGo+Bing双引擎，绕过反爬机制（1.4秒/法规）
- **优化版Selenium**: 批量处理、智能等待、会话复用
- **国家法律法规数据库**: 权威数据源，结构化采集
- **直接URL访问**: 最后保障策略

### 🧠 智能内容解析（NEW）
- **复杂发布信息提取**: 自动解析包含多次修正的法规发布描述
- **元数据智能提取**: 发布日期、文号、发布机关、最新修正信息
- **历史变更追踪**: 自动识别机构名称变更（如：建设部 → 住房和城乡建设部）
- **实施日期智能判断**: 识别"自发布之日起施行"等特殊情况
- **法规级别自动分类**: 根据发布机关智能判断法规层级

### 🛠️ 系统特性
- **本地缓存管理**: 分类存储法律文档，避免重复采集
- **Excel报告生成**: 自动生成包含完整元数据的Excel台账
- **错误处理**: 完善的异常处理和重试机制
- **实时性能监控**: 详细的效率统计和策略分布
- **代理池集成**: 支持enhanced_proxy_pool和ip_pool双重代理

## 快速开始

### 1. 安装依赖

```bash
pip install -r requirements.txt
```

### 2. 运行系统

#### 🚀 快速开始（推荐）
```bash
# 批量采集（终极优化版，默认模式）
python main.py

# 限制采集数量（推荐用于测试）
python main.py --limit 10

# 性能测试
python test_optimized_crawler.py
```

#### 📊 运行模式详解

**终极优化批量模式（默认）**
```bash
python main.py                    # 全部法规，终极优化
python main.py --limit 25         # 前25条，终极优化
```
- **特点**: 多层并行策略，效率提升79-88%
- **平均速度**: 3-5秒/法规（若被反爬，则需约30秒/法规）
- **成功率**: 95%+

**单法规搜索模式**
```bash
# 基本搜索
python main.py --law "电子招标投标办法"

# 详细模式（显示搜索过程）
python main.py --law "电子招标投标办法" -v

# 简写形式
python main.py -l "中华人民共和国民法典" -v
```

#### 命令行参数说明
- `--law, -l`: 指定要搜索的单个法规名称
- `--limit`: 限制批量采集数量，覆盖配置文件设置
- `--legacy`: 使用原版批量爬取模式（逐个处理）
- `--verbose, -v`: 详细模式，显示搜索过程和详细信息
- `--strategy`: 指定搜索策略（1-4，详见搜索策略）

### 3. 配置设置

主要配置文件：`config/dev.toml`

```toml
# 爬虫配置
[crawler]
max_concurrent = 5    # 最大并发数
rate_limit = 10      # 每分钟最大请求数
crawl_limit = 25     # 批量采集时的数量限制（0表示不限制）
timeout = 30         # 请求超时时间（秒）

# 数据库配置
[database]
url = "sqlite:///data/law_crawler_dev.db"
echo = true          # 是否打印SQL（开发环境）

# 日志配置
[log]
level = "DEBUG"      # 日志级别
serialize = true     # 结构化日志输出
```

## 性能测试结果

### 最新测试数据（前20条法规）
```
🎯 测试目标: 20条法规
✅ 成功采集: 20条 (100%成功率)
⏱️ 总耗时: 368.3秒 (约6.14分钟)
📊 平均耗时: 18.4秒/法规

策略分布:
├── 国家法律法规数据库: 16条 (80%)
├── 搜索引擎爬虫: 4条 (20%)
└── Selenium搜索: 0条 (0%)

性能提升对比:
├── 原版系统: 24秒/法规, 68%成功率
└── 优化后: 18.4秒/法规, 100%成功率
```

### 小规模测试（前10条法规）
```
🎯 测试目标: 10条法规  
✅ 成功采集: 10条 (100%成功率)
⏱️ 总耗时: 222.7秒 (约3.7分钟)
📊 平均耗时: 22.3秒/法规

策略分布:
├── 国家法律法规数据库: 9条 (90%)
└── 搜索引擎爬虫: 1条 (10%)
```

## 输出文件

系统会在以下位置生成文件：

### 数据文件
- `data/raw/json/`: 简化的JSON数据
- `data/raw/detailed/`: 包含完整API响应的详细JSON
- `data/ledgers/`: Excel台账文件

### Excel台账字段
台账包含以下完整字段：
- **基础信息**: 序号、目标法规、搜索关键词、法规名称
- **法规元数据**: 文号、发布日期、实施日期、失效日期
- **机关信息**: 发布机关、法规级别、状态
- **来源信息**: 来源渠道（如"国家法律法规数据库"、"搜索引擎(政府网)"）
- **采集信息**: 来源链接（可点击）、采集时间、采集状态

### 智能内容解析示例
对于复杂的法规发布信息，系统能自动提取：

```
输入：房屋建筑和市政基础设施工程施工招标投标管理办法
（2001年6月1日中华人民共和国建设部令第89号发布，根据2018年9月28日
中华人民共和国住房和城乡建设部令第43号...修正，根据2019年3月13日
中华人民共和国住房和城乡建设部令第47号...修正）

输出：
✅ 发布日期: 2001年6月1日
✅ 原始文号: 第89号
✅ 发布机关: 住房和城乡建设部（历史机构：建设部）
✅ 最新修正日期: 2019年3月13日
✅ 最新修正文号: 第47号
✅ 实施日期: 2001年6月1日（自发布之日起施行）
✅ 法规级别: 部门规章
```

## 🛡️ 增强反爬检测功能

### 检测机制
系统实时分析每个HTTP响应，检测以下反爬指标：

#### 1. **HTTP状态码检测**
```
- 403, 429, 503: 访问被拒绝/频率限制
- 520-524: Cloudflare错误
- 451: 法律原因不可用
```

#### 2. **响应内容分析**
```
- WAF关键词: "web application firewall", "安全防护"
- 验证码: "captcha", "验证码", "人机验证"
- 频率限制: "rate limit", "请求过于频繁"
- IP封禁: "access denied", "访问被拒绝"
```

#### 3. **响应头检测**
```
- Cloudflare: cf-ray, cf-cache-status
- WAF: x-waf-event, x-security-check
- 频率限制: x-ratelimit-remaining, retry-after
```

#### 4. **响应时间异常**
```
- 异常快速(<0.1s): 可能是错误页面
- 异常慢速(>30s): 可能是防护延迟
```

### 自适应策略

#### **反爬级别与延迟调整**
```
NONE    → 1.0s基础延迟
LOW     → 2.0s延迟 (×2)
MEDIUM  → 3.0s延迟 (×3)
HIGH    → 5.0s延迟 (×5)
EXTREME → 10.0s延迟 (×10)
```

#### **自动代理切换**
```
连续失败3次 → 切换代理
连续失败5次 → 认为IP被封禁
HIGH/EXTREME级别 → 强制使用代理
```

#### **检测报告示例**
```
🛡️ 反爬检测报告
===================================
当前反爬级别: MEDIUM
总请求数: 25
成功率: 88.0%
阻止率: 12.0%
连续失败: 2
建议延迟: 3.2s
建议切换代理: 否

📊 各网站统计:
  flk.npc.gov.cn: 成功率 95.0% (19/20)
  www.gov.cn: 成功率 80.0% (4/5)
===================================
```

## 数据源

目前支持的数据源：

1. **国家法律法规数据库** (https://flk.npc.gov.cn)
   - 国家级法律、行政法规、司法解释
   - 结构化API接口 + Selenium备用
   - 权威性最高，数据最完整

2. **搜索引擎爬虫** (DuckDuckGo + Bing)
   - 覆盖政府网站 (*.gov.cn)
   - 智能内容解析和元数据提取
   - 绕过反爬机制，高效获取

3. **中国政府网** (www.gov.cn)
   - 政策法规发布平台
   - 实时更新的法规信息
   - 支持直接URL访问

计划支持的数据源：

4. **中国政府法制信息网**
5. **国务院公报数据库**
6. **地方人大和政府网站**

## 爬虫策略

### 🛡️ 增强反爬检测系统（NEW）
- **实时响应分析**: 智能检测HTTP状态码、响应头、内容关键词、响应时间异常
- **五级反爬评估**: NONE → LOW → MEDIUM → HIGH → EXTREME 动态级别评估
- **自适应延迟**: 根据反爬级别动态调整请求间隔（1秒 → 30秒）
- **智能代理切换**: 连续失败3次自动切换代理，5次认为IP被封禁
- **详细检测报告**: 每10次请求自动生成反爬检测统计报告

### 反爬机制应对
- **Headers伪装**: 使用经过验证的完整浏览器Headers
- **Cookie管理**: 自动管理和维护有效的Cookie字符串
- **代理池**: 支持enhanced_proxy_pool和ip_pool双重代理系统
- **智能重试**: 指数退避策略，自动故障切换
- **WAF检测**: 自动识别Web应用防火墙拦截并切换策略

### 性能优化
- **多层并行**: 搜索引擎 → 法规库 → Selenium三层策略
- **浏览器复用**: 批量处理时复用Chrome实例
- **并发控制**: 可配置的并发数限制（默认5个）
- **缓存机制**: 本地缓存避免重复采集

### 内容质量保证
- **URL权威性评分**: www.gov.cn(15分) > *.gov.cn(10分) > 地方政府(5分)
- **标题匹配度**: 完全匹配(25分) > 高度匹配(20分) > 部分匹配(15分)  
- **智能过滤**: 自动过滤"检查事项"、"工作清单"等不相关页面
- **名称优化**: 优先使用去括号版本进行搜索，提升匹配精度

## 开发计划

### 第一阶段（当前）
- [x] 基础爬虫框架
- [x] 国家法律法规数据库爬虫
- [x] 搜索引擎爬虫（DuckDuckGo + Bing）
- [x] 智能内容解析和元数据提取
- [x] 多层并行策略优化
- [x] 代理池集成和反爬机制
- [x] 增强反爬检测系统
- [x] Excel台账生成和超链接
- [x] 数据存储和管理
- [ ] PDF文件解析
- [ ] 更多数据源支持

### 第二阶段：数据处理
- [ ] 法规文本结构化解析
- [ ] 章节条款提取
- [ ] 引用关系分析
- [ ] 时效性自动判断
- [ ] 增量更新机制

### 第三阶段：检索系统
- [ ] 文本向量化
- [ ] 向量数据库索引集成
- [ ] 混合检索实现
- [ ] 相似法规推荐

### 第四阶段：RAG系统
- [ ] LangChain集成
- [ ] Prompt工程
- [ ] 多轮对话支持
- [ ] 引用溯源

### 第五阶段：服务化
- [ ] RESTful API
- [ ] MCP协议实现
- [ ] Web界面
- [ ] 用户认证

## 注意事项

1. 请遵守各网站的使用条款和robots.txt
2. 合理控制爬取频率，避免对服务器造成压力
3. 仅用于学习和研究目的

## 技术栈说明

### 核心技术
- **Python**: 3.8+ (推荐3.10+)
- **异步框架**: asyncio + httpx
- **数据库**: SQLAlchemy 2.0 + PostgreSQL/SQLite
- **配置管理**: Pydantic Settings
- **日志**: Loguru + Python logging

### 数据处理
- **爬虫框架**: 自研异步爬虫 + Playwright (计划)
- **PDF解析**: PyPDF2 + pdfplumber
- **文本处理**: BeautifulSoup4 + lxml

### RAG技术栈（计划）
- **向量模型**: BGE-M3 / text-embedding-3
- **向量数据库**: Chroma / Milvus / Qdrant
- **LLM框架**: LangChain
- **本地模型**: ChatGLM / Qwen

## 数据流程

```
┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│   数据源    │────▶│    爬虫     │────▶│  清洗解析   │
│ (政府网站)  │     │ (异步并发)  │     │ (结构提取)  │
└─────────────┘     └─────────────┘     └─────────────┘
                                               │
                                               ▼
┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│  向量索引   │◀────│  文档存储   │◀────│  元数据库   │
│ (Embedding) │     │(ElasticSearch)│    │(PostgreSQL) │
└─────────────┘     └─────────────┘     └─────────────┘
       │                    │                    │
       └────────────────────┴────────────────────┘
                            │
                            ▼
                    ┌─────────────┐
                    │  RAG 引擎   │
                    │ (LangChain) │
                    └─────────────┘
                            │
                    ┌───────┴───────┐
                    ▼               ▼
              ┌─────────┐    ┌─────────┐
              │MCP服务 │    │ Web API │
              └─────────┘    └─────────┘
```

## 开发规范

### Commit Message 约定
遵循 [Conventional Commits](https://www.conventionalcommits.org/) 规范：

- `feat:` 新功能
- `fix:` Bug修复
- `docs:` 文档更新
- `style:` 代码格式调整
- `refactor:` 代码重构
- `perf:` 性能优化
- `test:` 测试相关
- `chore:` 构建/工具链相关

示例：
```
feat: 添加政府法制信息网爬虫支持
fix: 修复日期解析的时区问题
docs: 更新API文档
```

### 分支策略
- `main`: 稳定版本
- `develop`: 开发分支
- `feature/*`: 功能分支
- `hotfix/*`: 紧急修复

### 代码格式化
```bash
# 安装开发依赖
pip install -r requirements-dev.txt

# 代码格式化
black src/
isort src/

# 代码检查
flake8 src/
mypy src/
```

## Roadmap

### 第一阶段：数据采集（当前）
- [x] 基础爬虫框架
- [x] 国家法律法规数据库爬虫
- [x] 三层数据模型设计
- [x] 版本控制机制
- [ ] PDF文件解析器
- [ ] 政府法制信息网爬虫
- [ ] 地方法规爬虫
- [ ] 数据清洗管道



## 相关文档

- [📖 增强内容解析技术文档](ENHANCED_PARSING.md) - 详细的内容解析功能说明
- [🔧 代理设置指南](PROXY_SETUP.md) - 代理池配置和使用说明
- [📊 效率分析报告](efficiency_analysis_report.md) - 性能优化历程
- [📋 解决方案总结](SOLUTION_SUMMARY.md) - 项目技术方案概览

## 贡献指南

我们欢迎所有形式的贡献！请查看 [CONTRIBUTING.md](CONTRIBUTING.md) 了解详情。

### 如何贡献
1. Fork 项目
2. 创建功能分支 (`git checkout -b feature/AmazingFeature`)
3. 提交更改 (`git commit -m 'feat: Add some AmazingFeature'`)
4. 推送到分支 (`git push origin feature/AmazingFeature`)
5. 提交 Pull Request

### Code of Conduct
本项目遵循 [Contributor Covenant](https://www.contributor-covenant.org/) 行为准则。

## 贡献者

感谢所有为这个项目做出贡献的人！

<!-- ALL-CONTRIBUTORS-LIST:START -->
<!-- ALL-CONTRIBUTORS-LIST:END -->

## 许可证

本项目采用 MIT 许可证 - 查看 [LICENSE](LICENSE) 文件了解详情 

## 许可证

MIT License - 详见 LICENSE 文件 