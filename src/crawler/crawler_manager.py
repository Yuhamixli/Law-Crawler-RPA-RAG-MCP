#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
çˆ¬è™«ç®¡ç†å™¨
è´Ÿè´£åè°ƒä¸åŒçš„çˆ¬è™«ç­–ç•¥ï¼Œä¼˜åŒ–æ•ˆç‡
"""

import asyncio
import pandas as pd
import json
import hashlib
from pathlib import Path
from typing import List, Dict, Any, Optional
from loguru import logger
import sys
import os
import aiohttp
import time
from datetime import datetime

sys.path.append(os.path.dirname(os.path.dirname(__file__)))
from src.crawler.strategies.search_based_crawler import SearchBasedCrawler
from src.crawler.strategies.selenium_gov_crawler import SeleniumGovCrawler
from src.crawler.strategies.search_engine_crawler import SearchEngineCrawler
from src.crawler.strategies.selenium_search_crawler import SeleniumSearchCrawler
from src.crawler.strategies.direct_url_crawler import DirectUrlCrawler
from src.crawler.strategies.optimized_selenium_crawler import OptimizedSeleniumCrawler

from config.settings import settings


class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨ - å‚è€ƒexample project"""
    
    def __init__(self, cache_dir: str = "data/cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºåˆ†ç±»ç›®å½•
        self.categories = {
            "æ³•å¾‹": "laws",
            "è¡Œæ”¿æ³•è§„": "regulations", 
            "éƒ¨é—¨è§„ç« ": "departmental_rules",
            "åœ°æ–¹æ€§æ³•è§„": "local_regulations",
            "å¸æ³•è§£é‡Š": "judicial_interpretations",
            "å…¶ä»–": "others"
        }
        
        for category_name, category_dir in self.categories.items():
            category_path = self.cache_dir / category_dir
            category_path.mkdir(exist_ok=True)
        
    def _get_cache_key(self, data: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return hashlib.sha1(data.encode()).hexdigest()
        
    def get(self, key: str) -> Optional[Dict]:
        """è·å–ç¼“å­˜"""
        cache_file = self.cache_dir / f"{key}.json"
        if cache_file.exists():
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"è¯»å–ç¼“å­˜å¤±è´¥: {e}")
        return None
        
    def set(self, key: str, data: Dict):
        """è®¾ç½®ç¼“å­˜"""
        cache_file = self.cache_dir / f"{key}.json"
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"å†™å…¥ç¼“å­˜å¤±è´¥: {e}")
            
    def write_law(self, law_data: Dict):
        """å†™å…¥æ³•å¾‹æ–‡ä»¶åˆ°åˆ†ç±»ç›®å½• - å‚è€ƒexample project"""
        try:
            # ç¡®å®šåˆ†ç±»
            category = self._determine_category(law_data.get('name', ''))
            category_dir = self.categories.get(category, 'others')
            
            # ç”Ÿæˆæ–‡ä»¶å
            law_name = law_data.get('name', 'unknown')
            safe_name = self._sanitize_filename(law_name)
            file_path = self.cache_dir / category_dir / f"{safe_name}.json"
            
            # ç¡®ä¿åŒ…å«æ‰€æœ‰å¿…è¦å­—æ®µ
            enriched_data = {
                "law_id": law_data.get("law_id"),
                "name": law_data.get("name"),
                "number": law_data.get("number"),
                "law_type": law_data.get("law_type"),
                "issuing_authority": law_data.get("issuing_authority"),
                
                # æ—¶é—´å­—æ®µ
                "publish_date": law_data.get("publish_date"),  # å‘å¸ƒæ—¥æœŸ
                "valid_from": law_data.get("valid_from"),      # å®æ–½æ—¥æœŸ
                "valid_to": law_data.get("valid_to"),          # å¤±æ•ˆæ—¥æœŸ
                "crawl_time": law_data.get("crawl_time"),      # çˆ¬å–æ—¥æœŸ
                
                # æ¥æºä¿¡æ¯
                "source_url": law_data.get("source_url"),      # å®é™…ç½‘é¡µURL
                "source": law_data.get("source"),              # æ•°æ®æº
                
                # å†…å®¹
                "content": law_data.get("content"),
                "keywords": law_data.get("keywords"),
                "category": category,
                
                # å…ƒæ•°æ®
                "status": law_data.get("status", "effective"),
                "version": law_data.get("version", "1.0")
            }
            
            # å†™å…¥æ–‡ä»¶
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(enriched_data, f, ensure_ascii=False, indent=2)
                
            logger.info(f"æ³•å¾‹æ–‡ä»¶å·²ä¿å­˜åˆ°: {file_path}")
            return str(file_path)
            
        except Exception as e:
            logger.error(f"å†™å…¥æ³•å¾‹æ–‡ä»¶å¤±è´¥: {e}")
            return None
            
    def _determine_category(self, law_name: str) -> str:
        """æ ¹æ®æ³•å¾‹åç§°ç¡®å®šåˆ†ç±»"""
        if not law_name:
            return "å…¶ä»–"
            
        # æ³•å¾‹åˆ†ç±»è§„åˆ™
        if any(keyword in law_name for keyword in ["æ³•", "æ³•å…¸"]):
            return "æ³•å¾‹"
        elif any(keyword in law_name for keyword in ["æ¡ä¾‹", "è§„å®š", "åŠæ³•"]):
            if any(keyword in law_name for keyword in ["å›½åŠ¡é™¢", "æ”¿åºœ"]):
                return "è¡Œæ”¿æ³•è§„"
            else:
                return "éƒ¨é—¨è§„ç« "
        elif any(keyword in law_name for keyword in ["è§£é‡Š", "å¸æ³•è§£é‡Š"]):
            return "å¸æ³•è§£é‡Š"
        elif any(keyword in law_name for keyword in ["çœ", "å¸‚", "è‡ªæ²»åŒº", "åœ°æ–¹"]):
            return "åœ°æ–¹æ€§æ³•è§„"
        else:
            return "å…¶ä»–"
            
    def _sanitize_filename(self, filename: str) -> str:
        """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦"""
        import re
        # ç§»é™¤æˆ–æ›¿æ¢éæ³•å­—ç¬¦
        filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
        # é™åˆ¶é•¿åº¦
        if len(filename) > 100:
            filename = filename[:100]
        return filename


class CrawlerManager:
    """
    çˆ¬è™«ç®¡ç†å™¨ - åŒæ•°æ®æºç­–ç•¥
    
    æ•°æ®æºä¼˜å…ˆçº§ï¼š
    1. å›½å®¶æ³•å¾‹æ³•è§„æ•°æ®åº“ (flk.npc.gov.cn) - SearchBasedCrawler [æ³•å¾‹æ³•è§„æ•°æ®åº“]
    2. æœç´¢å¼•æ“çˆ¬è™« (SearchEngineCrawler) - é€šè¿‡æœç´¢å¼•æ“å®šä½æ³•è§„
    
    ç‰¹æ€§ï¼š
    - æ³•å¾‹æ³•è§„æ•°æ®åº“ä¼˜å…ˆï¼Œç¡®ä¿æƒå¨æ€§
    - æœç´¢å¼•æ“è¡¥å……ï¼Œæé«˜è¦†ç›–ç‡
    - å°è´¦å­—æ®µå®Œæ•´æå–
    """
    
    def __init__(self):
        self.logger = logger
        # å»¶è¿Ÿåˆå§‹åŒ–çˆ¬è™«ï¼Œå®ç°æµè§ˆå™¨å¤ç”¨
        self._search_crawler = None
        self._selenium_crawler = None
        self._optimized_selenium_crawler = None
        self._search_engine_crawler = None
        self._selenium_search_crawler = None
        self._direct_url_crawler = None
        self.cache = CacheManager()
        self.semaphore = asyncio.Semaphore(settings.crawler.max_concurrent)
    
    def _get_search_crawler(self):
        """è·å–æœç´¢çˆ¬è™«å®ä¾‹"""
        if self._search_crawler is None:
            self._search_crawler = SearchBasedCrawler()
        return self._search_crawler
    
    def _get_selenium_crawler(self):
        """è·å–Seleniumçˆ¬è™«å®ä¾‹ï¼ˆå¤ç”¨æµè§ˆå™¨ï¼‰"""
        if self._selenium_crawler is None:
            self._selenium_crawler = SeleniumGovCrawler()
            # é¢„å…ˆåˆå§‹åŒ–æµè§ˆå™¨
            self._selenium_crawler.setup_driver()
        return self._selenium_crawler
    
    def _get_search_engine_crawler(self):
        """è·å–æœç´¢å¼•æ“çˆ¬è™«å®ä¾‹"""
        if self._search_engine_crawler is None:
            self._search_engine_crawler = SearchEngineCrawler()
        return self._search_engine_crawler
    
    def _get_selenium_search_crawler(self):
        """è·å–Seleniumæœç´¢å¼•æ“çˆ¬è™«å®ä¾‹"""
        if self._selenium_search_crawler is None:
            self._selenium_search_crawler = SeleniumSearchCrawler(settings)
        return self._selenium_search_crawler
    
    def _get_direct_url_crawler(self):
        """è·å–ç›´æ¥URLçˆ¬è™«å®ä¾‹"""
        if self._direct_url_crawler is None:
            self._direct_url_crawler = DirectUrlCrawler()
        return self._direct_url_crawler
    
    def _get_optimized_selenium_crawler(self):
        """è·å–ä¼˜åŒ–ç‰ˆSeleniumçˆ¬è™«å®ä¾‹"""
        if self._optimized_selenium_crawler is None:
            self._optimized_selenium_crawler = OptimizedSeleniumCrawler()
        return self._optimized_selenium_crawler
    
    async def fetch(self, url: str, params: Dict = None, headers: Dict = None) -> aiohttp.ClientResponse:
        """é€šç”¨HTTPè¯·æ±‚æ–¹æ³•"""
        if headers is None:
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "application/json, text/html, */*",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8"
            }
            
        timeout = aiohttp.ClientTimeout(total=settings.crawler.timeout)
        
        async with aiohttp.ClientSession(timeout=timeout) as session:
            try:
                if params:
                    async with session.get(url, params=params, headers=headers) as response:
                        return response
                else:
                    async with session.get(url, headers=headers) as response:
                        return response
            except Exception as e:
                logger.error(f"HTTPè¯·æ±‚å¤±è´¥: {url}, é”™è¯¯: {e}")
                raise
        
    async def crawl_law(self, law_name: str, law_number: str = None, strategy: int = None) -> Dict[str, Any]:
        """
        çˆ¬å–å•ä¸ªæ³•è§„
        strategy: æŒ‡å®šç­–ç•¥ (1-5)ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨é»˜è®¤å¤šå±‚ç­–ç•¥
            1 - å›½å®¶æ³•å¾‹æ³•è§„æ•°æ®åº“ï¼ˆæƒå¨æ•°æ®æºï¼‰
            2 - HTTPæœç´¢å¼•æ“ï¼ˆå¿«é€Ÿæœç´¢ï¼Œå…ˆç›´è¿åä»£ç†ï¼‰
            3 - Seleniumæœç´¢å¼•æ“ï¼ˆæµè§ˆå™¨æœç´¢å¼•æ“ï¼‰
            4 - Seleniumæ”¿åºœç½‘ï¼ˆé’ˆå¯¹æ”¿åºœç½‘ä¼˜åŒ–ï¼‰
            5 - ç›´æ¥URLè®¿é—®ï¼ˆæœ€åä¿éšœï¼‰
        """
        self.logger.info(f"å¼€å§‹çˆ¬å–æ³•è§„: {law_name}")
        
        if strategy:
            # å•ä¸€ç­–ç•¥æ¨¡å¼
            self.logger.info(f"ä½¿ç”¨æŒ‡å®šç­–ç•¥ {strategy}")
            return await self._crawl_with_single_strategy(law_name, law_number, strategy)
        else:
            # é»˜è®¤å¤šå±‚ç­–ç•¥æ¨¡å¼
            return await self._crawl_with_multi_strategy(law_name, law_number)
    
    async def _crawl_with_single_strategy(self, law_name: str, law_number: str, strategy: int) -> Dict[str, Any]:
        """ä½¿ç”¨æŒ‡å®šçš„å•ä¸€ç­–ç•¥çˆ¬å–"""
        try:
            # å¯¼å…¥é…ç½®æ£€æŸ¥
            from config.settings import settings
            enable_selenium_search = getattr(settings.crawler, 'enable_selenium_search', True)
            enable_optimized_selenium = getattr(settings.crawler, 'enable_optimized_selenium', True)
            
            if strategy == 1:
                # ç­–ç•¥1: å›½å®¶æ³•å¾‹æ³•è§„æ•°æ®åº“
                self.logger.info("ä½¿ç”¨ç­–ç•¥1: å›½å®¶æ³•å¾‹æ³•è§„æ•°æ®åº“ï¼ˆæƒå¨æ•°æ®æºï¼‰")
                search_crawler = self._get_search_crawler()
                result = await search_crawler.crawl_law(law_name, law_number)
                
                if result and result.get('success'):
                    self.logger.success(f"å›½å®¶æ³•å¾‹æ³•è§„æ•°æ®åº“æˆåŠŸ: {law_name}")
                    result['crawler_strategy'] = 'search_based'
                    return result
                    
            elif strategy == 2:
                # ç­–ç•¥2: HTTPæœç´¢å¼•æ“
                self.logger.info("ä½¿ç”¨ç­–ç•¥2: HTTPæœç´¢å¼•æ“ï¼ˆå¿«é€Ÿç›´è¿ï¼‰")
                search_engine_crawler = self._get_search_engine_crawler()
                result = await search_engine_crawler.crawl_law(law_name, law_number)
                
                if result and result.get('success'):
                    self.logger.success(f"HTTPæœç´¢å¼•æ“æˆåŠŸ: {law_name}")
                    result['crawler_strategy'] = 'search_engine'
                    return result
                    
            elif strategy == 3:
                # ç­–ç•¥3: Seleniumæœç´¢å¼•æ“ - æ£€æŸ¥æ˜¯å¦å¯ç”¨
                if not enable_selenium_search:
                    self.logger.warning(f"Seleniumæœç´¢å¼•æ“ç­–ç•¥å·²ç¦ç”¨ï¼Œè·³è¿‡: {law_name}")
                    return self._create_failed_result(law_name, "Seleniumæœç´¢å¼•æ“ç­–ç•¥å·²ç¦ç”¨")
                    
                self.logger.info("ä½¿ç”¨ç­–ç•¥3: Seleniumæœç´¢å¼•æ“ï¼ˆæµè§ˆå™¨æœç´¢ï¼‰")
                selenium_search_crawler = self._get_selenium_search_crawler()
                result = await selenium_search_crawler.crawl(law_name, law_number=law_number)
                
                if result and result.get('success'):
                    self.logger.success(f"Seleniumæœç´¢å¼•æ“æˆåŠŸ: {law_name}")
                    result['crawler_strategy'] = 'selenium_search'
                    return result
                    
            elif strategy == 4:
                # ç­–ç•¥4: Seleniumæ”¿åºœç½‘ - æ£€æŸ¥æ˜¯å¦å¯ç”¨
                if not enable_optimized_selenium:
                    self.logger.warning(f"Seleniumæ”¿åºœç½‘ç­–ç•¥å·²ç¦ç”¨ï¼Œè·³è¿‡: {law_name}")
                    return self._create_failed_result(law_name, "Seleniumæ”¿åºœç½‘ç­–ç•¥å·²ç¦ç”¨")
                    
                self.logger.info("ä½¿ç”¨ç­–ç•¥4: Seleniumæ”¿åºœç½‘çˆ¬è™«")
                selenium_crawler = self._get_selenium_crawler()
                result = await selenium_crawler.crawl_law(law_name, law_number)
                
                if result and result.get('success'):
                    self.logger.success(f"Seleniumæ”¿åºœç½‘çˆ¬è™«æˆåŠŸ: {law_name}")
                    result['crawler_strategy'] = 'selenium_gov'
                    return result
                    
            elif strategy == 5:
                # ç­–ç•¥5: ç›´æ¥URLè®¿é—®
                self.logger.info("ä½¿ç”¨ç­–ç•¥5: ç›´æ¥URLè®¿é—®çˆ¬è™«")
                direct_url_crawler = self._get_direct_url_crawler()
                result = await direct_url_crawler.crawl_law(law_name, law_number)
                
                if result and result.get('success'):
                    self.logger.success(f"ç›´æ¥URLè®¿é—®æˆåŠŸ: {law_name}")
                    result['crawler_strategy'] = 'direct_url'
                    return result
            
            # ç­–ç•¥å¤±è´¥
            self.logger.error(f"æŒ‡å®šç­–ç•¥ {strategy} å¤±è´¥: {law_name}")
            return self._create_failed_result(law_name, f"ç­–ç•¥ {strategy} å¤±è´¥")
            
        except Exception as e:
            self.logger.error(f"ç­–ç•¥ {strategy} æ‰§è¡Œå¼‚å¸¸: {e}")
            return self._create_failed_result(law_name, f"ç­–ç•¥ {strategy} æ‰§è¡Œå¼‚å¸¸: {e}")

    async def _crawl_with_multi_strategy(self, law_name: str, law_number: str) -> Dict[str, Any]:
        """ä½¿ç”¨é»˜è®¤å¤šå±‚ç­–ç•¥çˆ¬å–"""
        # å¯¼å…¥é…ç½®æ£€æŸ¥
        from config.settings import settings
        enable_selenium_search = getattr(settings.crawler, 'enable_selenium_search', True)
        enable_optimized_selenium = getattr(settings.crawler, 'enable_optimized_selenium', True)
        
        # ç­–ç•¥1: å›½å®¶æ³•å¾‹æ³•è§„æ•°æ®åº“çˆ¬è™«ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        # ä¼˜åŠ¿ï¼šæ•°æ®æƒå¨ï¼Œç»“æ„åŒ–å¥½ï¼Œå®˜æ–¹æ•°æ®æº
        try:
            self.logger.info("å°è¯•ç­–ç•¥1: å›½å®¶æ³•å¾‹æ³•è§„æ•°æ®åº“ï¼ˆæƒå¨æ•°æ®æºï¼‰")
            search_crawler = self._get_search_crawler()
            result = await search_crawler.crawl_law(law_name, law_number)
            
            if result and result.get('success'):
                self.logger.success(f"å›½å®¶æ³•å¾‹æ³•è§„æ•°æ®åº“æˆåŠŸ: {law_name}")
                result['crawler_strategy'] = 'search_based'
                return result
            else:
                self.logger.warning(f"å›½å®¶æ³•å¾‹æ³•è§„æ•°æ®åº“æ— ç»“æœ: {law_name}")
        except Exception as e:
            self.logger.warning(f"å›½å®¶æ³•å¾‹æ³•è§„æ•°æ®åº“å¤±è´¥: {e}")
        
        # ç­–ç•¥2: å¿«é€ŸHTTPæœç´¢å¼•æ“çˆ¬è™«ï¼ˆå…ˆç›´è¿ï¼Œå¤±è´¥åç”¨ä»£ç†ï¼‰
        # ä¼˜åŠ¿ï¼šé€Ÿåº¦å¿«ï¼Œç»•è¿‡åçˆ¬æœºåˆ¶ï¼Œä¸ä¾èµ–æµè§ˆå™¨
        try:
            self.logger.info("å°è¯•ç­–ç•¥2: å¿«é€ŸHTTPæœç´¢å¼•æ“ï¼ˆå…ˆç›´è¿åä»£ç†ï¼‰")
            search_engine_crawler = self._get_search_engine_crawler()
            result = await search_engine_crawler.crawl_law(law_name, law_number)
            
            if result and result.get('success'):
                self.logger.success(f"HTTPæœç´¢å¼•æ“æˆåŠŸ: {law_name}")
                result['crawler_strategy'] = 'search_engine'
                return result
            else:
                self.logger.warning(f"HTTPæœç´¢å¼•æ“æ— ç»“æœ: {law_name}")
        except Exception as e:
            self.logger.warning(f"HTTPæœç´¢å¼•æ“å¤±è´¥: {e}")
        
        # ç­–ç•¥3: Seleniumæœç´¢å¼•æ“çˆ¬è™«ï¼ˆæµè§ˆå™¨æœç´¢å¼•æ“ï¼‰- æ£€æŸ¥æ˜¯å¦å¯ç”¨
        if enable_selenium_search:
            try:
                self.logger.info("å°è¯•ç­–ç•¥3: Seleniumæœç´¢å¼•æ“ï¼ˆæµè§ˆå™¨æœç´¢ï¼‰")
                selenium_search_crawler = self._get_selenium_search_crawler()
                result = await selenium_search_crawler.crawl(law_name, law_number=law_number)
                
                if result and result.get('success'):
                    self.logger.success(f"Seleniumæœç´¢å¼•æ“æˆåŠŸ: {law_name}")
                    result['crawler_strategy'] = 'selenium_search'
                    return result
                else:
                    self.logger.warning(f"Seleniumæœç´¢å¼•æ“æ— ç»“æœ: {law_name}")
            except Exception as e:
                self.logger.warning(f"Seleniumæœç´¢å¼•æ“å¤±è´¥: {e}")
        else:
            self.logger.info("âš¡ è·³è¿‡Seleniumæœç´¢å¼•æ“ç­–ç•¥ - å·²ç¦ç”¨ä»¥æé«˜æ€§èƒ½")
        
        # ç­–ç•¥4: Seleniumæ”¿åºœç½‘çˆ¬è™« - æ£€æŸ¥æ˜¯å¦å¯ç”¨
        if enable_optimized_selenium:
            try:
                self.logger.info("å°è¯•ç­–ç•¥4: Seleniumæ”¿åºœç½‘çˆ¬è™«")
                selenium_crawler = self._get_selenium_crawler()
                result = await selenium_crawler.crawl_law(law_name, law_number)
                
                if result and result.get('success'):
                    self.logger.success(f"Seleniumæ”¿åºœç½‘çˆ¬è™«æˆåŠŸ: {law_name}")
                    result['crawler_strategy'] = 'selenium_gov'
                    return result
                else:
                    self.logger.warning(f"Seleniumæ”¿åºœç½‘çˆ¬è™«æ— ç»“æœ: {law_name}")
            except Exception as e:
                self.logger.warning(f"Seleniumæ”¿åºœç½‘çˆ¬è™«å¤±è´¥: {e}")
        else:
            self.logger.info("âš¡ è·³è¿‡Seleniumæ”¿åºœç½‘ç­–ç•¥ - å·²ç¦ç”¨ä»¥æé«˜æ€§èƒ½")
        
        # ç­–ç•¥5: ç›´æ¥URLè®¿é—®çˆ¬è™«ï¼ˆæœ€åä¿éšœï¼‰
        # ä¼˜åŠ¿ï¼šç›´æ¥è®¿é—®å·²çŸ¥çš„æ”¿åºœç½‘é“¾æ¥ï¼Œç»•è¿‡æœç´¢é™åˆ¶
        try:
            self.logger.info("å°è¯•ç­–ç•¥5: ç›´æ¥URLè®¿é—®çˆ¬è™«")
            direct_url_crawler = self._get_direct_url_crawler()
            result = await direct_url_crawler.crawl_law(law_name, law_number)
            
            if result and result.get('success'):
                self.logger.success(f"ç›´æ¥URLè®¿é—®æˆåŠŸ: {law_name}")
                result['crawler_strategy'] = 'direct_url'
                return result
            else:
                self.logger.warning(f"ç›´æ¥URLè®¿é—®æ— ç»“æœ: {law_name}")
        except Exception as e:
            self.logger.warning(f"ç›´æ¥URLè®¿é—®å¤±è´¥: {e}")
        
        # æ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥
        self.logger.error(f"æ‰€æœ‰çˆ¬å–ç­–ç•¥éƒ½å¤±è´¥: {law_name}")
        return self._create_failed_result(law_name, "æ‰€æœ‰çˆ¬å–ç­–ç•¥éƒ½å¤±è´¥")
    
    async def crawl_laws_batch(self, law_list: List[Dict[str, str]], limit: int = None, strategy: int = None) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡çˆ¬å–æ³•è§„ - ç»ˆæä¼˜åŒ–ç‰ˆæœ¬
        å®ç°å¤šç­–ç•¥å¹¶è¡Œï¼Œæµè§ˆå™¨å¤ç”¨ï¼Œæ˜¾è‘—æé«˜æ•ˆç‡
        strategy: æŒ‡å®šç­–ç•¥ (1-5)ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨é»˜è®¤å¤šå±‚ç­–ç•¥
        """
        if limit:
            law_list = law_list[:limit]
        
        total_count = len(law_list)
        
        if strategy:
            self.logger.info(f"å¼€å§‹æ‰¹é‡çˆ¬å– {total_count} ä¸ªæ³•è§„ï¼ˆå•ä¸€ç­–ç•¥ {strategy} æ¨¡å¼ï¼‰")
            return await self._crawl_laws_batch_single_strategy(law_list, strategy)
        else:
            self.logger.info(f"å¼€å§‹æ‰¹é‡çˆ¬å– {total_count} ä¸ªæ³•è§„ï¼ˆç»ˆæä¼˜åŒ–æ¨¡å¼ï¼‰")
            return await self._crawl_laws_batch_multi_strategy(law_list)
    
    async def _crawl_laws_batch_single_strategy(self, law_list: List[Dict[str, str]], strategy: int) -> List[Dict[str, Any]]:
        """ä½¿ç”¨å•ä¸€ç­–ç•¥æ‰¹é‡çˆ¬å–"""
        total_count = len(law_list)
        law_names = [law_info.get('åç§°', law_info.get('name', '')) for law_info in law_list]
        
        strategy_names = {
            1: "å›½å®¶æ³•å¾‹æ³•è§„æ•°æ®åº“",
            2: "HTTPæœç´¢å¼•æ“",
            3: "Seleniumæœç´¢å¼•æ“",
            4: "Seleniumæ”¿åºœç½‘",
            5: "ç›´æ¥URLè®¿é—®"
        }
        
        self.logger.info(f"ä½¿ç”¨å•ä¸€ç­–ç•¥: {strategy} - {strategy_names.get(strategy, 'æœªçŸ¥ç­–ç•¥')}")
        
        start_time = time.time()
        results = []
        
        # å¹¶è¡Œå¤„ç†æ‰€æœ‰æ³•è§„
        tasks = []
        for law_name in law_names:
            tasks.append(self._crawl_with_single_strategy(law_name, None, strategy))
        
        batch_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        success_count = 0
        for law_info, result in zip(law_list, batch_results):
            law_name = law_info.get('åç§°', law_info.get('name', ''))
            
            if isinstance(result, Exception):
                self.logger.warning(f"ç­–ç•¥ {strategy} å¼‚å¸¸: {law_name} - {result}")
                results.append(self._create_failed_result(law_name, f"ç­–ç•¥ {strategy} å¼‚å¸¸: {result}"))
            elif result and result.get('success'):
                results.append(result)
                success_count += 1
                self.logger.success(f"ç­–ç•¥ {strategy} æˆåŠŸ: {law_name}")
            else:
                results.append(self._create_failed_result(law_name, f"ç­–ç•¥ {strategy} å¤±è´¥"))
                self.logger.warning(f"ç­–ç•¥ {strategy} å¤±è´¥: {law_name}")
        
        total_time = time.time() - start_time
        success_rate = (success_count / total_count) * 100 if total_count > 0 else 0
        avg_time_per_law = total_time / total_count if total_count > 0 else 0
        
        self.logger.success(f"ğŸ‰ å•ä¸€ç­–ç•¥æ‰¹é‡çˆ¬å–å®Œæˆï¼")
        self.logger.info(f"ğŸ“Š ç­–ç•¥: {strategy} - {strategy_names.get(strategy, 'æœªçŸ¥')}")
        self.logger.info(f"ğŸ“Š æ€»æ•°: {total_count}, æˆåŠŸ: {success_count}, æˆåŠŸç‡: {success_rate:.1f}%")
        self.logger.info(f"â±ï¸ æ€»ç”¨æ—¶: {total_time:.1f}ç§’, å¹³å‡: {avg_time_per_law:.2f}ç§’/æ³•è§„")
        
        return results
    
    async def _crawl_laws_batch_multi_strategy(self, law_list: List[Dict[str, str]]) -> List[Dict[str, Any]]:
        """ä½¿ç”¨é»˜è®¤å¤šå±‚ç­–ç•¥æ‰¹é‡çˆ¬å–"""
        from config.settings import settings
        
        total_count = len(law_list)
        
        # æå–æ³•è§„åç§°åˆ—è¡¨
        law_names = [law_info.get('åç§°', law_info.get('name', '')) for law_info in law_list]
        
        start_time = time.time()
        
        # ç­–ç•¥1: å›½å®¶æ³•å¾‹æ³•è§„æ•°æ®åº“æ‰¹é‡çˆ¬å–ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        search_based_results = {}
        try:
            self.logger.info(f"[PHASE1] é˜¶æ®µ1: å›½å®¶æ³•å¾‹æ³•è§„æ•°æ®åº“æ‰¹é‡çˆ¬å–ï¼ˆæƒå¨æ•°æ®æºï¼‰- æ€»æ•°: {total_count}")
            search_crawler = self._get_search_crawler()
            
            search_tasks = []
            for i, law_name in enumerate(law_names, 1):
                self.logger.info(f"[{i}/{total_count}] å‡†å¤‡çˆ¬å–: {law_name}")
                search_tasks.append(search_crawler.crawl_law(law_name))
            
            search_results = await asyncio.gather(*search_tasks, return_exceptions=True)
            
            for i, (law_name, result) in enumerate(zip(law_names, search_results), 1):
                if isinstance(result, Exception):
                    self.logger.warning(f"[{i}/{total_count}] æ³•è§„åº“å¼‚å¸¸: {law_name} - {result}")
                elif result and result.get('success'):
                    search_based_results[law_name] = result
                    self.logger.success(f"[{i}/{total_count}] [DB] æ³•è§„åº“æˆåŠŸ: {law_name}")
                else:
                    self.logger.warning(f"[{i}/{total_count}] æ³•è§„åº“å¤±è´¥: {law_name}")
            
            search_success_rate = len(search_based_results) / len(law_names) * 100
            self.logger.info(f"æ³•è§„åº“é˜¶æ®µå®Œæˆ: {len(search_based_results)}/{len(law_names)} æˆåŠŸ ({search_success_rate:.1f}%)")
            
        except Exception as e:
            self.logger.error(f"æ³•è§„åº“æ‰¹é‡çˆ¬å–å¤±è´¥: {e}")
        
        # ç­–ç•¥2: å¿«é€ŸHTTPæœç´¢å¼•æ“æ‰¹é‡çˆ¬å–ï¼ˆå…ˆç›´è¿ï¼Œå¤±è´¥åç”¨ä»£ç†ï¼‰
        remaining_laws = [name for name in law_names if name not in search_based_results]
        search_engine_results = {}
        
        if remaining_laws:
            try:
                self.logger.info(f"[PHASE2] é˜¶æ®µ2: å¿«é€ŸHTTPæœç´¢å¼•æ“æ‰¹é‡çˆ¬å– ({len(remaining_laws)}ä¸ªå‰©ä½™)")
                search_engine_crawler = self._get_search_engine_crawler()
                
                search_tasks = []
                for i, law_name in enumerate(remaining_laws, 1):
                    remaining_index = len(search_based_results) + i
                    self.logger.info(f"[{remaining_index}/{total_count}] æœç´¢å¼•æ“å‡†å¤‡: {law_name}")
                    search_tasks.append(search_engine_crawler.crawl_law(law_name))
                
                search_results = await asyncio.gather(*search_tasks, return_exceptions=True)
                
                for i, (law_name, result) in enumerate(zip(remaining_laws, search_results), 1):
                    remaining_index = len(search_based_results) + i
                    if isinstance(result, Exception):
                        self.logger.warning(f"[{remaining_index}/{total_count}] HTTPæœç´¢å¼•æ“å¼‚å¸¸: {law_name} - {result}")
                    elif result and result.get('success'):
                        search_engine_results[law_name] = result
                        self.logger.success(f"[{remaining_index}/{total_count}] [SE] HTTPæœç´¢å¼•æ“æˆåŠŸ: {law_name}")
                    else:
                        self.logger.warning(f"[{remaining_index}/{total_count}] HTTPæœç´¢å¼•æ“å¤±è´¥: {law_name}")
                
                search_engine_success_rate = len(search_engine_results) / len(remaining_laws) * 100 if remaining_laws else 0
                self.logger.info(f"HTTPæœç´¢å¼•æ“é˜¶æ®µå®Œæˆ: {len(search_engine_results)}/{len(remaining_laws)} æˆåŠŸ ({search_engine_success_rate:.1f}%)")
                
            except Exception as e:
                self.logger.error(f"HTTPæœç´¢å¼•æ“æ‰¹é‡çˆ¬å–å¤±è´¥: {e}")
        
        # [OPTIMIZE] æ€§èƒ½ä¼˜åŒ–ï¼šå®Œå…¨è·³è¿‡å¤±æ•ˆçš„Seleniumç­–ç•¥
        selenium_search_results = {}
        selenium_results = {}
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨Seleniumç­–ç•¥
        enable_selenium_search = getattr(settings.crawler, 'enable_selenium_search', True)
        enable_optimized_selenium = getattr(settings.crawler, 'enable_optimized_selenium', True)
        
        # å¼ºåˆ¶ç¦ç”¨Seleniumç­–ç•¥ä»¥é¿å…Chromeå´©æºƒ
        enable_selenium_search = False
        enable_optimized_selenium = False
        
        remaining_laws_2 = [name for name in law_names if name not in search_based_results and name not in search_engine_results]
        
        if remaining_laws_2 and enable_selenium_search:
            try:
                self.logger.info(f"ğŸŒ é˜¶æ®µ3: Seleniumæœç´¢å¼•æ“æ‰¹é‡çˆ¬å– ({len(remaining_laws_2)}ä¸ªå‰©ä½™)")
                selenium_search_crawler = self._get_selenium_search_crawler()
                
                search_tasks = []
                for law_name in remaining_laws_2:
                    search_tasks.append(selenium_search_crawler.crawl(law_name))
                
                search_results = await asyncio.gather(*search_tasks, return_exceptions=True)
                
                for law_name, result in zip(remaining_laws_2, search_results):
                    if isinstance(result, Exception):
                        self.logger.warning(f"Seleniumæœç´¢å¼•æ“å¼‚å¸¸: {law_name} - {result}")
                    elif result and result.get('success'):
                        selenium_search_results[law_name] = result
                        self.logger.success(f"ğŸ” Seleniumæœç´¢å¼•æ“æˆåŠŸ: {law_name}")
                
                selenium_search_success_rate = len(selenium_search_results) / len(remaining_laws_2) * 100 if remaining_laws_2 else 0
                self.logger.info(f"Seleniumæœç´¢å¼•æ“é˜¶æ®µå®Œæˆ: {len(selenium_search_results)}/{len(remaining_laws_2)} æˆåŠŸ ({selenium_search_success_rate:.1f}%)")
                
            except Exception as e:
                self.logger.error(f"Seleniumæœç´¢å¼•æ“æ‰¹é‡çˆ¬å–å¤±è´¥: {e}")
        else:
            if remaining_laws_2 and not enable_selenium_search:
                self.logger.info(f"[SKIP] è·³è¿‡Seleniumæœç´¢å¼•æ“ç­–ç•¥ - å·²ç¦ç”¨ä»¥æé«˜æ€§èƒ½")
        
        # ç­–ç•¥4: ä¼˜åŒ–ç‰ˆSeleniumæ”¿åºœç½‘æ‰¹é‡çˆ¬å–ï¼ˆæœ€éš¾çš„æ³•è§„ï¼‰
        final_remaining_laws = [name for name in law_names if name not in search_based_results and name not in search_engine_results and name not in selenium_search_results]
        
        if final_remaining_laws and enable_optimized_selenium:
            try:
                self.logger.info(f"ğŸ”§ é˜¶æ®µ4: ä¼˜åŒ–ç‰ˆSeleniumæ”¿åºœç½‘æ‰¹é‡çˆ¬å– ({len(final_remaining_laws)}ä¸ªå›°éš¾æ³•è§„)")
                optimized_selenium_crawler = self._get_optimized_selenium_crawler()
                
                # ä½¿ç”¨ä¼˜åŒ–ç‰ˆSeleniumçš„æ‰¹é‡å¤„ç†æ–¹æ³•
                selenium_batch_results = await optimized_selenium_crawler.crawl_laws_batch(final_remaining_laws)
                
                for result in selenium_batch_results:
                    if result and result.get('success'):
                        law_name = result.get('target_name', result.get('name', ''))
                        selenium_results[law_name] = result
                        self.logger.success(f"âš¡ ä¼˜åŒ–SeleniumæˆåŠŸ: {law_name}")
                
                selenium_success_rate = len(selenium_results) / len(final_remaining_laws) * 100 if final_remaining_laws else 0
                self.logger.info(f"Seleniumæ”¿åºœç½‘é˜¶æ®µå®Œæˆ: {len(selenium_results)}/{len(final_remaining_laws)} æˆåŠŸ ({selenium_success_rate:.1f}%)")
                
            except Exception as e:
                self.logger.error(f"ä¼˜åŒ–Seleniumæ‰¹é‡çˆ¬å–å¤±è´¥: {e}")
        else:
            if final_remaining_laws and not enable_optimized_selenium:
                self.logger.info(f"[SKIP] è·³è¿‡ä¼˜åŒ–ç‰ˆSeleniumç­–ç•¥ - å·²ç¦ç”¨ä»¥æé«˜æ€§èƒ½")

        # åˆå¹¶æ‰€æœ‰ç»“æœ
        results = []
        success_count = 0
        
        for law_info in law_list:
            law_name = law_info.get('åç§°', law_info.get('name', ''))
            
            if law_name in search_based_results:
                result = search_based_results[law_name]
                result['crawler_strategy'] = 'search_based'
                results.append(result)
                success_count += 1
            elif law_name in search_engine_results:
                result = search_engine_results[law_name]
                result['crawler_strategy'] = 'search_engine'
                results.append(result)
                success_count += 1
            elif law_name in selenium_search_results:
                result = selenium_search_results[law_name]
                result['crawler_strategy'] = 'selenium_search'
                results.append(result)
                success_count += 1
            elif law_name in selenium_results:
                result = selenium_results[law_name]
                result['crawler_strategy'] = 'optimized_selenium'
                results.append(result)
                success_count += 1
            else:
                results.append(self._create_failed_result(law_name, "æ‰€æœ‰æ‰¹é‡ç­–ç•¥éƒ½å¤±è´¥"))
        
        total_time = time.time() - start_time
        success_rate = (success_count / total_count) * 100
        avg_time_per_law = total_time / total_count
        
        self.logger.success(f"[COMPLETED] æ‰¹é‡çˆ¬å–å®Œæˆï¼")
        self.logger.info(f"[STATS] æ€»æ•°: {total_count}, æˆåŠŸ: {success_count}, æˆåŠŸç‡: {success_rate:.1f}%")
        self.logger.info(f"[TIME] æ€»ç”¨æ—¶: {total_time:.1f}ç§’, å¹³å‡: {avg_time_per_law:.2f}ç§’/æ³•è§„")
        self.logger.info(f"[STRATEGY] ç­–ç•¥åˆ†å¸ƒ: æœç´¢å¼•æ“({len(search_engine_results)}), æ³•è§„åº“({len(search_based_results)}), Selenium({len(selenium_results)})")
        
        return results
    
    async def async_cleanup(self):
        """å¼‚æ­¥æ¸…ç†èµ„æº"""
        try:
            if self._selenium_crawler:
                self._selenium_crawler.close_driver()
                self.logger.info("Seleniumæµè§ˆå™¨å·²å…³é—­")
            if self._optimized_selenium_crawler:
                self._optimized_selenium_crawler.close_session()
                self.logger.info("ä¼˜åŒ–ç‰ˆSeleniumæµè§ˆå™¨å·²å…³é—­")
            if self._search_engine_crawler:
                try:
                    await self._search_engine_crawler.close()
                    self.logger.info("æœç´¢å¼•æ“çˆ¬è™«è¿æ¥å·²å…³é—­")
                except Exception as close_error:
                    self.logger.warning(f"å…³é—­æœç´¢å¼•æ“çˆ¬è™«è¿æ¥å¤±è´¥: {close_error}")
        except Exception as e:
            self.logger.warning(f"å¼‚æ­¥æ¸…ç†èµ„æºæ—¶å‡ºé”™: {e}")
            
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            if self._selenium_crawler:
                self._selenium_crawler.close_driver()
                self.logger.info("Seleniumæµè§ˆå™¨å·²å…³é—­")
            if self._optimized_selenium_crawler:
                self._optimized_selenium_crawler.close_session()
                self.logger.info("ä¼˜åŒ–ç‰ˆSeleniumæµè§ˆå™¨å·²å…³é—­")
            if self._search_engine_crawler:
                # åŒæ­¥è°ƒç”¨å¼‚æ­¥å…³é—­
                import asyncio
                try:
                    loop = asyncio.get_event_loop()
                    if loop.is_running():
                        # å¦‚æœäº‹ä»¶å¾ªç¯æ­£åœ¨è¿è¡Œï¼Œåˆ›å»ºä»»åŠ¡
                        loop.create_task(self._search_engine_crawler.close())
                    else:
                        # å¦‚æœäº‹ä»¶å¾ªç¯æœªè¿è¡Œï¼Œç›´æ¥æ‰§è¡Œ
                        loop.run_until_complete(self._search_engine_crawler.close())
                    self.logger.info("æœç´¢å¼•æ“çˆ¬è™«è¿æ¥å·²å…³é—­")
                except Exception as close_error:
                    self.logger.warning(f"å…³é—­æœç´¢å¼•æ“çˆ¬è™«è¿æ¥å¤±è´¥: {close_error}")
        except Exception as e:
            self.logger.warning(f"æ¸…ç†èµ„æºæ—¶å‡ºé”™: {e}")
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œç¡®ä¿èµ„æºæ¸…ç†"""
        self.cleanup()
    
    def _create_failed_result(self, law_name: str, error_message: str) -> Dict[str, Any]:
        """åˆ›å»ºå¤±è´¥ç»“æœçš„æ ‡å‡†æ ¼å¼"""
        from datetime import datetime
        return {
            'success': False,
            'name': law_name,
            'title': law_name,
            'number': '',
            'document_number': '',
            'publish_date': '',
            'valid_from': '',
            'valid_to': '',
            'office': '',
            'issuing_authority': '',
            'level': '',
            'law_level': '',
            'status': '',
            'source_url': '',
            'content': '',
            'target_name': law_name,
            'search_keyword': law_name,
            'crawl_time': datetime.now().isoformat(),
            'source': 'failed',
            'error': error_message,
            'crawler_strategy': 'failed'
        }


def create_crawler_manager():
    """åˆ›å»ºçˆ¬è™«ç®¡ç†å™¨å®ä¾‹"""
    return CrawlerManager()
        
 