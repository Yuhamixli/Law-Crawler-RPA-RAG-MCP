#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
çˆ¬è™«ç®¡ç†å™¨
è´Ÿè´£åè°ƒä¸åŒçš„çˆ¬è™«ç­–ç•¥ï¼Œä¼˜åŒ–æ•ˆç‡
"""

import asyncio
import pandas as pd
import json
import hashlib
from pathlib import Path
from typing import List, Dict, Any, Optional
from loguru import logger
import sys
import os
import aiohttp
import time
from datetime import datetime

sys.path.append(os.path.dirname(os.path.dirname(__file__)))
from src.crawler.strategies.search_based_crawler import SearchBasedCrawler
from src.crawler.strategies.selenium_gov_crawler import SeleniumGovCrawler
from src.crawler.strategies.search_engine_crawler import SearchEngineCrawler
from src.crawler.strategies.direct_url_crawler import DirectUrlCrawler
from src.crawler.strategies.optimized_selenium_crawler import OptimizedSeleniumCrawler

from config.settings import settings


class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨ - å‚è€ƒexample project"""
    
    def __init__(self, cache_dir: str = "data/cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºåˆ†ç±»ç›®å½•
        self.categories = {
            "æ³•å¾‹": "laws",
            "è¡Œæ”¿æ³•è§„": "regulations", 
            "éƒ¨é—¨è§„ç« ": "departmental_rules",
            "åœ°æ–¹æ€§æ³•è§„": "local_regulations",
            "å¸æ³•è§£é‡Š": "judicial_interpretations",
            "å…¶ä»–": "others"
        }
        
        for category_name, category_dir in self.categories.items():
            category_path = self.cache_dir / category_dir
            category_path.mkdir(exist_ok=True)
        
    def _get_cache_key(self, data: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return hashlib.sha1(data.encode()).hexdigest()
        
    def get(self, key: str) -> Optional[Dict]:
        """è·å–ç¼“å­˜"""
        cache_file = self.cache_dir / f"{key}.json"
        if cache_file.exists():
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"è¯»å–ç¼“å­˜å¤±è´¥: {e}")
        return None
        
    def set(self, key: str, data: Dict):
        """è®¾ç½®ç¼“å­˜"""
        cache_file = self.cache_dir / f"{key}.json"
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"å†™å…¥ç¼“å­˜å¤±è´¥: {e}")
            
    def write_law(self, law_data: Dict):
        """å†™å…¥æ³•å¾‹æ–‡ä»¶åˆ°åˆ†ç±»ç›®å½• - å‚è€ƒexample project"""
        try:
            # ç¡®å®šåˆ†ç±»
            category = self._determine_category(law_data.get('name', ''))
            category_dir = self.categories.get(category, 'others')
            
            # ç”Ÿæˆæ–‡ä»¶å
            law_name = law_data.get('name', 'unknown')
            safe_name = self._sanitize_filename(law_name)
            file_path = self.cache_dir / category_dir / f"{safe_name}.json"
            
            # ç¡®ä¿åŒ…å«æ‰€æœ‰å¿…è¦å­—æ®µ
            enriched_data = {
                "law_id": law_data.get("law_id"),
                "name": law_data.get("name"),
                "number": law_data.get("number"),
                "law_type": law_data.get("law_type"),
                "issuing_authority": law_data.get("issuing_authority"),
                
                # æ—¶é—´å­—æ®µ
                "publish_date": law_data.get("publish_date"),  # å‘å¸ƒæ—¥æœŸ
                "valid_from": law_data.get("valid_from"),      # å®æ–½æ—¥æœŸ
                "valid_to": law_data.get("valid_to"),          # å¤±æ•ˆæ—¥æœŸ
                "crawl_time": law_data.get("crawl_time"),      # çˆ¬å–æ—¥æœŸ
                
                # æ¥æºä¿¡æ¯
                "source_url": law_data.get("source_url"),      # å®é™…ç½‘é¡µURL
                "source": law_data.get("source"),              # æ•°æ®æº
                
                # å†…å®¹
                "content": law_data.get("content"),
                "keywords": law_data.get("keywords"),
                "category": category,
                
                # å…ƒæ•°æ®
                "status": law_data.get("status", "effective"),
                "version": law_data.get("version", "1.0")
            }
            
            # å†™å…¥æ–‡ä»¶
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(enriched_data, f, ensure_ascii=False, indent=2)
                
            logger.info(f"æ³•å¾‹æ–‡ä»¶å·²ä¿å­˜åˆ°: {file_path}")
            return str(file_path)
            
        except Exception as e:
            logger.error(f"å†™å…¥æ³•å¾‹æ–‡ä»¶å¤±è´¥: {e}")
            return None
            
    def _determine_category(self, law_name: str) -> str:
        """æ ¹æ®æ³•å¾‹åç§°ç¡®å®šåˆ†ç±»"""
        if not law_name:
            return "å…¶ä»–"
            
        # æ³•å¾‹åˆ†ç±»è§„åˆ™
        if any(keyword in law_name for keyword in ["æ³•", "æ³•å…¸"]):
            return "æ³•å¾‹"
        elif any(keyword in law_name for keyword in ["æ¡ä¾‹", "è§„å®š", "åŠæ³•"]):
            if any(keyword in law_name for keyword in ["å›½åŠ¡é™¢", "æ”¿åºœ"]):
                return "è¡Œæ”¿æ³•è§„"
            else:
                return "éƒ¨é—¨è§„ç« "
        elif any(keyword in law_name for keyword in ["è§£é‡Š", "å¸æ³•è§£é‡Š"]):
            return "å¸æ³•è§£é‡Š"
        elif any(keyword in law_name for keyword in ["çœ", "å¸‚", "è‡ªæ²»åŒº", "åœ°æ–¹"]):
            return "åœ°æ–¹æ€§æ³•è§„"
        else:
            return "å…¶ä»–"
            
    def _sanitize_filename(self, filename: str) -> str:
        """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦"""
        import re
        # ç§»é™¤æˆ–æ›¿æ¢éæ³•å­—ç¬¦
        filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
        # é™åˆ¶é•¿åº¦
        if len(filename) > 100:
            filename = filename[:100]
        return filename


class CrawlerManager:
    """
    çˆ¬è™«ç®¡ç†å™¨ - åŒæ•°æ®æºç­–ç•¥
    
    æ•°æ®æºä¼˜å…ˆçº§ï¼š
    1. å›½å®¶æ³•å¾‹æ³•è§„æ•°æ®åº“ (flk.npc.gov.cn) - SearchBasedCrawler [æ³•å¾‹æ³•è§„æ•°æ®åº“]
    2. æœç´¢å¼•æ“çˆ¬è™« (SearchEngineCrawler) - é€šè¿‡æœç´¢å¼•æ“å®šä½æ³•è§„
    
    ç‰¹æ€§ï¼š
    - æ³•å¾‹æ³•è§„æ•°æ®åº“ä¼˜å…ˆï¼Œç¡®ä¿æƒå¨æ€§
    - æœç´¢å¼•æ“è¡¥å……ï¼Œæé«˜è¦†ç›–ç‡
    - å°è´¦å­—æ®µå®Œæ•´æå–
    """
    
    def __init__(self):
        self.logger = logger
        # å»¶è¿Ÿåˆå§‹åŒ–çˆ¬è™«ï¼Œå®ç°æµè§ˆå™¨å¤ç”¨
        self._search_crawler = None
        self._selenium_crawler = None
        self._optimized_selenium_crawler = None
        self._search_engine_crawler = None
        self._direct_url_crawler = None
        self.cache = CacheManager()
        self.semaphore = asyncio.Semaphore(settings.crawler.max_concurrent)
    
    def _get_search_crawler(self):
        """è·å–æœç´¢çˆ¬è™«å®ä¾‹"""
        if self._search_crawler is None:
            self._search_crawler = SearchBasedCrawler()
        return self._search_crawler
    
    def _get_selenium_crawler(self):
        """è·å–Seleniumçˆ¬è™«å®ä¾‹ï¼ˆå¤ç”¨æµè§ˆå™¨ï¼‰"""
        if self._selenium_crawler is None:
            self._selenium_crawler = SeleniumGovCrawler()
            # é¢„å…ˆåˆå§‹åŒ–æµè§ˆå™¨
            self._selenium_crawler.setup_driver()
        return self._selenium_crawler
    
    def _get_search_engine_crawler(self):
        """è·å–æœç´¢å¼•æ“çˆ¬è™«å®ä¾‹"""
        if self._search_engine_crawler is None:
            self._search_engine_crawler = SearchEngineCrawler()
        return self._search_engine_crawler
    
    def _get_direct_url_crawler(self):
        """è·å–ç›´æ¥URLçˆ¬è™«å®ä¾‹"""
        if self._direct_url_crawler is None:
            self._direct_url_crawler = DirectUrlCrawler()
        return self._direct_url_crawler
    
    def _get_optimized_selenium_crawler(self):
        """è·å–ä¼˜åŒ–ç‰ˆSeleniumçˆ¬è™«å®ä¾‹"""
        if self._optimized_selenium_crawler is None:
            self._optimized_selenium_crawler = OptimizedSeleniumCrawler()
        return self._optimized_selenium_crawler
    
    async def fetch(self, url: str, params: Dict = None, headers: Dict = None) -> aiohttp.ClientResponse:
        """é€šç”¨HTTPè¯·æ±‚æ–¹æ³•"""
        if headers is None:
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "application/json, text/html, */*",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8"
            }
            
        timeout = aiohttp.ClientTimeout(total=settings.crawler.timeout)
        
        async with aiohttp.ClientSession(timeout=timeout) as session:
            try:
                if params:
                    async with session.get(url, params=params, headers=headers) as response:
                        return response
                else:
                    async with session.get(url, headers=headers) as response:
                        return response
            except Exception as e:
                logger.error(f"HTTPè¯·æ±‚å¤±è´¥: {url}, é”™è¯¯: {e}")
                raise
        
    async def crawl_law(self, law_name: str, law_number: str = None) -> Dict[str, Any]:
        """
        çˆ¬å–å•ä¸ªæ³•è§„
        ä¼˜åŒ–ç­–ç•¥ï¼šæŒ‰æ•ˆç‡å’ŒæˆåŠŸç‡æ’åº
        """
        self.logger.info(f"å¼€å§‹çˆ¬å–æ³•è§„: {law_name}")
        
        # ç­–ç•¥1: å›½å®¶æ³•å¾‹æ³•è§„æ•°æ®åº“çˆ¬è™«ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        # ä¼˜åŠ¿ï¼šæ•°æ®æƒå¨ï¼Œç»“æ„åŒ–å¥½ï¼Œå®˜æ–¹æ•°æ®æº
        try:
            self.logger.info("å°è¯•ç­–ç•¥1: å›½å®¶æ³•å¾‹æ³•è§„æ•°æ®åº“ï¼ˆæƒå¨æ•°æ®æºï¼‰")
            search_crawler = self._get_search_crawler()
            result = await search_crawler.crawl_law(law_name, law_number)
            
            if result and result.get('success'):
                self.logger.success(f"å›½å®¶æ³•å¾‹æ³•è§„æ•°æ®åº“æˆåŠŸ: {law_name}")
                result['crawler_strategy'] = 'search_based'
                return result
            else:
                self.logger.warning(f"å›½å®¶æ³•å¾‹æ³•è§„æ•°æ®åº“æ— ç»“æœ: {law_name}")
        except Exception as e:
            self.logger.warning(f"å›½å®¶æ³•å¾‹æ³•è§„æ•°æ®åº“å¤±è´¥: {e}")
        
        # ç­–ç•¥2: æœç´¢å¼•æ“çˆ¬è™«
        # ä¼˜åŠ¿ï¼šç»•è¿‡åçˆ¬æœºåˆ¶ï¼Œè¦†ç›–é¢å¹¿ï¼Œé€Ÿåº¦å¿«ï¼Œä¸ä¾èµ–æµè§ˆå™¨
        try:
            self.logger.info("å°è¯•ç­–ç•¥2: æœç´¢å¼•æ“çˆ¬è™«ï¼ˆè¡¥å……ç­–ç•¥ï¼‰")
            search_engine_crawler = self._get_search_engine_crawler()
            result = await search_engine_crawler.crawl_law(law_name, law_number)
            
            if result and result.get('success'):
                self.logger.success(f"æœç´¢å¼•æ“çˆ¬è™«æˆåŠŸ: {law_name}")
                result['crawler_strategy'] = 'search_engine'
                return result
            else:
                self.logger.warning(f"æœç´¢å¼•æ“çˆ¬è™«æ— ç»“æœ: {law_name}")
        except Exception as e:
            self.logger.warning(f"æœç´¢å¼•æ“çˆ¬è™«å¤±è´¥: {e}")
        
        # ç­–ç•¥3: Seleniumæ”¿åºœç½‘çˆ¬è™«
        # ä¼˜åŠ¿ï¼šæˆåŠŸç‡é«˜ï¼Œä½†é€Ÿåº¦æ…¢
        try:
            self.logger.info("å°è¯•ç­–ç•¥3: Seleniumæ”¿åºœç½‘çˆ¬è™«")
            selenium_crawler = self._get_selenium_crawler()
            result = await selenium_crawler.crawl_law(law_name, law_number)
            
            if result and result.get('success'):
                self.logger.success(f"Seleniumæ”¿åºœç½‘çˆ¬è™«æˆåŠŸ: {law_name}")
                result['crawler_strategy'] = 'selenium_gov'
                return result
            else:
                self.logger.warning(f"Seleniumæ”¿åºœç½‘çˆ¬è™«æ— ç»“æœ: {law_name}")
        except Exception as e:
            self.logger.warning(f"Seleniumæ”¿åºœç½‘çˆ¬è™«å¤±è´¥: {e}")
        
        # ç­–ç•¥4: ç›´æ¥URLè®¿é—®çˆ¬è™«ï¼ˆæœ€åä¿éšœï¼‰
        # ä¼˜åŠ¿ï¼šç›´æ¥è®¿é—®å·²çŸ¥çš„æ”¿åºœç½‘é“¾æ¥ï¼Œç»•è¿‡æœç´¢é™åˆ¶
        try:
            self.logger.info("å°è¯•ç­–ç•¥4: ç›´æ¥URLè®¿é—®çˆ¬è™«")
            direct_url_crawler = self._get_direct_url_crawler()
            result = await direct_url_crawler.crawl_law(law_name, law_number)
            
            if result and result.get('success'):
                self.logger.success(f"ç›´æ¥URLè®¿é—®æˆåŠŸ: {law_name}")
                result['crawler_strategy'] = 'direct_url'
                return result
            else:
                self.logger.warning(f"ç›´æ¥URLè®¿é—®æ— ç»“æœ: {law_name}")
        except Exception as e:
            self.logger.warning(f"ç›´æ¥URLè®¿é—®å¤±è´¥: {e}")
        
        # æ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥
        self.logger.error(f"æ‰€æœ‰çˆ¬å–ç­–ç•¥éƒ½å¤±è´¥: {law_name}")
        return self._create_failed_result(law_name, "æ‰€æœ‰çˆ¬å–ç­–ç•¥éƒ½å¤±è´¥")
    
    async def crawl_laws_batch(self, law_list: List[Dict[str, str]], limit: int = None) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡çˆ¬å–æ³•è§„ - ç»ˆæä¼˜åŒ–ç‰ˆæœ¬
        å®ç°å¤šç­–ç•¥å¹¶è¡Œï¼Œæµè§ˆå™¨å¤ç”¨ï¼Œæ˜¾è‘—æé«˜æ•ˆç‡
        """
        if limit:
            law_list = law_list[:limit]
        
        total_count = len(law_list)
        self.logger.info(f"å¼€å§‹æ‰¹é‡çˆ¬å– {total_count} ä¸ªæ³•è§„ï¼ˆç»ˆæä¼˜åŒ–æ¨¡å¼ï¼‰")
        
        # æå–æ³•è§„åç§°åˆ—è¡¨
        law_names = [law_info.get('åç§°', law_info.get('name', '')) for law_info in law_list]
        
        start_time = time.time()
        
        # ç­–ç•¥1: å›½å®¶æ³•å¾‹æ³•è§„æ•°æ®åº“æ‰¹é‡çˆ¬å–ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        search_based_results = {}
        try:
            self.logger.info("ğŸ›ï¸ é˜¶æ®µ1: å›½å®¶æ³•å¾‹æ³•è§„æ•°æ®åº“æ‰¹é‡çˆ¬å–ï¼ˆæƒå¨æ•°æ®æºï¼‰")
            search_crawler = self._get_search_crawler()
            
            search_tasks = []
            for law_name in law_names:
                search_tasks.append(search_crawler.crawl_law(law_name))
            
            search_results = await asyncio.gather(*search_tasks, return_exceptions=True)
            
            for law_name, result in zip(law_names, search_results):
                if isinstance(result, Exception):
                    self.logger.warning(f"æ³•è§„åº“å¼‚å¸¸: {law_name} - {result}")
                elif result and result.get('success'):
                    search_based_results[law_name] = result
                    self.logger.success(f"ğŸ“š æ³•è§„åº“æˆåŠŸ: {law_name}")
            
            search_success_rate = len(search_based_results) / len(law_names) * 100
            self.logger.info(f"æ³•è§„åº“é˜¶æ®µå®Œæˆ: {len(search_based_results)}/{len(law_names)} æˆåŠŸ ({search_success_rate:.1f}%)")
            
        except Exception as e:
            self.logger.error(f"æ³•è§„åº“æ‰¹é‡çˆ¬å–å¤±è´¥: {e}")
        
        # ç­–ç•¥2: æœç´¢å¼•æ“æ‰¹é‡çˆ¬å–ï¼ˆæœªæ‰¾åˆ°çš„æ³•è§„ï¼‰
        remaining_laws = [name for name in law_names if name not in search_based_results]
        search_engine_results = {}
        
        if remaining_laws:
            try:
                self.logger.info(f"ğŸš€ é˜¶æ®µ2: æœç´¢å¼•æ“æ‰¹é‡çˆ¬å– ({len(remaining_laws)}ä¸ªå‰©ä½™)")
                search_engine_crawler = self._get_search_engine_crawler()
                
                search_tasks = []
                for law_name in remaining_laws:
                    search_tasks.append(search_engine_crawler.crawl_law(law_name))
                
                search_results = await asyncio.gather(*search_tasks, return_exceptions=True)
                
                for law_name, result in zip(remaining_laws, search_results):
                    if isinstance(result, Exception):
                        self.logger.warning(f"æœç´¢å¼•æ“å¼‚å¸¸: {law_name} - {result}")
                    elif result and result.get('success'):
                        search_engine_results[law_name] = result
                        self.logger.success(f"ğŸ¯ æœç´¢å¼•æ“æˆåŠŸ: {law_name}")
                
                search_engine_success_rate = len(search_engine_results) / len(remaining_laws) * 100 if remaining_laws else 0
                self.logger.info(f"æœç´¢å¼•æ“é˜¶æ®µå®Œæˆ: {len(search_engine_results)}/{len(remaining_laws)} æˆåŠŸ ({search_engine_success_rate:.1f}%)")
                
            except Exception as e:
                self.logger.error(f"æœç´¢å¼•æ“æ‰¹é‡çˆ¬å–å¤±è´¥: {e}")
        
        # ç­–ç•¥3: ä¼˜åŒ–ç‰ˆSeleniumæ‰¹é‡çˆ¬å–ï¼ˆæœ€éš¾çš„æ³•è§„ï¼‰
        final_remaining_laws = [name for name in law_names if name not in search_based_results and name not in search_engine_results]
        selenium_results = {}
        
        if final_remaining_laws:
            try:
                self.logger.info(f"ğŸ”§ é˜¶æ®µ3: ä¼˜åŒ–ç‰ˆSeleniumæ‰¹é‡çˆ¬å– ({len(final_remaining_laws)}ä¸ªå›°éš¾æ³•è§„)")
                optimized_selenium_crawler = self._get_optimized_selenium_crawler()
                
                # ä½¿ç”¨ä¼˜åŒ–ç‰ˆSeleniumçš„æ‰¹é‡å¤„ç†æ–¹æ³•
                selenium_batch_results = await optimized_selenium_crawler.crawl_laws_batch(final_remaining_laws)
                
                for result in selenium_batch_results:
                    if result and result.get('success'):
                        law_name = result.get('target_name', result.get('name', ''))
                        selenium_results[law_name] = result
                        self.logger.success(f"âš¡ ä¼˜åŒ–SeleniumæˆåŠŸ: {law_name}")
                
                selenium_success_rate = len(selenium_results) / len(final_remaining_laws) * 100 if final_remaining_laws else 0
                self.logger.info(f"Seleniumé˜¶æ®µå®Œæˆ: {len(selenium_results)}/{len(final_remaining_laws)} æˆåŠŸ ({selenium_success_rate:.1f}%)")
                
            except Exception as e:
                self.logger.error(f"ä¼˜åŒ–Seleniumæ‰¹é‡çˆ¬å–å¤±è´¥: {e}")
        
        # åˆå¹¶æ‰€æœ‰ç»“æœ
        results = []
        success_count = 0
        
        for law_info in law_list:
            law_name = law_info.get('åç§°', law_info.get('name', ''))
            
            if law_name in search_based_results:
                result = search_based_results[law_name]
                result['crawler_strategy'] = 'search_based'
                results.append(result)
                success_count += 1
            elif law_name in search_engine_results:
                result = search_engine_results[law_name]
                result['crawler_strategy'] = 'search_engine'
                results.append(result)
                success_count += 1
            elif law_name in selenium_results:
                result = selenium_results[law_name]
                result['crawler_strategy'] = 'optimized_selenium'
                results.append(result)
                success_count += 1
            else:
                results.append(self._create_failed_result(law_name, "æ‰€æœ‰æ‰¹é‡ç­–ç•¥éƒ½å¤±è´¥"))
        
        total_time = time.time() - start_time
        success_rate = (success_count / total_count) * 100
        avg_time_per_law = total_time / total_count
        
        self.logger.success(f"ğŸ‰ æ‰¹é‡çˆ¬å–å®Œæˆï¼")
        self.logger.info(f"ğŸ“Š æ€»æ•°: {total_count}, æˆåŠŸ: {success_count}, æˆåŠŸç‡: {success_rate:.1f}%")
        self.logger.info(f"â±ï¸ æ€»ç”¨æ—¶: {total_time:.1f}ç§’, å¹³å‡: {avg_time_per_law:.2f}ç§’/æ³•è§„")
        self.logger.info(f"ğŸš€ ç­–ç•¥åˆ†å¸ƒ: æœç´¢å¼•æ“({len(search_engine_results)}), æ³•è§„åº“({len(search_based_results)}), Selenium({len(selenium_results)})")
        
        return results
    
    async def async_cleanup(self):
        """å¼‚æ­¥æ¸…ç†èµ„æº"""
        try:
            if self._selenium_crawler:
                self._selenium_crawler.close_driver()
                self.logger.info("Seleniumæµè§ˆå™¨å·²å…³é—­")
            if self._optimized_selenium_crawler:
                self._optimized_selenium_crawler.close_session()
                self.logger.info("ä¼˜åŒ–ç‰ˆSeleniumæµè§ˆå™¨å·²å…³é—­")
            if self._search_engine_crawler:
                try:
                    await self._search_engine_crawler.close()
                    self.logger.info("æœç´¢å¼•æ“çˆ¬è™«è¿æ¥å·²å…³é—­")
                except Exception as close_error:
                    self.logger.warning(f"å…³é—­æœç´¢å¼•æ“çˆ¬è™«è¿æ¥å¤±è´¥: {close_error}")
        except Exception as e:
            self.logger.warning(f"å¼‚æ­¥æ¸…ç†èµ„æºæ—¶å‡ºé”™: {e}")
            
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            if self._selenium_crawler:
                self._selenium_crawler.close_driver()
                self.logger.info("Seleniumæµè§ˆå™¨å·²å…³é—­")
            if self._optimized_selenium_crawler:
                self._optimized_selenium_crawler.close_session()
                self.logger.info("ä¼˜åŒ–ç‰ˆSeleniumæµè§ˆå™¨å·²å…³é—­")
            if self._search_engine_crawler:
                # åŒæ­¥è°ƒç”¨å¼‚æ­¥å…³é—­
                import asyncio
                try:
                    loop = asyncio.get_event_loop()
                    if loop.is_running():
                        # å¦‚æœäº‹ä»¶å¾ªç¯æ­£åœ¨è¿è¡Œï¼Œåˆ›å»ºä»»åŠ¡
                        loop.create_task(self._search_engine_crawler.close())
                    else:
                        # å¦‚æœäº‹ä»¶å¾ªç¯æœªè¿è¡Œï¼Œç›´æ¥æ‰§è¡Œ
                        loop.run_until_complete(self._search_engine_crawler.close())
                    self.logger.info("æœç´¢å¼•æ“çˆ¬è™«è¿æ¥å·²å…³é—­")
                except Exception as close_error:
                    self.logger.warning(f"å…³é—­æœç´¢å¼•æ“çˆ¬è™«è¿æ¥å¤±è´¥: {close_error}")
        except Exception as e:
            self.logger.warning(f"æ¸…ç†èµ„æºæ—¶å‡ºé”™: {e}")
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œç¡®ä¿èµ„æºæ¸…ç†"""
        self.cleanup()
    
    def _create_failed_result(self, law_name: str, error_message: str) -> Dict[str, Any]:
        """åˆ›å»ºå¤±è´¥ç»“æœçš„æ ‡å‡†æ ¼å¼"""
        from datetime import datetime
        return {
            'success': False,
            'name': law_name,
            'title': law_name,
            'number': '',
            'document_number': '',
            'publish_date': '',
            'valid_from': '',
            'valid_to': '',
            'office': '',
            'issuing_authority': '',
            'level': '',
            'law_level': '',
            'status': '',
            'source_url': '',
            'content': '',
            'target_name': law_name,
            'search_keyword': law_name,
            'crawl_time': datetime.now().isoformat(),
            'source': 'failed',
            'error': error_message,
            'crawler_strategy': 'failed'
        }


def create_crawler_manager():
    """åˆ›å»ºçˆ¬è™«ç®¡ç†å™¨å®ä¾‹"""
    return CrawlerManager()
        
 