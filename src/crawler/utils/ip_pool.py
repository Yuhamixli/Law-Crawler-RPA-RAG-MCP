#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½IPæ± ç®¡ç†ç³»ç»Ÿ
æ”¯æŒï¼š
1. å…è´¹ä»£ç†è‡ªåŠ¨è·å–
2. ä»£ç†å¥åº·æ£€æŸ¥
3. æ™ºèƒ½è½®æ¢ç­–ç•¥
4. å¤±è´¥è‡ªåŠ¨ç§»é™¤
5. å®æ—¶çŠ¶æ€ç›‘æ§
"""

import asyncio
import aiohttp
import random
import time
import json
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any
from loguru import logger
import threading
from concurrent.futures import ThreadPoolExecutor


class ProxyInfo:
    """ä»£ç†ä¿¡æ¯ç±»"""
    
    def __init__(self, ip: str, port: int, proxy_type: str = "http", 
                 username: str = None, password: str = None):
        self.ip = ip
        self.port = port
        self.proxy_type = proxy_type.lower()
        self.username = username
        self.password = password
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.success_count = 0
        self.failure_count = 0
        self.last_used = None
        self.last_check = None
        self.response_time = float('inf')
        self.is_alive = True
        
        # åˆ›å»ºæ—¶é—´
        self.created_at = datetime.now()
    
    @property
    def proxy_url(self) -> str:
        """è·å–ä»£ç†URL"""
        if self.username and self.password:
            return f"{self.proxy_type}://{self.username}:{self.password}@{self.ip}:{self.port}"
        else:
            return f"{self.proxy_type}://{self.ip}:{self.port}"
    
    @property
    def proxy_dict(self) -> Dict[str, str]:
        """è·å–aiohttpæ ¼å¼çš„ä»£ç†å­—å…¸"""
        return {
            'http': self.proxy_url,
            'https': self.proxy_url
        }
    
    @property
    def success_rate(self) -> float:
        """æˆåŠŸç‡"""
        total = self.success_count + self.failure_count
        return self.success_count / total if total > 0 else 0.0
    
    def mark_success(self, response_time: float = None):
        """æ ‡è®°æˆåŠŸä½¿ç”¨"""
        self.success_count += 1
        self.last_used = datetime.now()
        if response_time:
            self.response_time = min(self.response_time, response_time)
        self.is_alive = True
    
    def mark_failure(self):
        """æ ‡è®°å¤±è´¥ä½¿ç”¨"""
        self.failure_count += 1
        self.last_used = datetime.now()
        
        # å¤±è´¥ç‡è¿‡é«˜æ—¶æ ‡è®°ä¸ºæ­»äº¡
        if self.failure_count > 5 and self.success_rate < 0.3:
            self.is_alive = False
    
    def __str__(self):
        return f"{self.ip}:{self.port} (æˆåŠŸç‡: {self.success_rate:.1%}, å“åº”: {self.response_time:.2f}s)"


class FreeProxyFetcher:
    """å…è´¹ä»£ç†è·å–å™¨"""
    
    def __init__(self):
        self.logger = logger
        
        # å…è´¹ä»£ç†APIåˆ—è¡¨
        self.proxy_apis = [
            {
                "name": "ProxyList",
                "url": "https://www.proxy-list.download/api/v1/get?type=http",
                "parser": self._parse_proxylist_response
            },
            {
                "name": "FreeProxyList", 
                "url": "https://free-proxy-list.net/",
                "parser": self._parse_html_table
            },
            {
                "name": "ProxyScrape",
                "url": "https://api.proxyscrape.com/v2/?request=get&protocol=http&timeout=5000&country=all&ssl=all&anonymity=all",
                "parser": self._parse_proxyscrape_response
            }
        ]
    
    async def fetch_proxies(self, limit: int = 50) -> List[ProxyInfo]:
        """è·å–å…è´¹ä»£ç†åˆ—è¡¨"""
        all_proxies = []
        
        async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30)) as session:
            for api_info in self.proxy_apis:
                try:
                    self.logger.info(f"ä»{api_info['name']}è·å–ä»£ç†...")
                    proxies = await self._fetch_from_api(session, api_info)
                    all_proxies.extend(proxies)
                    self.logger.info(f"ä»{api_info['name']}è·å–åˆ°{len(proxies)}ä¸ªä»£ç†")
                    
                    if len(all_proxies) >= limit:
                        break
                        
                except Exception as e:
                    self.logger.warning(f"ä»{api_info['name']}è·å–ä»£ç†å¤±è´¥: {e}")
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        unique_proxies = self._deduplicate_proxies(all_proxies)
        return unique_proxies[:limit]
    
    async def _fetch_from_api(self, session: aiohttp.ClientSession, 
                            api_info: Dict) -> List[ProxyInfo]:
        """ä»å•ä¸ªAPIè·å–ä»£ç†"""
        try:
            async with session.get(api_info["url"]) as response:
                if response.status == 200:
                    content = await response.text()
                    return api_info["parser"](content)
                else:
                    self.logger.warning(f"{api_info['name']} APIè¿”å›çŠ¶æ€ç : {response.status}")
                    return []
        except Exception as e:
            self.logger.error(f"è¯·æ±‚{api_info['name']} APIå¤±è´¥: {e}")
            return []
    
    def _parse_proxylist_response(self, content: str) -> List[ProxyInfo]:
        """è§£æProxyList APIå“åº”"""
        proxies = []
        try:
            for line in content.strip().split('\n'):
                if ':' in line:
                    ip, port = line.strip().split(':')
                    proxies.append(ProxyInfo(ip, int(port)))
        except Exception as e:
            self.logger.error(f"è§£æProxyListå“åº”å¤±è´¥: {e}")
        return proxies
    
    def _parse_proxyscrape_response(self, content: str) -> List[ProxyInfo]:
        """è§£æProxyScrape APIå“åº”"""
        proxies = []
        try:
            for line in content.strip().split('\n'):
                if ':' in line:
                    ip, port = line.strip().split(':')
                    proxies.append(ProxyInfo(ip, int(port)))
        except Exception as e:
            self.logger.error(f"è§£æProxyScrapeå“åº”å¤±è´¥: {e}")
        return proxies
    
    def _parse_html_table(self, content: str) -> List[ProxyInfo]:
        """è§£æHTMLè¡¨æ ¼æ ¼å¼çš„ä»£ç†åˆ—è¡¨"""
        proxies = []
        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(content, 'html.parser')
            
            # æŸ¥æ‰¾ä»£ç†è¡¨æ ¼
            table = soup.find('table', {'id': 'proxylisttable'})
            if table:
                rows = table.find_all('tr')[1:]  # è·³è¿‡è¡¨å¤´
                for row in rows[:20]:  # åªå–å‰20ä¸ª
                    cols = row.find_all('td')
                    if len(cols) >= 2:
                        ip = cols[0].text.strip()
                        port = cols[1].text.strip()
                        if ip and port.isdigit():
                            proxies.append(ProxyInfo(ip, int(port)))
        except ImportError:
            self.logger.warning("éœ€è¦å®‰è£…BeautifulSoup4æ¥è§£æHTML: pip install beautifulsoup4")
        except Exception as e:
            self.logger.error(f"è§£æHTMLè¡¨æ ¼å¤±è´¥: {e}")
        return proxies
    
    def _deduplicate_proxies(self, proxies: List[ProxyInfo]) -> List[ProxyInfo]:
        """å»é‡ä»£ç†åˆ—è¡¨"""
        seen = set()
        unique_proxies = []
        for proxy in proxies:
            key = f"{proxy.ip}:{proxy.port}"
            if key not in seen:
                seen.add(key)
                unique_proxies.append(proxy)
        return unique_proxies


class ProxyChecker:
    """ä»£ç†æ£€æŸ¥å™¨"""
    
    def __init__(self):
        self.logger = logger
        
        # æµ‹è¯•URLåˆ—è¡¨
        self.test_urls = [
            "http://httpbin.org/ip",
            "https://api.ipify.org?format=json",
            "http://icanhazip.com/"
        ]
    
    async def check_proxy(self, proxy: ProxyInfo, timeout: float = 10.0) -> bool:
        """æ£€æŸ¥å•ä¸ªä»£ç†æ˜¯å¦å¯ç”¨"""
        try:
            start_time = time.time()
            
            async with aiohttp.ClientSession(
                connector=aiohttp.TCPConnector(ssl=False),
                timeout=aiohttp.ClientTimeout(total=timeout)
            ) as session:
                
                # éšæœºé€‰æ‹©ä¸€ä¸ªæµ‹è¯•URL
                test_url = random.choice(self.test_urls)
                
                async with session.get(test_url, proxy=proxy.proxy_url) as response:
                    if response.status == 200:
                        response_time = time.time() - start_time
                        proxy.mark_success(response_time)
                        proxy.last_check = datetime.now()
                        self.logger.debug(f"ä»£ç†æ£€æŸ¥æˆåŠŸ: {proxy.ip}:{proxy.port} ({response_time:.2f}s)")
                        return True
                    else:
                        proxy.mark_failure()
                        return False
                        
        except Exception as e:
            proxy.mark_failure()
            self.logger.debug(f"ä»£ç†æ£€æŸ¥å¤±è´¥: {proxy.ip}:{proxy.port} - {e}")
            return False
    
    async def batch_check_proxies(self, proxies: List[ProxyInfo], 
                                concurrent: int = 10) -> List[ProxyInfo]:
        """æ‰¹é‡æ£€æŸ¥ä»£ç†"""
        self.logger.info(f"å¼€å§‹æ‰¹é‡æ£€æŸ¥{len(proxies)}ä¸ªä»£ç†...")
        
        semaphore = asyncio.Semaphore(concurrent)
        
        async def check_with_semaphore(proxy):
            async with semaphore:
                return await self.check_proxy(proxy)
        
        # å¹¶å‘æ£€æŸ¥æ‰€æœ‰ä»£ç†
        tasks = [check_with_semaphore(proxy) for proxy in proxies]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç­›é€‰å¯ç”¨ä»£ç†
        working_proxies = []
        for proxy, result in zip(proxies, results):
            if result is True and proxy.is_alive:
                working_proxies.append(proxy)
        
        self.logger.info(f"ä»£ç†æ£€æŸ¥å®Œæˆ: {len(working_proxies)}/{len(proxies)} å¯ç”¨")
        return working_proxies


class SmartIPPool:
    """æ™ºèƒ½IPæ± ç®¡ç†å™¨"""
    
    def __init__(self, min_proxies: int = 5, max_proxies: int = 50):
        self.logger = logger
        self.min_proxies = min_proxies
        self.max_proxies = max_proxies
        
        # ä»£ç†æ± 
        self.proxies: List[ProxyInfo] = []
        self.current_index = 0
        self.lock = asyncio.Lock()
        
        # ç»„ä»¶
        self.fetcher = FreeProxyFetcher()
        self.checker = ProxyChecker()
        
        # çŠ¶æ€
        self.last_refresh = None
        self.refresh_interval = timedelta(hours=1)  # 1å°æ—¶åˆ·æ–°ä¸€æ¬¡
        
        # ç»Ÿè®¡
        self.total_requests = 0
        self.total_failures = 0
    
    async def initialize(self):
        """åˆå§‹åŒ–IPæ± """
        self.logger.info("åˆå§‹åŒ–IPæ± ...")
        await self.refresh_proxies()
    
    async def refresh_proxies(self):
        """åˆ·æ–°ä»£ç†æ± """
        async with self.lock:
            self.logger.info("å¼€å§‹åˆ·æ–°ä»£ç†æ± ...")
            
            # è·å–æ–°çš„ä»£ç†
            new_proxies = await self.fetcher.fetch_proxies(self.max_proxies)
            
            if new_proxies:
                # æ£€æŸ¥å¯ç”¨æ€§
                working_proxies = await self.checker.batch_check_proxies(new_proxies)
                
                # æ›´æ–°ä»£ç†æ± 
                self.proxies = working_proxies
                self.current_index = 0
                self.last_refresh = datetime.now()
                
                self.logger.success(f"ä»£ç†æ± åˆ·æ–°å®Œæˆ: {len(self.proxies)}ä¸ªå¯ç”¨ä»£ç†")
            else:
                self.logger.warning("æœªèƒ½è·å–åˆ°æ–°ä»£ç†")
    
    async def get_proxy(self) -> Optional[ProxyInfo]:
        """è·å–ä¸‹ä¸€ä¸ªå¯ç”¨ä»£ç†"""
        async with self.lock:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°
            if self._should_refresh():
                await self.refresh_proxies()
            
            # å¦‚æœæ²¡æœ‰å¯ç”¨ä»£ç†
            if not self.proxies:
                self.logger.warning("ä»£ç†æ± ä¸ºç©º")
                return None
            
            # è½®æ¢è·å–ä»£ç†
            proxy = self.proxies[self.current_index]
            self.current_index = (self.current_index + 1) % len(self.proxies)
            
            self.total_requests += 1
            return proxy
    
    async def mark_proxy_failed(self, proxy: ProxyInfo):
        """æ ‡è®°ä»£ç†å¤±è´¥"""
        proxy.mark_failure()
        self.total_failures += 1
        
        # å¦‚æœä»£ç†å¤±è´¥è¿‡å¤šï¼Œä»æ± ä¸­ç§»é™¤
        if not proxy.is_alive:
            async with self.lock:
                try:
                    self.proxies.remove(proxy)
                    self.logger.info(f"ç§»é™¤å¤±æ•ˆä»£ç†: {proxy.ip}:{proxy.port}")
                    
                    # å¦‚æœä»£ç†æ•°é‡è¿‡å°‘ï¼Œç«‹å³åˆ·æ–°
                    if len(self.proxies) < self.min_proxies:
                        self.logger.warning(f"ä»£ç†æ•°é‡ä¸è¶³({len(self.proxies)})ï¼Œå¼€å§‹ç´§æ€¥åˆ·æ–°...")
                        await self.refresh_proxies()
                        
                except ValueError:
                    pass  # ä»£ç†å·²ç»è¢«ç§»é™¤
    
    def _should_refresh(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ·æ–°ä»£ç†æ± """
        if not self.last_refresh:
            return True
        
        # æ—¶é—´é—´éš”åˆ·æ–°
        if datetime.now() - self.last_refresh > self.refresh_interval:
            return True
        
        # ä»£ç†æ•°é‡ä¸è¶³åˆ·æ–°
        if len(self.proxies) < self.min_proxies:
            return True
        
        return False
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        working_count = len([p for p in self.proxies if p.is_alive])
        
        return {
            "total_proxies": len(self.proxies),
            "working_proxies": working_count,
            "total_requests": self.total_requests,
            "total_failures": self.total_failures,
            "failure_rate": self.total_failures / self.total_requests if self.total_requests > 0 else 0.0,
            "last_refresh": self.last_refresh.isoformat() if self.last_refresh else None,
            "next_refresh": (self.last_refresh + self.refresh_interval).isoformat() if self.last_refresh else None
        }
    
    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_stats()
        
        print("\n" + "="*50)
        print("ğŸ“Š IPæ± ç»Ÿè®¡ä¿¡æ¯")
        print("="*50)
        print(f"æ€»ä»£ç†æ•°: {stats['total_proxies']}")
        print(f"å¯ç”¨ä»£ç†: {stats['working_proxies']}")
        print(f"æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
        print(f"å¤±è´¥æ¬¡æ•°: {stats['total_failures']}")
        print(f"å¤±è´¥ç‡: {stats['failure_rate']:.1%}")
        print(f"ä¸Šæ¬¡åˆ·æ–°: {stats['last_refresh']}")
        print(f"ä¸‹æ¬¡åˆ·æ–°: {stats['next_refresh']}")
        
        if self.proxies:
            print(f"\nğŸŒ å½“å‰ä»£ç†åˆ—è¡¨:")
            for i, proxy in enumerate(self.proxies[:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
                status = "âœ…" if proxy.is_alive else "âŒ"
                print(f"  {i}. {status} {proxy}")
        
        print("="*50)


# å…¨å±€IPæ± å®ä¾‹
_global_ip_pool: Optional[SmartIPPool] = None


async def get_ip_pool() -> SmartIPPool:
    """è·å–å…¨å±€IPæ± å®ä¾‹"""
    global _global_ip_pool
    
    if _global_ip_pool is None:
        _global_ip_pool = SmartIPPool()
        await _global_ip_pool.initialize()
    
    return _global_ip_pool


async def test_ip_pool():
    """æµ‹è¯•IPæ± åŠŸèƒ½"""
    print("ğŸ§ª IPæ± åŠŸèƒ½æµ‹è¯•")
    
    # åˆ›å»ºIPæ± 
    ip_pool = SmartIPPool(min_proxies=3, max_proxies=20)
    await ip_pool.initialize()
    
    # æ˜¾ç¤ºç»Ÿè®¡
    ip_pool.print_stats()
    
    # æµ‹è¯•è·å–ä»£ç†
    print("\nğŸ”„ æµ‹è¯•ä»£ç†è·å–:")
    for i in range(5):
        proxy = await ip_pool.get_proxy()
        if proxy:
            print(f"  {i+1}. è·å–ä»£ç†: {proxy.ip}:{proxy.port}")
        else:
            print(f"  {i+1}. æ— å¯ç”¨ä»£ç†")
    
    print("\nâœ… IPæ± æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    asyncio.run(test_ip_pool()) 