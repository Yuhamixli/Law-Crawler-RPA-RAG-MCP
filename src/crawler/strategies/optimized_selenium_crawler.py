#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–ç‰ˆSeleniumæ”¿åºœç½‘çˆ¬è™«
å…³é”®ä¼˜åŒ–:
1. æµè§ˆå™¨ä¼šè¯å¤ç”¨ - é¿å…é‡å¤å¯åŠ¨Chrome
2. æ™ºèƒ½ç­‰å¾…ç­–ç•¥ - æ›¿ä»£å›ºå®šsleep
3. æ‰¹é‡å¤„ç†æ¨¡å¼ - ä¸€æ¬¡ä¼šè¯å¤„ç†å¤šä¸ªæ³•è§„
4. é¢„åŠ è½½ä¼˜åŒ– - ç¼“å­˜å¸¸ç”¨é¡µé¢å…ƒç´ 
"""

import asyncio
import time
from datetime import datetime
from typing import Dict, List, Any, Optional
from urllib.parse import quote_plus

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from loguru import logger

from ..base_crawler import BaseCrawler
from ..utils.webdriver_manager import get_local_chromedriver_path


class OptimizedSeleniumCrawler(BaseCrawler):
    """ä¼˜åŒ–ç‰ˆSeleniumæ”¿åºœç½‘çˆ¬è™« - ä¼šè¯å¤ç”¨æ¨¡å¼"""
    
    def __init__(self):
        super().__init__("optimized_selenium")
        self.driver = None
        self.wait = None
        self.session_start_time = None
        self.max_session_time = 1800  # 30åˆ†é’Ÿæœ€å¤§ä¼šè¯æ—¶é—´
        self.requests_count = 0
        self.max_requests_per_session = 50  # æ¯ä¸ªä¼šè¯æœ€å¤§è¯·æ±‚æ•°
        self.processed_count = 0
        self.batch_size = 10  # æ¯ä¸ªä¼šè¯å¤„ç†çš„æ³•è§„æ•°é‡
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'browser_starts': 0,
            'total_time': 0,
            'search_time': 0,
            'detail_time': 0,
            'success_count': 0,
            'failure_count': 0
        }
    
    def setup_driver_session(self):
        """
        å»ºç«‹æµè§ˆå™¨ä¼šè¯
        å…³é”®ï¼šä¸€æ¬¡å¯åŠ¨ï¼Œå¤šæ¬¡ä½¿ç”¨ï¼Œé¿å…é¢‘ç¹é‡å¯æµè§ˆå™¨
        """
        if self.driver:
            return  # ä¼šè¯å·²å­˜åœ¨
        
        try:
            chrome_options = Options()
            
            # æé€Ÿæ¨¡å¼è®¾ç½®
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--disable-web-security')
            chrome_options.add_argument('--disable-features=VizDisplayCompositor')
            chrome_options.add_argument('--disable-extensions')
            chrome_options.add_argument('--disable-plugins')
            chrome_options.add_argument('--disable-images')  # ç¦ç”¨å›¾ç‰‡åŠ è½½
            chrome_options.add_argument('--disable-javascript')  # éƒ¨åˆ†é¡µé¢å¯ç¦ç”¨JS
            chrome_options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
            
            # ç¦ç”¨ä¸å¿…è¦çš„åŠŸèƒ½
            chrome_options.add_experimental_option('excludeSwitches', ['enable-automation'])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            # è®¾ç½®é¡µé¢åŠ è½½ç­–ç•¥
            chrome_options.add_argument('--page-load-strategy=eager')  # å¿«é€ŸåŠ è½½ç­–ç•¥
            
            # å‡å°‘èµ„æºå ç”¨
            chrome_options.add_argument('--max_old_space_size=4096')
            chrome_options.add_argument('--memory-pressure-off')
            
            # ä½¿ç”¨æœ¬åœ°ç¼“å­˜çš„ChromeDriver
            driver_path = get_local_chromedriver_path()
            if driver_path:
                service = Service(driver_path)
                self.driver = webdriver.Chrome(service=service, options=chrome_options)
                logger.info(f"ä½¿ç”¨æœ¬åœ°ç¼“å­˜çš„ChromeDriver: {driver_path}")
            else:
                # å›é€€åˆ°é»˜è®¤æ–¹å¼
                self.driver = webdriver.Chrome(options=chrome_options)
                logger.info("ä½¿ç”¨ç³»ç»ŸPATHä¸­çš„ChromeDriver")
            
            self.wait = WebDriverWait(self.driver, 10)  # æ™ºèƒ½ç­‰å¾…æœ€å¤§10ç§’
            
            # è®¾ç½®é¡µé¢åŠ è½½è¶…æ—¶
            self.driver.set_page_load_timeout(15)
            self.driver.implicitly_wait(3)
            
            self.session_start_time = time.time()
            self.stats['browser_starts'] += 1
            
            setup_time = time.time() - self.session_start_time
            logger.success(f"æµè§ˆå™¨ä¼šè¯åˆå§‹åŒ–æˆåŠŸ (è€—æ—¶: {setup_time:.2f}ç§’)")
            
        except Exception as e:
            logger.error(f"æµè§ˆå™¨ä¼šè¯åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def close_session(self):
        """å…³é—­æµè§ˆå™¨ä¼šè¯"""
        if self.driver:
            try:
                self.driver.quit()
                logger.info("æµè§ˆå™¨ä¼šè¯å·²å…³é—­")
            except Exception as e:
                logger.warning(f"å…³é—­æµè§ˆå™¨ä¼šè¯æ—¶å‡ºé”™: {e}")
            finally:
                self.driver = None
                self.wait = None
    
    async def crawl_laws_batch(self, law_names: List[str]) -> List[Dict[str, Any]]:
        """æ‰¹é‡çˆ¬å–æ³•è§„ - æ ¸å¿ƒä¼˜åŒ–æ–¹æ³•"""
        total_start = time.time()
        results = []
        
        logger.info(f"å¼€å§‹æ‰¹é‡çˆ¬å– {len(law_names)} ä¸ªæ³•è§„ (ä¼˜åŒ–æ¨¡å¼)")
        
        # å¯åŠ¨æµè§ˆå™¨ä¼šè¯
        self.setup_driver_session()
        
        try:
            for i, law_name in enumerate(law_names, 1):
                logger.info(f"[{i}/{len(law_names)}] å¤„ç†: {law_name}")
                
                try:
                    result = await self.crawl_single_law_in_session(law_name)
                    results.append(result)
                    
                    if result and result.get('success'):
                        self.stats['success_count'] += 1
                        logger.info(f"âœ… æˆåŠŸ: {law_name}")
                    else:
                        self.stats['failure_count'] += 1
                        logger.warning(f"âŒ å¤±è´¥: {law_name}")
                    
                    # æ¯å¤„ç†batch_sizeä¸ªæ³•è§„é‡å¯ä¼šè¯(é¿å…å†…å­˜æ³„æ¼)
                    self.processed_count += 1
                    if self.processed_count % self.batch_size == 0:
                        logger.info(f"è¾¾åˆ°æ‰¹æ¬¡é™åˆ¶({self.batch_size})ï¼Œé‡å¯æµè§ˆå™¨ä¼šè¯...")
                        self.close_session()
                        time.sleep(1)  # çŸ­æš‚ä¼‘æ¯
                        self.setup_driver_session()
                        
                except Exception as e:
                    logger.error(f"å¤„ç†æ³•è§„å¼‚å¸¸: {law_name} - {e}")
                    results.append(self._create_failed_result(law_name, f"å¤„ç†å¼‚å¸¸: {str(e)}"))
                    self.stats['failure_count'] += 1
        
        finally:
            self.close_session()
        
        total_time = time.time() - total_start
        self.stats['total_time'] = total_time
        
        self._log_performance_stats(len(law_names), total_time)
        
        return results
    
    async def crawl_single_law_in_session(self, law_name: str) -> Dict[str, Any]:
        """åœ¨å·²æœ‰ä¼šè¯ä¸­çˆ¬å–å•ä¸ªæ³•è§„"""
        search_start = time.time()
        
        try:
            # 1. å¿«é€Ÿæœç´¢
            search_url = self._build_search_url(law_name)
            self.driver.get(search_url)
            
            # 2. æ™ºèƒ½ç­‰å¾…æœç´¢ç»“æœ
            try:
                # ç­‰å¾…é¡µé¢å…³é”®å…ƒç´ åŠ è½½å®Œæˆ
                self.wait.until(
                    EC.any_of(
                        EC.presence_of_element_located((By.CLASS_NAME, "result")),
                        EC.presence_of_element_located((By.CLASS_NAME, "no-result")),
                        EC.text_to_be_present_in_element((By.TAG_NAME, "body"), "æ²¡æœ‰æ‰¾åˆ°")
                    )
                )
            except TimeoutException:
                logger.warning(f"æœç´¢ç»“æœé¡µé¢åŠ è½½è¶…æ—¶: {law_name}")
                return self._create_failed_result(law_name, "æœç´¢è¶…æ—¶")
            
            search_time = time.time() - search_start
            self.stats['search_time'] += search_time
            
            # 3. å¿«é€Ÿè§£ææœç´¢ç»“æœ
            page_source = self.driver.page_source
            search_results = self._parse_search_results_fast(page_source, law_name)
            
            if not search_results:
                # å¦‚æœå¯ç”¨äº†è°ƒè¯•æ¨¡å¼ï¼Œä¿å­˜è°ƒè¯•ä¿¡æ¯
                if getattr(self, 'debug_enabled', False):
                    self._save_debug_screenshot(law_name, "no_result")
                return self._create_failed_result(law_name, "æœªæ‰¾åˆ°æœç´¢ç»“æœ")
            
            # 4. è·å–è¯¦æƒ…é¡µé¢ä¿¡æ¯
            detail_start = time.time()
            detail_info = await self._get_detail_info_fast(search_results[0]['url'])
            detail_time = time.time() - detail_start
            self.stats['detail_time'] += detail_time
            
            if not detail_info:
                # å¦‚æœå¯ç”¨äº†è°ƒè¯•æ¨¡å¼ï¼Œä¿å­˜è°ƒè¯•ä¿¡æ¯
                if getattr(self, 'debug_enabled', False):
                    self._save_debug_screenshot(law_name, "detail_failed")
                return self._create_failed_result(law_name, "è¯¦æƒ…é¡µé¢è·å–å¤±è´¥")
            
            # 5. æ•´åˆç»“æœ
            result = {
                **search_results[0],
                **detail_info,
                'success': True,
                'target_name': law_name,
                'search_keyword': law_name,
                'crawl_time': datetime.now().isoformat(),
                'source': 'ä¸­å›½æ”¿åºœç½‘',
                'crawler_strategy': 'optimized_selenium'
            }
            
            return result
            
        except Exception as e:
            logger.error(f"çˆ¬å–æ³•è§„å¤±è´¥: {law_name} - {e}")
            return self._create_failed_result(law_name, f"çˆ¬å–å¼‚å¸¸: {str(e)}")
    
    def _build_search_url(self, law_name: str) -> str:
        """æ„å»ºæœç´¢URL"""
        encoded_name = quote_plus(law_name)
        return f"https://sousuo.www.gov.cn/sousuo/search.shtml?code=17da70961a7&searchWord={encoded_name}&dataTypeId=107&sign=9c1d305f-d6a7-46ba-9d42-ca7411f93ffe"
    
    def _parse_search_results_fast(self, html_content: str, target_name: str) -> List[Dict[str, Any]]:
        """å¿«é€Ÿè§£ææœç´¢ç»“æœ"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            results = []
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç»“æœ
            if "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç»“æœ" in html_content or "no-result" in html_content:
                return results
            
            # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
            links = soup.find_all('a', href=True)
            
            for link in links:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                title = link.get('title', '')
                
                # ç­›é€‰æ”¿åºœç½‘é“¾æ¥
                if 'gov.cn' in href and any(keyword in (text + title).replace(' ', '') 
                                          for keyword in target_name.replace('ï¼ˆ', '').replace('ï¼‰', '').split('ï¼ˆ')[0][:6]):
                    
                    # ç¡®ä¿å®Œæ•´URL
                    if href.startswith('http'):
                        full_url = href
                    else:
                        full_url = f"https://www.gov.cn{href}"
                    
                    results.append({
                        'name': text or title or target_name,
                        'title': title or text or target_name,
                        'url': full_url,
                        'source_url': full_url
                    })
                    
                    # åªå–ç¬¬ä¸€ä¸ªåŒ¹é…ç»“æœï¼Œæé«˜æ•ˆç‡
                    break
            
            return results
            
        except Exception as e:
            logger.error(f"è§£ææœç´¢ç»“æœå¤±è´¥: {e}")
            return []
    
    async def _get_detail_info_fast(self, detail_url: str) -> Optional[Dict[str, Any]]:
        """å¿«é€Ÿè·å–è¯¦æƒ…é¡µé¢ä¿¡æ¯"""
        try:
            self.driver.get(detail_url)
            
            # ç­‰å¾…é¡µé¢å†…å®¹åŠ è½½
            try:
                self.wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
            except TimeoutException:
                logger.warning(f"è¯¦æƒ…é¡µé¢åŠ è½½è¶…æ—¶: {detail_url}")
                return None
            
            page_source = self.driver.page_source
            return self._extract_law_details_from_html(page_source)
            
        except Exception as e:
            logger.error(f"è·å–è¯¦æƒ…é¡µé¢å¤±è´¥: {detail_url} - {e}")
            return None
    
    def _extract_law_details_from_html(self, html_content: str) -> Dict[str, Any]:
        """ä»HTMLä¸­æå–æ³•è§„è¯¦ç»†ä¿¡æ¯"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # æå–å®Œæ•´å†…å®¹
            content_div = soup.find('div', class_='pages_content') or soup.find('div', class_='TRS_Editor')
            content = content_div.get_text(strip=True) if content_div else ""
            
            # åŸºç¡€ä¿¡æ¯æå–
            result = {
                'content': content,
                'publish_date': '',
                'valid_from': '',
                'valid_to': '',
                'office': '',
                'issuing_authority': '',
                'document_number': '',
                'law_level': 'éƒ¨é—¨è§„ç« ',
                'status': 'æœ‰æ•ˆ'
            }
            
            # é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼å¿«é€Ÿæå–å…³é”®ä¿¡æ¯
            import re
            
            # æå–å‘å¸ƒæ—¶é—´
            date_patterns = [
                r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',
                r'(\d{4}-\d{1,2}-\d{1,2})',
                r'(\d{4}\.\d{1,2}\.\d{1,2})'
            ]
            
            for pattern in date_patterns:
                matches = re.findall(pattern, content[:1000])  # åªåœ¨å‰1000å­—ç¬¦ä¸­æŸ¥æ‰¾
                if matches:
                    result['publish_date'] = matches[0]
                    break
            
            # æå–æ–‡å·
            number_patterns = [
                r'ç¬¬(\d+)å·',
                r'(\d+å¹´ç¬¬\d+å·)',
                r'ä»¤\s*(\d+å·)'
            ]
            
            for pattern in number_patterns:
                matches = re.findall(pattern, content[:500])
                if matches:
                    result['document_number'] = matches[0]
                    break
            
            return result
            
        except Exception as e:
            logger.error(f"æå–æ³•è§„è¯¦æƒ…å¤±è´¥: {e}")
            return {}
    
    def _log_performance_stats(self, total_laws: int, total_time: float):
        """è®°å½•æ€§èƒ½ç»Ÿè®¡"""
        avg_time = total_time / total_laws if total_laws > 0 else 0
        success_rate = (self.stats['success_count'] / total_laws) * 100 if total_laws > 0 else 0
        
        logger.info("=" * 50)
        logger.info("ğŸ“Š ä¼˜åŒ–ç‰ˆçˆ¬è™«æ€§èƒ½ç»Ÿè®¡")
        logger.info("=" * 50)
        logger.info(f"æ€»æ³•è§„æ•°: {total_laws}")
        logger.info(f"æˆåŠŸæ•°: {self.stats['success_count']}")
        logger.info(f"å¤±è´¥æ•°: {self.stats['failure_count']}")
        logger.info(f"æˆåŠŸç‡: {success_rate:.1f}%")
        logger.info(f"æ€»è€—æ—¶: {total_time:.2f}ç§’")
        logger.info(f"å¹³å‡è€—æ—¶: {avg_time:.2f}ç§’/æ³•è§„")
        logger.info(f"æµè§ˆå™¨å¯åŠ¨æ¬¡æ•°: {self.stats['browser_starts']}")
        logger.info(f"æœç´¢æ€»è€—æ—¶: {self.stats['search_time']:.2f}ç§’")
        logger.info(f"è¯¦æƒ…è·å–æ€»è€—æ—¶: {self.stats['detail_time']:.2f}ç§’")
        
        # æ•ˆç‡æå‡è®¡ç®—
        old_avg_time = 24  # ä¹‹å‰çš„å¹³å‡è€—æ—¶
        improvement = ((old_avg_time - avg_time) / old_avg_time) * 100
        logger.info(f"æ•ˆç‡æå‡: {improvement:.1f}%")
        logger.info("=" * 50)
    
    def _create_failed_result(self, law_name: str, error_message: str) -> Dict[str, Any]:
        """åˆ›å»ºå¤±è´¥ç»“æœ"""
        return {
            'success': False,
            'name': law_name,
            'title': law_name,
            'number': '',
            'document_number': '',
            'publish_date': '',
            'valid_from': '',
            'valid_to': '',
            'office': '',
            'issuing_authority': '',
            'level': '',
            'law_level': '',
            'status': '',
            'source_url': '',
            'content': '',
            'target_name': law_name,
            'search_keyword': law_name,
            'crawl_time': datetime.now().isoformat(),
            'source': 'failed',
            'error': error_message,
            'crawler_strategy': 'optimized_selenium'
        }
    
    # å…¼å®¹ç°æœ‰æ¥å£
    async def crawl_law(self, law_name: str, law_number: str = None) -> Dict[str, Any]:
        """å•ä¸ªæ³•è§„çˆ¬å–æ¥å£(å…¼å®¹æ€§)"""
        self.setup_driver_session()
        try:
            return await self.crawl_single_law_in_session(law_name)
        finally:
            self.close_session()
    
    def close_driver(self):
        """å…¼å®¹æ€§æ–¹æ³•"""
        self.close_session()
    
    async def search(self, law_name: str, law_number: Optional[str] = None) -> List[Dict[str, Any]]:
        """æœç´¢æ³•è§„ - å®ç°æŠ½è±¡æ–¹æ³•"""
        result = await self.crawl_law(law_name, law_number)
        return [result] if result and result.get('success') else []
    
    async def get_detail(self, law_id: str) -> Dict[str, Any]:
        """è·å–æ³•è§„è¯¦æƒ… - å®ç°æŠ½è±¡æ–¹æ³•"""
        return await self._get_detail_info_fast(law_id) or {}
    
    async def download_file(self, url: str, save_path: str) -> bool:
        """ä¸‹è½½æ–‡ä»¶ - å®ç°æŠ½è±¡æ–¹æ³•"""
        try:
            if not self.driver:
                return False
            
            self.driver.get(url)
            page_source = self.driver.page_source
            
            with open(save_path, 'w', encoding='utf-8') as f:
                f.write(page_source)
            return True
        except Exception as e:
            logger.error(f"ä¸‹è½½æ–‡ä»¶å¤±è´¥: {e}")
            return False

    # ========== ä» selenium_gov_crawler è¿ç§»çš„è°ƒè¯•åŠŸèƒ½ ==========
    
    def _save_debug_screenshot(self, keyword: str, suffix: str = "debug"):
        """ä¿å­˜è°ƒè¯•æˆªå›¾å’Œé¡µé¢æºç """
        if not self.driver:
            return
            
        try:
            import os
            os.makedirs("tests/debug", exist_ok=True)
            
            # ä¿å­˜é¡µé¢æºç 
            safe_keyword = keyword.replace(' ', '_').replace('/', '_').replace('\\', '_')
            debug_file = f"tests/debug/optimized_selenium_{safe_keyword}_{suffix}.html"
            with open(debug_file, 'w', encoding='utf-8') as f:
                f.write(self.driver.page_source)
            
            # ä¿å­˜é¡µé¢æˆªå›¾
            screenshot_path = f"tests/debug/optimized_selenium_{safe_keyword}_{suffix}.png"
            self.driver.save_screenshot(screenshot_path)
            
            logger.info(f"è°ƒè¯•æ–‡ä»¶å·²ä¿å­˜: {debug_file}, {screenshot_path}")
            
        except Exception as e:
            logger.warning(f"ä¿å­˜è°ƒè¯•æ–‡ä»¶å¤±è´¥: {e}")
    
    def enable_debug_mode(self, enabled: bool = True):
        """å¯ç”¨æˆ–ç¦ç”¨è°ƒè¯•æ¨¡å¼"""
        self.debug_enabled = getattr(self, 'debug_enabled', False)
        self.debug_enabled = enabled
        if enabled:
            logger.info("è°ƒè¯•æ¨¡å¼å·²å¯ç”¨ - å°†ä¿å­˜é¡µé¢æˆªå›¾å’Œæºç ")
        else:
            logger.info("è°ƒè¯•æ¨¡å¼å·²ç¦ç”¨") 