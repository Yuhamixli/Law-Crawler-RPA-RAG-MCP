#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
åŸºäºæœç´¢å¼•æ“çš„æ”¿åºœç½‘æ³•è§„çˆ¬è™« - é‡æ„ç‰ˆ
é€šè¿‡DuckDuckGoå’ŒBingæœç´¢ site:gov.cn çš„æ³•è§„æ–‡æ¡£
"""

import asyncio
import aiohttp
import json
import re
from datetime import datetime
from typing import Dict, List, Optional, Any
from urllib.parse import quote_plus, urljoin, urlparse, quote
from bs4 import BeautifulSoup
from loguru import logger
import random
import time

# æ·»åŠ Seleniumç›¸å…³å¯¼å…¥
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager

from ..base_crawler import BaseCrawler
from ..utils.ip_pool import get_ip_pool, SmartIPPool
from ..utils.enhanced_proxy_pool import get_enhanced_proxy_pool, EnhancedProxyPool
from ..utils.anti_detection_enhanced import get_anti_detection, EnhancedAntiDetection, ResponseAnalysisResult, AntiCrawlerLevel
from config.settings import get_settings


class AntiDetectionManager:
    """ååçˆ¬æ£€æµ‹ç®¡ç†å™¨"""
    
    def __init__(self):
        self.logger = logger
        
        # ç°åœ¨ä½¿ç”¨çœŸå®çš„ä»£ç†æ± 
        self.enhanced_proxy_pool: Optional[EnhancedProxyPool] = None
        self.ip_pool: Optional[SmartIPPool] = None
        
        # User-Agentæ± 
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
        ]
        
        # è¯·æ±‚å»¶è¿Ÿé…ç½® - æé€Ÿä¼˜åŒ–ç‰ˆ
        self.delay_config = {
            'min_delay': 0.5,  # æœ€å°å»¶è¿Ÿå¤§å¹…å‡å°‘
            'max_delay': 1.5,  # æœ€å¤§å»¶è¿Ÿå¤§å¹…å‡å°‘
            'retry_delay': 3.0,  # é‡è¯•å»¶è¿Ÿå‡å°‘
            'timeout': 10.0,  # è¯·æ±‚è¶…æ—¶å‡å°‘
        }
        
        # å¤±è´¥è®¡æ•°
        self.failure_counts = {}
    
    async def initialize_proxy_pools(self):
        """åˆå§‹åŒ–ä»£ç†æ± """
        settings = get_settings()
        
        try:
            # 1. ä¼˜å…ˆä½¿ç”¨enhanced_proxy_pool
            if settings.proxy_pool.enabled:
                self.enhanced_proxy_pool = await get_enhanced_proxy_pool()
                self.logger.success("Enhancedä»£ç†æ± åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            self.logger.warning(f"Enhancedä»£ç†æ± åˆå§‹åŒ–å¤±è´¥: {e}")
        
        try:
            # 2. IPæ± ä½œä¸ºå¤‡ç”¨
            if settings.ip_pool.enabled:
                from ..utils.ip_pool import get_ip_pool
                self.ip_pool = await get_ip_pool()
                self.logger.success("IPæ± åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            self.logger.warning(f"IPæ± åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def get_proxy(self):
        """è·å–ä»£ç† - å®Œå…¨ç¦ç”¨ï¼Œå¼ºåˆ¶ç›´è¿"""
        # å®Œå…¨ç¦ç”¨ä»£ç†ï¼Œä½¿ç”¨ç›´è¿æ¨¡å¼æé«˜ç¨³å®šæ€§
        return None
        
        # # åŸä»£ç†è·å–é€»è¾‘å·²ç¦ç”¨
        # try:
        #     # ä¼˜å…ˆä½¿ç”¨Enhancedä»£ç†æ± 
        #     if self.enhanced_proxy_pool:
        #         proxy_info = await self.enhanced_proxy_pool.get_proxy(prefer_paid=True)
        #         if proxy_info:
        #             self.logger.debug(f"ä½¿ç”¨Enhancedä»£ç†: {proxy_info.name}")
        #             return proxy_info.proxy_url
        # 
        #     # å¤‡ç”¨IPæ± 
        #     if self.ip_pool:
        #         proxy_info = await self.ip_pool.get_proxy()
        #         if proxy_info:
        #             self.logger.debug(f"ä½¿ç”¨IPæ± ä»£ç†: {proxy_info.ip}:{proxy_info.port}")
        #             return proxy_info.proxy_url
        # 
        # except Exception as e:
        #     self.logger.debug(f"ä»£ç†è·å–å¤±è´¥: {e}")
        # 
        # return None
    
    def mark_proxy_failed(self, proxy_url: str):
        """æ ‡è®°ä»£ç†å¤±è´¥"""
        # è¿™é‡Œå¯ä»¥é€šçŸ¥ä»£ç†æ± æŸä¸ªä»£ç†å¤±è´¥äº†
        # å®ç°ä¼šæ¯”è¾ƒå¤æ‚ï¼Œæš‚æ—¶ç®€åŒ–
        self.logger.debug(f"æ ‡è®°ä»£ç†å¤±è´¥: {proxy_url}")
    
    def get_random_user_agent(self) -> str:
        """è·å–éšæœºUser-Agent"""
        return random.choice(self.user_agents)
    
    async def smart_delay(self, operation_type: str = "default"):
        """æ™ºèƒ½å»¶è¿Ÿ"""
        base_delay = random.uniform(self.delay_config['min_delay'], self.delay_config['max_delay'])
        
        # æ ¹æ®æ“ä½œç±»å‹è°ƒæ•´å»¶è¿Ÿ
        if operation_type == "search":
            base_delay *= 1.2  # æœç´¢æ“ä½œç¨é•¿å»¶è¿Ÿ
        elif operation_type == "retry":
            base_delay = self.delay_config['retry_delay']
            
        await asyncio.sleep(base_delay)
    
    def get_random_headers(self) -> Dict[str, str]:
        """ç”Ÿæˆéšæœºè¯·æ±‚å¤´"""
        return {
            'User-Agent': self.get_random_user_agent(),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0',
        }
    
    def get_headers(self) -> Dict[str, str]:
        """è·å–è¯·æ±‚å¤´ - get_random_headersçš„åˆ«å"""
        return self.get_random_headers()


class SeleniumSearchEngine:
    """Seleniumæœç´¢å¼•æ“æ“ä½œå™¨"""
    
    def __init__(self, anti_detection: AntiDetectionManager):
        self.anti_detection = anti_detection
        self.logger = logger
        self.driver = None
        
    async def setup_driver(self) -> webdriver.Chrome:
        """è®¾ç½®Chromeé©±åŠ¨ - ä¼˜åŒ–ç‰ˆ"""
        try:
            # å¯¼å…¥æœ¬åœ°ChromeDriverç®¡ç†åŠŸèƒ½
            from ..utils.webdriver_manager import get_local_chromedriver_path
            
            options = Options()
            
            # åŸºç¡€åæ£€æµ‹é…ç½®
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-blink-features=AutomationControlled')
            options.add_experimental_option("excludeSwitches", ["enable-automation"])
            options.add_experimental_option('useAutomationExtension', False)
            
            # æ€§èƒ½ä¼˜åŒ–é…ç½®
            options.add_argument('--headless')  # æ— å¤´æ¨¡å¼æå‡é€Ÿåº¦
            options.add_argument('--disable-gpu')
            options.add_argument('--disable-web-security')
            options.add_argument('--disable-features=VizDisplayCompositor')
            options.add_argument('--disable-extensions')
            options.add_argument('--disable-plugins')
            options.add_argument('--disable-images')
            options.add_argument('--disable-javascript')  # ç¦ç”¨JSæå‡é€Ÿåº¦
            options.add_argument('--no-first-run')
            options.add_argument('--disable-default-apps')
            options.add_argument('--disable-background-timer-throttling')
            options.add_argument('--disable-renderer-backgrounding')
            options.add_argument('--disable-backgrounding-occluded-windows')
            
            # éšæœºçª—å£å¤§å°
            width = random.randint(1200, 1920)
            height = random.randint(800, 1080)
            options.add_argument(f'--window-size={width},{height}')
            
            # éšæœºUser-Agent
            user_agent = self.anti_detection.get_random_user_agent()
            options.add_argument(f'--user-agent={user_agent}')
            
            # ç¦ç”¨å›¾ç‰‡å’Œåª’ä½“åŠ è½½ä»¥æå‡é€Ÿåº¦
            prefs = {
                "profile.managed_default_content_settings.images": 2,
                "profile.default_content_setting_values.notifications": 2,
                "profile.default_content_setting_values.media_stream": 2,
                "profile.managed_default_content_settings.media_stream": 2
            }
            options.add_experimental_option("prefs", prefs)
            
            # ä»£ç†é…ç½® - ä½¿ç”¨æ–°çš„ä»£ç†æ± 
            proxy_url = self.anti_detection.get_proxy()  # ä¿®å¤ï¼šç§»é™¤é”™è¯¯çš„await
            if proxy_url:
                # è§£æä»£ç†URL
                if proxy_url.startswith('http://'):
                    proxy_address = proxy_url.replace('http://', '')
                elif proxy_url.startswith('https://'):
                    proxy_address = proxy_url.replace('https://', '')
                elif proxy_url.startswith('socks5://'):
                    proxy_address = proxy_url.replace('socks5://', '')
                    options.add_argument(f'--proxy-server=socks5://{proxy_address}')
                else:
                    proxy_address = proxy_url
                
                if not proxy_url.startswith('socks5://'):
                    options.add_argument(f'--proxy-server={proxy_address}')
                    
                self.logger.info(f"Seleniumä½¿ç”¨ä»£ç†: {proxy_address}")
            
            # ä½¿ç”¨æœ¬åœ°ç¼“å­˜çš„ChromeDriver
            driver_path = get_local_chromedriver_path()
            if driver_path:
                service = Service(driver_path)
                driver = webdriver.Chrome(service=service, options=options)
                self.logger.info(f"ä½¿ç”¨æœ¬åœ°ç¼“å­˜çš„ChromeDriver: {driver_path}")
            else:
                # å›é€€åˆ°é»˜è®¤æ–¹å¼
                driver = webdriver.Chrome(options=options)
                self.logger.info("ä½¿ç”¨ç³»ç»ŸPATHä¸­çš„ChromeDriver")
            
            # è®¾ç½®è¶…æ—¶
            driver.set_page_load_timeout(10)
            driver.implicitly_wait(5)  # éšå¼ç­‰å¾…5ç§’
            
            # æ‰§è¡Œåæ£€æµ‹è„šæœ¬
            driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            self.logger.info("Selenium Chromeé©±åŠ¨åˆå§‹åŒ–æˆåŠŸï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰")
            return driver
            
        except Exception as e:
            self.logger.error(f"Seleniumé©±åŠ¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return None
    
    async def search_with_selenium(self, query: str, engine: str = "baidu") -> List[Dict[str, Any]]:
        """ä½¿ç”¨Seleniumè¿›è¡Œæœç´¢"""
        if not self.driver:
            self.driver = await self.setup_driver()
            if not self.driver:
                return []
        
        try:
            results = []
            
            if engine == "baidu":
                results = await self._search_baidu_selenium(query)
            elif engine == "bing":
                results = await self._search_bing_selenium(query)
            elif engine == "google":
                results = await self._search_google_selenium(query)
                
            return results
            
        except Exception as e:
            self.logger.error(f"Seleniumæœç´¢å¤±è´¥ ({engine}): {e}")
            return []
    
    async def _search_baidu_selenium(self, query: str) -> List[Dict[str, Any]]:
        """ä½¿ç”¨Seleniumæœç´¢ç™¾åº¦"""
        try:
            search_url = f"https://www.baidu.com/s?wd={quote(query + ' site:gov.cn')}"
            self.logger.info(f"Seleniumç™¾åº¦æœç´¢: {search_url}")
            
            self.driver.get(search_url)
            
            # ç­‰å¾…æœç´¢ç»“æœåŠ è½½ - æé€Ÿä¼˜åŒ–
            await asyncio.sleep(random.uniform(0.5, 1.0))
            
            # æŸ¥æ‰¾æœç´¢ç»“æœ
            results = []
            result_elements = self.driver.find_elements(By.CSS_SELECTOR, 'div.result')
            
            self.logger.debug(f"æ‰¾åˆ° {len(result_elements)} ä¸ªæœç´¢ç»“æœå…ƒç´ ")
            
            for i, element in enumerate(result_elements[:3], 1):  # åªå–å‰3ä¸ªç»“æœï¼Œæé«˜é€Ÿåº¦
                try:
                    # 1. é¦–å…ˆå°è¯•ä»muå±æ€§è·å–çœŸå®URLï¼ˆæœ€å¯é ï¼‰
                    real_url = element.get_attribute('mu')
                    
                    # 2. è·å–æ ‡é¢˜
                    title = ""
                    title_selectors = ['h3', 'h3 a', '.t', '.t a']
                    for title_sel in title_selectors:
                        try:
                            title_elem = element.find_element(By.CSS_SELECTOR, title_sel)
                            title = title_elem.text.strip()
                            if title:
                                break
                        except:
                            continue
                    
                    # 3. å¦‚æœæ²¡æœ‰muå±æ€§ï¼Œå°è¯•ä»é“¾æ¥è·å–
                    if not real_url:
                        try:
                            link_elem = element.find_element(By.CSS_SELECTOR, 'h3 a, .t a')
                            link_url = link_elem.get_attribute('href')
                            
                            # å¤„ç†ç™¾åº¦é‡å®šå‘URL
                            if link_url and 'baidu.com/link?' in link_url:
                                try:
                                    # å°è¯•ä»ç™¾åº¦é“¾æ¥ä¸­æå–çœŸå®URL
                                    import urllib.parse
                                    parsed = urllib.parse.parse_qs(urllib.parse.urlparse(link_url).query)
                                    if 'url' in parsed:
                                        real_url = parsed['url'][0]
                                    else:
                                        real_url = link_url  # ä¿ç•™ç™¾åº¦é“¾æ¥ä½œä¸ºå¤‡ç”¨
                                except:
                                    real_url = link_url
                            else:
                                real_url = link_url
                        except:
                            continue
                    
                    # 4. æå–æè¿°
                    description = ""
                    desc_selectors = [
                        '.c-abstract', 
                        '.c-span9', 
                        '.summary-text_560AW',
                        'span[class*="summary"]',
                        'span[class*="abstract"]',
                        '.content-gap_3jlQr'
                    ]
                    for desc_sel in desc_selectors:
                        try:
                            desc_elem = element.find_element(By.CSS_SELECTOR, desc_sel)
                            description = desc_elem.text.strip()
                            if description:
                                break
                        except:
                            continue
                    
                    # 5. éªŒè¯æ˜¯å¦æ˜¯æœ‰æ•ˆçš„æ”¿åºœç½‘ç»“æœ
                    if real_url and title:
                        # æ£€æŸ¥URLæˆ–æ ‡é¢˜ä¸­æ˜¯å¦åŒ…å«gov.cn
                        if 'gov.cn' in real_url or 'gov.cn' in title.lower():
                            results.append({
                                'title': title,
                                'url': real_url,
                                'snippet': description,
                                'source': 'Baidu_Selenium',
                                'rank': i
                            })
                            self.logger.debug(f"æ‰¾åˆ°æœ‰æ•ˆç»“æœ {i}: {title} -> {real_url[:100]}...")
                        else:
                            self.logger.debug(f"è·³è¿‡éæ”¿åºœç½‘ç»“æœ {i}: {title}")
                    else:
                        self.logger.debug(f"è·³è¿‡æ— æ•ˆç»“æœ {i}: title={bool(title)}, url={bool(real_url)}")
                        
                except Exception as e:
                    self.logger.debug(f"è§£æç»“æœå…ƒç´  {i} æ—¶å‡ºé”™: {e}")
                    continue
            
            self.logger.info(f"Seleniumç™¾åº¦æ‰¾åˆ° {len(results)} ä¸ªæœ‰æ•ˆæ”¿åºœç½‘ç»“æœ")
            return results
            
        except Exception as e:
            self.logger.error(f"Seleniumç™¾åº¦æœç´¢å¤±è´¥: {e}")
            return []
    
    async def _search_bing_selenium(self, query: str) -> List[Dict[str, Any]]:
        """ä½¿ç”¨Seleniumæœç´¢Bing"""
        try:
            search_url = f"https://www.bing.com/search?q={quote(query + ' site:gov.cn')}"
            self.logger.info(f"Selenium Bingæœç´¢: {search_url}")
            
            self.driver.get(search_url)
            
            # ç­‰å¾…æœç´¢ç»“æœåŠ è½½ - æé€Ÿä¼˜åŒ–
            await asyncio.sleep(random.uniform(0.2, 0.5))
            
            # æŸ¥æ‰¾Bingæœç´¢ç»“æœ
            results = []
            result_elements = self.driver.find_elements(By.CSS_SELECTOR, '.b_algo, li.b_algo')
            
            for element in result_elements[:3]:  # åªå–å‰3ä¸ªç»“æœï¼Œæé«˜é€Ÿåº¦
                try:
                    # æå–æ ‡é¢˜å’Œé“¾æ¥
                    title_elem = element.find_element(By.CSS_SELECTOR, 'h2 a, .b_title a')
                    title = title_elem.text.strip()
                    url = title_elem.get_attribute('href')
                    
                    # æå–æè¿°
                    description = ""
                    try:
                        desc_elem = element.find_element(By.CSS_SELECTOR, '.b_caption p, .b_snippet')
                        description = desc_elem.text.strip()
                    except:
                        pass
                    
                    if 'gov.cn' in url:
                        results.append({
                            'title': title,
                            'url': url,
                            'snippet': description,
                            'source': 'Bing_Selenium'
                        })
                        
                except Exception as e:
                    continue
            
            self.logger.info(f"Selenium Bingæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
            return results
            
        except Exception as e:
            self.logger.error(f"Selenium Bingæœç´¢å¤±è´¥: {e}")
            return []
    
    def close(self):
        """å…³é—­é©±åŠ¨"""
        if self.driver:
            try:
                self.driver.quit()
                self.logger.info("Seleniumé©±åŠ¨å·²å…³é—­")
            except:
                pass
            self.driver = None


class SearchEngineCrawler(BaseCrawler):
    """æœç´¢å¼•æ“çˆ¬è™« - å¢å¼ºWAFå¯¹æŠ—ç‰ˆæœ¬"""
    
    def __init__(self, **config):
        super().__init__(source_name="æœç´¢å¼•æ“çˆ¬è™«")
        self.name = "æœç´¢å¼•æ“çˆ¬è™«"
        # æ·»åŠ loggerå±æ€§ï¼Œé˜²æ­¢AttributeError
        self.logger = logger
        
        self.search_engines = [
            "https://www.google.com/search?q=",
            "https://www.bing.com/search?q=",
            "https://duckduckgo.com/?q="
        ]
        
        # WAFå¯¹æŠ—é…ç½®
        self.max_waf_retries = 3
        self.waf_retry_delay = 5  # ç§’
        self.proxy_rotation_threshold = 2  # è¿ç»­å¤±è´¥2æ¬¡å°±è½®æ¢IP
        self.consecutive_failures = 0
        
        # åˆå§‹åŒ–ä»£ç†æ± 
        self.enhanced_proxy_pool = None
        self.ip_pool = None
        self.current_proxy = None
        self._initialized = False
        
        self.logger.info(f"ğŸ” {self.name} åˆå§‹åŒ–å®Œæˆ - å¢å¼ºWAFå¯¹æŠ—")
        
        # åˆå§‹åŒ–åæ£€æµ‹ç®¡ç†å™¨
        self.anti_detection = AntiDetectionManager()
        
        # åˆå§‹åŒ–Seleniumæœç´¢å¼•æ“
        self.selenium_engine = SeleniumSearchEngine(self.anti_detection)
        
        # åˆå§‹åŒ–æ ‡å¿—
        self.initialized = False
        
        # IPæ± 
        self.ip_pool: Optional[SmartIPPool] = None
        
        # æœç´¢å¼•æ“é…ç½® - ä¼˜åŒ–ç­–ç•¥é¡ºåºï¼Œä¼˜å…ˆä½¿ç”¨ç›´è¿
        self.search_engines = [
            {
                "name": "DuckDuckGo",
                "enabled": True,
                "priority": 1,  # æœ€é«˜ä¼˜å…ˆçº§ï¼šå¿«é€ŸHTTPæœç´¢
                "api_url": "https://html.duckduckgo.com/html/",
                "method": "requests",
                "use_proxy": False  # ä¼˜å…ˆç›´è¿
            },
            {
                "name": "Bing",
                "enabled": True,
                "priority": 2,  # æ¬¡ä¼˜å…ˆçº§ï¼šBing HTTPæœç´¢
                "api_url": "https://www.bing.com/search",
                "method": "requests",
                "use_proxy": False  # ä¼˜å…ˆç›´è¿
            },
            {
                "name": "Baidu_Selenium",
                "enabled": False,  # æš‚æ—¶ç¦ç”¨Seleniumï¼Œå¤ªæ…¢
                "priority": 3,  # å¤‡ç”¨ç­–ç•¥ï¼šSeleniumç™¾åº¦
                "method": "selenium"
            },
            {
                "name": "Bing_Selenium", 
                "enabled": False,  # æš‚æ—¶ç¦ç”¨Seleniumï¼Œå¤ªæ…¢
                "priority": 4,  # å¤‡ç”¨ç­–ç•¥ï¼šSelenium Bing
                "method": "selenium"
            }
        ]
        
        # è¶…æ—¶æ§åˆ¶é…ç½® - å¤§å¹…ä¼˜åŒ–è¶…æ—¶æ—¶é—´
        self.timeout_config = {
            'single_law_timeout': 15.0,  # å•ä¸ªæ³•è§„æ€»è¶…æ—¶æ—¶é—´
            'single_request_timeout': 8.0,  # å•ä¸ªè¯·æ±‚è¶…æ—¶æ—¶é—´
            'selenium_timeout': 10.0,  # Seleniumæ“ä½œè¶…æ—¶æ—¶é—´
            'selenium_search_timeout': 12.0,  # Seleniumæœç´¢è¶…æ—¶æ—¶é—´
        }
        
        # ååçˆ¬é…ç½® - æé€Ÿä¼˜åŒ–
        self.anti_detection_config = {
            "min_delay": 0.1,  # æœ€å°å»¶è¿Ÿæé€Ÿä¼˜åŒ–
            "max_delay": 0.3,  # æœ€å¤§å»¶è¿Ÿæé€Ÿä¼˜åŒ–
            "retry_delay": 1.0,  # é‡è¯•å»¶è¿Ÿæé€Ÿä¼˜åŒ–
            "max_retries": 1,  # å‡å°‘é‡è¯•æ¬¡æ•°
            "rotate_headers": True,  # è½®æ¢è¯·æ±‚å¤´
            "use_proxy": False  # ä¼˜å…ˆç›´è¿ï¼Œä»£ç†ä½œä¸ºå¤‡ç”¨
        }
        
        # è¯·æ±‚å¤´ - æ¨¡æ‹Ÿæ›´çœŸå®çš„æµè§ˆå™¨è¡Œä¸º
        self.headers = self._get_random_headers()
    
    def _get_random_headers(self) -> Dict[str, str]:
        """è·å–éšæœºçš„æµè§ˆå™¨å¤´ä¿¡æ¯ï¼Œé¿å…è¢«è¯†åˆ«"""
        import random
        
        # å¤šç§çœŸå®çš„User-Agent
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36 Edg/121.0.0.0',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'
        ]
        
        return {
            'User-Agent': random.choice(user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,en-US;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'cross-site',
            'Sec-Fetch-User': '?1',
            'Upgrade-Insecure-Requests': '1'
        }
    
    async def _ensure_session(self):
        """ç¡®ä¿aiohttpä¼šè¯å­˜åœ¨"""
        if self.session is None or self.session.closed:
            timeout = aiohttp.ClientTimeout(total=30, connect=10)
            connector = aiohttp.TCPConnector(limit=10, ttl_dns_cache=300)
            self.session = aiohttp.ClientSession(
                headers=self.headers,
                timeout=timeout,
                connector=connector
            )
    
    async def _ensure_initialized(self):
        """ç¡®ä¿çˆ¬è™«å·²åˆå§‹åŒ– - æŒ‰éœ€åˆå§‹åŒ–ä»£ç†æ± """
        if not self.initialized:
            # ä¸ç«‹å³åˆå§‹åŒ–ä»£ç†æ± ï¼Œç­‰åˆ°éœ€è¦æ—¶å†åˆå§‹åŒ–
            self.initialized = True
    
    async def _lazy_init_proxy_pools(self):
        """å»¶è¿Ÿåˆå§‹åŒ–ä»£ç†æ±  - åªåœ¨ç›´è¿å¤±è´¥æ—¶è°ƒç”¨"""
        if self.enhanced_proxy_pool is None and self.ip_pool is None:
            await self.anti_detection.initialize_proxy_pools()
            # å°†åˆå§‹åŒ–åçš„ä»£ç†æ± å¼•ç”¨èµ‹å€¼ç»™å½“å‰å®ä¾‹
            self.enhanced_proxy_pool = self.anti_detection.enhanced_proxy_pool
    
    async def _ensure_ip_pool(self):
        """ç¡®ä¿IPæ± å­˜åœ¨ - ä¼˜åŒ–ç‰ˆï¼Œå‡å°‘æ£€æŸ¥æ•°é‡"""
        if self.ip_pool is None:
            try:
                # åªåœ¨çœŸæ­£éœ€è¦æ—¶æ‰åˆå§‹åŒ–IPæ± ï¼Œå¹¶é™åˆ¶æ£€æŸ¥æ•°é‡
                from ..utils.ip_pool import get_ip_pool
                self.ip_pool = await get_ip_pool(max_check=10)  # åªæ£€æŸ¥10ä¸ªä»£ç†ï¼Œæé«˜é€Ÿåº¦
                self.logger.info("IPæ± åˆå§‹åŒ–å®Œæˆï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰")
            except Exception as e:
                self.logger.warning(f"IPæ± åˆå§‹åŒ–å¤±è´¥: {e}")
    
    async def _get_proxy_for_request(self, force_proxy: bool = False):
        """è·å–ç”¨äºè¯·æ±‚çš„ä»£ç† - ä¼˜åŒ–ç‰ˆ"""
        # å¦‚æœä¸å¼ºåˆ¶ä½¿ç”¨ä»£ç†ï¼Œå…ˆè¿”å›Noneï¼ˆç›´è¿ï¼‰
        if not force_proxy:
            return None
        
        await self._ensure_initialized()
        
        # å»¶è¿Ÿåˆå§‹åŒ–ä»£ç†æ± 
        await self._lazy_init_proxy_pools()
        
        # ä¼˜å…ˆä½¿ç”¨enhanced_proxy_pool
        proxy_url = await self.anti_detection.get_proxy()
        if proxy_url:
            return proxy_url
        
        # å¤‡ç”¨IPæ± ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰
        try:
            await self._ensure_ip_pool()
            if self.ip_pool:
                proxy = await self.ip_pool.get_proxy()
                if proxy:
                    self.logger.debug(f"ä½¿ç”¨IPæ± ä»£ç†: {proxy.ip}:{proxy.port}")
                    return proxy.proxy_url
        except Exception as e:
            self.logger.debug(f"è·å–ä»£ç†å¤±è´¥: {e}")
        return None
    
    async def search_law_via_engines(self, law_name: str) -> List[Dict[str, Any]]:
        """é€šè¿‡æœç´¢å¼•æ“æŸ¥æ‰¾æ³•è§„"""
        await self._ensure_session()
        
        # æ„å»ºæœç´¢æŸ¥è¯¢
        search_queries = self._build_search_queries(law_name)
        
        all_results = []
        
        # æŒ‰ä¼˜å…ˆçº§å°è¯•æœç´¢å¼•æ“
        search_engines = sorted(
            [engine for engine in self.search_engines if engine['enabled']], 
            key=lambda x: x['priority']
        )
        
        for query in search_queries:
            self.logger.info(f"æœç´¢å¼•æ“æŸ¥è¯¢: {query}")
            
            for engine in search_engines:
                engine_name = engine['name']
                self.logger.debug(f"å°è¯•{engine_name}æœç´¢...")
                
                results = []
                if engine_name == 'DuckDuckGo':
                    results = await self._search_duckduckgo(query)
                elif engine_name == 'Bing':
                    results = await self._search_bing(query)
                elif engine_name == 'Baidu_Selenium':
                    results = await self.selenium_engine.search_with_selenium(query, 'baidu')
                elif engine_name == 'Bing_Selenium':
                    results = await self.selenium_engine.search_with_selenium(query, 'bing')
                
                if results:
                    self.logger.success(f"{engine_name}æœç´¢æˆåŠŸï¼Œæ‰¾åˆ°{len(results)}ä¸ªç»“æœ")
                    all_results.extend(results)
                    break  # æ‰¾åˆ°ç»“æœå°±åœæ­¢å½“å‰æŸ¥è¯¢
                else:
                    self.logger.debug(f"{engine_name}æœç´¢æ— ç»“æœ")
            
            if all_results:
                break  # æ‰¾åˆ°ç»“æœå°±åœæ­¢æ‰€æœ‰æŸ¥è¯¢
        
        # è¿‡æ»¤å’Œæ’åºç»“æœ
        filtered_results = self._filter_and_rank_results(all_results, law_name)
        return filtered_results[:5]  # è¿”å›å‰5ä¸ªæœ€ç›¸å…³çš„ç»“æœ
    
    def _build_search_queries(self, law_name: str) -> List[str]:
        """æ„å»ºæœç´¢æŸ¥è¯¢åˆ—è¡¨"""
        queries = []
        
        # 1. ä¼˜å…ˆï¼šå»æ‰æ‹¬å·å†…å®¹ï¼ˆæ›´å®¹æ˜“æ‰¾åˆ°ç»“æœï¼‰
        clean_name = re.sub(r'[ï¼ˆ(].*?[ï¼‰)]', '', law_name).strip()
        if clean_name != law_name:
            queries.append(f'"{clean_name}" site:gov.cn')
        
        # 2. åŸå§‹åç§° + site:gov.cn
        queries.append(f'"{law_name}" site:gov.cn')
        
        # 3. ä¸ä½¿ç”¨å¼•å·çš„æœç´¢ï¼ˆæœ‰æ—¶å¼•å·ä¼šé™åˆ¶ç»“æœï¼‰
        queries.append(f'{law_name} site:gov.cn')
        
        # 4. æ·»åŠ "åŠæ³•"ã€"è§„å®š"ç­‰åç¼€å˜ä½“
        base_name = re.sub(r'(åŠæ³•|è§„å®š|æ¡ä¾‹|å®æ–½ç»†åˆ™|ç®¡ç†åŠæ³•|æš‚è¡ŒåŠæ³•|è¯•è¡ŒåŠæ³•)$', '', clean_name).strip()
        if base_name != clean_name:
            for suffix in ['åŠæ³•', 'ç®¡ç†åŠæ³•', 'è§„å®š']:
                variant = f'{base_name}{suffix}'
                if variant != law_name:
                    queries.append(f'"{variant}" site:gov.cn')
        
        # 5. æå–å…³é”®è¯æœç´¢
        keywords = self._extract_keywords(law_name)
        if len(keywords) >= 2:
            keyword_query = ' '.join(keywords[:3]) + ' site:gov.cn'
            queries.append(keyword_query)
        
        # 6. ç‰¹å®šäºæ”¿åºœç½‘ç«™çš„æœç´¢
        queries.append(f'{law_name} site:www.gov.cn')
        queries.append(f'{law_name} ä½å»ºéƒ¨ site:gov.cn')
        
        return queries
    
    def _extract_keywords(self, law_name: str) -> List[str]:
        """æå–æ³•è§„åç§°çš„å…³é”®è¯"""
        # ç§»é™¤å¸¸è§åç¼€
        clean_name = re.sub(r'[ï¼ˆ(].*?[ï¼‰)]', '', law_name)
        clean_name = re.sub(r'(åŠæ³•|è§„å®š|æ¡ä¾‹|å®æ–½ç»†åˆ™|ç®¡ç†åŠæ³•|æš‚è¡ŒåŠæ³•|è¯•è¡ŒåŠæ³•)$', '', clean_name)
        
        # åˆ†è¯ - ç®€å•çš„ä¸­æ–‡åˆ†è¯
        keywords = []
        
        # æå–é‡è¦è¯æ±‡
        important_patterns = [
            r'(é£Ÿå“|è¯å“|åŒ»ç–—|å»ºç­‘|å·¥ç¨‹|äº¤é€š|ç¯å¢ƒ|è´¨é‡|å®‰å…¨|æ ‡å‡†|è®¡é‡|ç‰¹ç§è®¾å¤‡)',
            r'(æ‹›æ ‡|æŠ•æ ‡|é‡‡è´­|ç›‘ç£|ç®¡ç†|å®¡æŸ¥|éªŒæ”¶|æ£€æµ‹|è®¤è¯)',
            r'(ä¼ä¸š|å…¬å¸|æœºæ„|å•ä½|è¡Œä¸š|é¢†åŸŸ)',
            r'(å›½å®¶|ä¸­åäººæ°‘å…±å’Œå›½|éƒ¨é—¨|æ”¿åºœ)'
        ]
        
        for pattern in important_patterns:
            matches = re.findall(pattern, clean_name)
            keywords.extend(matches)
        
        # å¦‚æœå…³é”®è¯å¤ªå°‘ï¼ŒæŒ‰å­—ç¬¦åˆ†ç»„
        if len(keywords) < 2:
            # 3-4å­—ç¬¦çš„è¯ç»„
            for i in range(0, len(clean_name)-2):
                word = clean_name[i:i+3]
                if len(word) == 3 and word not in keywords:
                    keywords.append(word)
        
        return keywords[:5]  # è¿”å›å‰5ä¸ªå…³é”®è¯
    
    async def _search_duckduckgo(self, query: str, max_results: int = 10) -> List[Dict[str, str]]:
        """DuckDuckGoæœç´¢ - ä»…ç›´è¿æ¨¡å¼"""
        self.logger.debug("å°è¯•DuckDuckGoç›´è¿æœç´¢...")
        
        try:
            # ä»…ä½¿ç”¨ç›´è¿æ¨¡å¼
            async with aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=10),
                headers=self.anti_detection.get_headers()
            ) as session:
                params = {
                    'q': query,
                    'format': 'json',
                    'no_redirect': '1',
                    'no_html': '1',
                    'skip_disambig': '1'
                }
                
                url = "https://api.duckduckgo.com"
                
                async with session.get(url, params=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        results = []
                        
                        # è§£æå³æ—¶ç­”æ¡ˆ
                        if data.get('AbstractURL'):
                            results.append({
                                'title': data.get('AbstractText', ''),
                                'url': data.get('AbstractURL', ''),
                                'snippet': data.get('AbstractText', '')
                            })
                        
                        # è§£æç›¸å…³ä¸»é¢˜
                        for topic in data.get('RelatedTopics', []):
                            if isinstance(topic, dict) and topic.get('FirstURL'):
                                results.append({
                                    'title': topic.get('Text', ''),
                                    'url': topic.get('FirstURL', ''),
                                    'snippet': topic.get('Text', '')
                                })
                        
                        if results:
                            self.logger.success(f"DuckDuckGoç›´è¿æˆåŠŸï¼Œæ‰¾åˆ°{len(results)}ä¸ªç»“æœ")
                            return results[:max_results]
                    
                    self.logger.debug(f"DuckDuckGo APIå“åº”çŠ¶æ€: {response.status}")
                    
        except Exception as e:
            self.logger.debug(f"DuckDuckGoç›´è¿å¼‚å¸¸: {e}")
        
        # ç›´è¿å¤±è´¥ï¼Œä¸å†å°è¯•ä»£ç†æœç´¢
        self.logger.debug("DuckDuckGoæœç´¢å¤±è´¥ï¼Œè·³è¿‡ä»£ç†æ¨¡å¼")
        return []
    
    async def _search_bing(self, query: str, max_results: int = 10) -> List[Dict[str, str]]:
        """Bingæœç´¢ - ä»…ç›´è¿æ¨¡å¼"""
        self.logger.debug("å°è¯•Bingç›´è¿æœç´¢...")
        
        try:
            # ä»…ä½¿ç”¨ç›´è¿æ¨¡å¼
            async with aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=10),
                headers=self.anti_detection.get_headers()
            ) as session:
                params = {
                    'q': query,
                    'count': max_results,
                    'offset': 0,
                    'mkt': 'zh-CN'
                }
                
                url = "https://www.bing.com/search"
                
                async with session.get(url, params=params) as response:
                    if response.status == 200:
                        html = await response.text()
                        results = self._parse_bing_results(html)
                        
                        if results:
                            self.logger.success(f"Bingç›´è¿æˆåŠŸï¼Œæ‰¾åˆ°{len(results)}ä¸ªç»“æœ")
                            return results[:max_results]
                    
                    self.logger.debug(f"Bingå“åº”çŠ¶æ€: {response.status}")
                    
        except Exception as e:
            self.logger.debug(f"Bingç›´è¿å¼‚å¸¸: {e}")
        
        # ç›´è¿å¤±è´¥ï¼Œä¸å†å°è¯•ä»£ç†æœç´¢
        self.logger.debug("Bingæœç´¢å¤±è´¥ï¼Œè·³è¿‡ä»£ç†æ¨¡å¼")
        return []
    
    # Googleæœç´¢å·²ç§»é™¤ - åœ¨å›½å†…è®¿é—®ä¸ç¨³å®š

    def _parse_duckduckgo_results(self, html: str) -> List[Dict[str, Any]]:
        """è§£æDuckDuckGoæœç´¢ç»“æœ"""
        results = []
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # DuckDuckGoç»“æœé€‰æ‹©å™¨
            result_items = soup.find_all('div', class_='result')
            
            for item in result_items:
                try:
                    # æå–æ ‡é¢˜
                    title_elem = item.find('a', class_='result__a')
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    href = title_elem.get('href', '')
                    
                    # å¤„ç†DuckDuckGoé‡å®šå‘URL
                    real_url = self._extract_real_url_from_duckduckgo(href)
                    
                    # ç¡®ä¿æ˜¯gov.cnåŸŸå
                    if 'gov.cn' not in real_url:
                        continue
                    
                    # æå–æ‘˜è¦
                    snippet_elem = item.find('a', class_='result__snippet')
                    snippet = snippet_elem.get_text(strip=True) if snippet_elem else ""
                    
                    results.append({
                        'title': title,
                        'url': real_url,
                        'snippet': snippet,
                        'source': 'DuckDuckGo'
                    })
                    
                except Exception as e:
                    self.logger.debug(f"è§£æDuckDuckGoç»“æœé¡¹å¤±è´¥: {e}")
                    continue
            
            self.logger.debug(f"DuckDuckGoæ‰¾åˆ° {len(results)} ä¸ªç»“æœé¡¹")
            
        except Exception as e:
            self.logger.error(f"è§£æDuckDuckGoç»“æœå¤±è´¥: {e}")
        
        return results
    
    def _extract_real_url_from_duckduckgo(self, duckduckgo_url: str) -> str:
        """ä»DuckDuckGoé‡å®šå‘URLä¸­æå–çœŸå®URL"""
        try:
            # DuckDuckGoçš„é‡å®šå‘URLæ ¼å¼:
            # //duckduckgo.com/l/?uddg=https%3A%2F%2Fwww.gov.cn%2F...&rut=...
            
            if 'duckduckgo.com/l/' in duckduckgo_url:
                # æå–uddgå‚æ•°
                import urllib.parse
                
                # ç¡®ä¿URLæœ‰åè®®
                if duckduckgo_url.startswith('//'):
                    duckduckgo_url = 'https:' + duckduckgo_url
                
                parsed = urllib.parse.urlparse(duckduckgo_url)
                query_params = urllib.parse.parse_qs(parsed.query)
                
                if 'uddg' in query_params:
                    # è§£ç çœŸå®URL
                    real_url = urllib.parse.unquote(query_params['uddg'][0])
                    return real_url
            
            # å¦‚æœä¸æ˜¯é‡å®šå‘URLï¼Œç›´æ¥è¿”å›
            return duckduckgo_url
            
        except Exception as e:
            self.logger.debug(f"è§£æDuckDuckGoé‡å®šå‘URLå¤±è´¥: {duckduckgo_url} - {e}")
            return duckduckgo_url
    
    def _parse_google_results(self, html: str) -> List[Dict[str, Any]]:
        """è§£æGoogleæœç´¢ç»“æœ"""
        results = []
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # Googleç»“æœé€‰æ‹©å™¨
            result_items = soup.find_all('div', class_='g')
            
            for item in result_items:
                try:
                    # æå–æ ‡é¢˜å’Œé“¾æ¥
                    title_elem = item.find('h3')
                    if not title_elem:
                        continue
                    
                    link_elem = title_elem.find_parent('a')
                    if not link_elem:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    href = link_elem.get('href', '')
                    
                    # è°ƒè¯•ï¼šè®°å½•æ‰€æœ‰æ‰¾åˆ°çš„é“¾æ¥
                    self.logger.debug(f"Googleå‘ç°é“¾æ¥: {title[:50]}... -> {href}")
                    
                    # ç¡®ä¿æ˜¯gov.cnåŸŸå
                    if 'gov.cn' not in href:
                        self.logger.debug(f"è·³è¿‡égov.cné“¾æ¥: {href}")
                        continue
                    
                    # æå–æè¿°
                    desc_elem = item.find('span', class_='aCOpRe')
                    if not desc_elem:
                        desc_elem = item.find('div', class_='VwiC3b')
                    
                    description = desc_elem.get_text(strip=True) if desc_elem else ""
                    
                    # è¿‡æ»¤PDFå’Œå…¶ä»–ä¸åˆé€‚çš„æ–‡ä»¶
                    if self._should_skip_url(href, title):
                        continue
                    
                    results.append({
                        'title': title,
                        'url': href,
                        'snippet': description,
                        'source': 'Google'
                    })
                    
                except Exception as e:
                    self.logger.debug(f"è§£æGoogleç»“æœé¡¹å¤±è´¥: {e}")
                    continue
            
            self.logger.debug(f"Googleæ‰¾åˆ° {len(results)} ä¸ªç»“æœé¡¹")
            
        except Exception as e:
            self.logger.warning(f"è§£æGoogleç»“æœå¤±è´¥: {e}")
        
        return results
    
    def _should_skip_url(self, url: str, title: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è·³è¿‡è¿™ä¸ªURL"""
        url_lower = url.lower()
        title_lower = title.lower()
        
        # è·³è¿‡PDFæ–‡ä»¶
        if url_lower.endswith('.pdf') or 'pdf' in url_lower:
            return True
        
        # è·³è¿‡ä¸‹è½½é“¾æ¥å’Œé™„ä»¶
        if any(keyword in url_lower for keyword in ['download', 'attachment', 'file', '.doc', '.docx']):
            return True
        
        # è·³è¿‡æ˜æ˜¾ä¸ç›¸å…³çš„é¡µé¢
        if any(keyword in title_lower for keyword in ['é¦–é¡µ', 'å¯¼èˆª', 'æœç´¢', 'ç™»å½•', 'æ³¨å†Œ']):
            return True
        
        return False
    
    def _parse_bing_results(self, html: str) -> List[Dict[str, Any]]:
        """è§£æBingæœç´¢ç»“æœ"""
        results = []
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # å¤šç§Bingç»“æœé€‰æ‹©å™¨
            result_selectors = [
                'li.b_algo',           # æ ‡å‡†ç»“æœ
                'div.b_algo',          # å¤‡ç”¨é€‰æ‹©å™¨
                'li[class*="algo"]',   # æ¨¡ç³ŠåŒ¹é…
                'div[class*="algo"]'   # æ¨¡ç³ŠåŒ¹é…
            ]
            
            result_items = []
            for selector in result_selectors:
                items = soup.select(selector)
                if items:
                    result_items = items
                    self.logger.debug(f"ä½¿ç”¨Bingé€‰æ‹©å™¨: {selector}, æ‰¾åˆ° {len(items)} ä¸ªç»“æœ")
                    break
            
            if not result_items:
                # å¦‚æœæ²¡æ‰¾åˆ°æ ‡å‡†ç»“æœï¼Œå°è¯•æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
                all_links = soup.find_all('a', href=True)
                gov_links = [link for link in all_links if 'gov.cn' in link.get('href', '')]
                self.logger.debug(f"å¤‡ç”¨æ–¹æ¡ˆï¼šæ‰¾åˆ° {len(gov_links)} ä¸ªgov.cné“¾æ¥")
                
                for link in gov_links[:5]:  # åªå–å‰5ä¸ª
                    href = link.get('href', '')
                    title = link.get_text(strip=True)
                    if title and len(title) > 10:  # è¿‡æ»¤æ‰å¤ªçŸ­çš„æ ‡é¢˜
                        results.append({
                            'title': title,
                            'url': href,
                            'snippet': '',
                            'source': 'Bing'
                        })
            else:
                for item in result_items:
                    try:
                        # æå–æ ‡é¢˜å’Œé“¾æ¥
                        title_elem = item.find('h2') or item.find('h3') or item.find('a')
                        if not title_elem:
                            continue
                        
                        link_elem = title_elem.find('a') if title_elem.name != 'a' else title_elem
                        if not link_elem:
                            continue
                        
                        title = link_elem.get_text(strip=True)
                        href = link_elem.get('href', '')
                        
                        # è°ƒè¯•ï¼šè®°å½•æ‰€æœ‰æ‰¾åˆ°çš„é“¾æ¥
                        self.logger.debug(f"Bingå‘ç°é“¾æ¥: {title[:50]}... -> {href}")
                        
                        # ç¡®ä¿æ˜¯gov.cnåŸŸå
                        if 'gov.cn' not in href:
                            self.logger.debug(f"è·³è¿‡égov.cné“¾æ¥: {href}")
                            continue
                        
                        # æå–æ‘˜è¦
                        snippet_elem = item.find('div', class_='b_caption') or item.find('p')
                        snippet = ""
                        if snippet_elem:
                            snippet = snippet_elem.get_text(strip=True)
                        
                        results.append({
                            'title': title,
                            'url': href,
                            'snippet': snippet,
                            'source': 'Bing'
                        })
                        
                    except Exception as e:
                        self.logger.debug(f"è§£æBingç»“æœé¡¹å¤±è´¥: {e}")
                        continue
            
            self.logger.debug(f"Bingæ‰¾åˆ° {len(results)} ä¸ªç»“æœé¡¹")
            
        except Exception as e:
            self.logger.error(f"è§£æBingç»“æœå¤±è´¥: {e}")
        
        return results
    
    def _parse_baidu_results(self, html: str) -> List[Dict[str, Any]]:
        """è§£æç™¾åº¦æœç´¢ç»“æœ"""
        results = []
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # ç™¾åº¦ç»“æœé€‰æ‹©å™¨ - å¤šç§å¯èƒ½çš„é€‰æ‹©å™¨
            selectors = [
                'div.result',  # æ ‡å‡†ç»“æœ
                'div[class*="result"]',  # åŒ…å«resultçš„class
                'div.c-container',  # æ–°ç‰ˆç™¾åº¦
                'div[tpl]'  # æœ‰tplå±æ€§çš„div
            ]
            
            result_items = []
            for selector in selectors:
                items = soup.select(selector)
                if items:
                    result_items = items
                    break
            
            for item in result_items:
                try:
                    # æå–æ ‡é¢˜å’Œé“¾æ¥ - å¤šç§å¯èƒ½çš„é€‰æ‹©å™¨
                    title_elem = (
                        item.find('h3') or 
                        item.find('a', class_='t') or
                        item.find('a', attrs={'data-click': True}) or
                        item.find('a')
                    )
                    
                    if not title_elem:
                        continue
                    
                    # å¦‚æœtitle_elemæ˜¯aæ ‡ç­¾ï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™æŸ¥æ‰¾å…¶ä¸­çš„aæ ‡ç­¾
                    if title_elem.name == 'a':
                        link_elem = title_elem
                        title = title_elem.get_text(strip=True)
                    else:
                        link_elem = title_elem.find('a')
                        title = title_elem.get_text(strip=True)
                    
                    if not link_elem:
                        continue
                    
                    href = link_elem.get('href', '')
                    
                    # å¤„ç†ç™¾åº¦é‡å®šå‘é“¾æ¥
                    if 'baidu.com/link?' in href:
                        # å°è¯•æå–çœŸå®URL
                        import urllib.parse
                        try:
                            parsed = urllib.parse.urlparse(href)
                            params = urllib.parse.parse_qs(parsed.query)
                            if 'url' in params:
                                href = urllib.parse.unquote(params['url'][0])
                        except:
                            pass
                    
                    # ç¡®ä¿æ˜¯gov.cnåŸŸå
                    if 'gov.cn' not in href:
                        continue
                    
                    # æå–æè¿°
                    desc_elem = (
                        item.find('div', class_='c-abstract') or
                        item.find('div', class_='c-span9') or
                        item.find('span', class_='content-right_8Zs40')
                    )
                    
                    description = desc_elem.get_text(strip=True) if desc_elem else ""
                    
                    # è¿‡æ»¤ä¸åˆé€‚çš„é“¾æ¥
                    if self._should_skip_url(href, title):
                        continue
                    
                    results.append({
                        'title': title,
                        'url': href,
                        'snippet': description,
                        'source': 'Baidu'
                    })
                    
                except Exception as e:
                    self.logger.debug(f"è§£æç™¾åº¦ç»“æœé¡¹å¤±è´¥: {e}")
                    continue
            
            self.logger.debug(f"ç™¾åº¦æ‰¾åˆ° {len(results)} ä¸ªç»“æœé¡¹")
            
        except Exception as e:
            self.logger.error(f"è§£æç™¾åº¦ç»“æœå¤±è´¥: {e}")
        
        return results
    
    def _parse_sogou_results(self, html: str) -> List[Dict[str, Any]]:
        """è§£ææœç‹—æœç´¢ç»“æœ"""
        results = []
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # æœç‹—ç»“æœé€‰æ‹©å™¨
            result_items = soup.find_all('div', class_='vrwrap')
            
            for item in result_items:
                try:
                    # æå–æ ‡é¢˜å’Œé“¾æ¥
                    title_elem = item.find('h3')
                    if not title_elem:
                        continue
                    
                    link_elem = title_elem.find('a')
                    if not link_elem:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    href = link_elem.get('href', '')
                    
                    # ç¡®ä¿æ˜¯gov.cnåŸŸå
                    if 'gov.cn' not in href:
                        continue
                    
                    # æå–æè¿°
                    desc_elem = item.find('div', class_='str_info')
                    description = desc_elem.get_text(strip=True) if desc_elem else ""
                    
                    # è¿‡æ»¤ä¸åˆé€‚çš„é“¾æ¥
                    if self._should_skip_url(href, title):
                        continue
                    
                    results.append({
                        'title': title,
                        'url': href,
                        'snippet': description,
                        'source': 'Sogou'
                    })
                    
                except Exception as e:
                    self.logger.debug(f"è§£ææœç‹—ç»“æœé¡¹å¤±è´¥: {e}")
                    continue
            
            self.logger.debug(f"æœç‹—æ‰¾åˆ° {len(results)} ä¸ªç»“æœé¡¹")
            
        except Exception as e:
            self.logger.error(f"è§£ææœç‹—ç»“æœå¤±è´¥: {e}")
        
        return results
    
    def _filter_and_rank_results(self, results: List[Dict[str, Any]], law_name: str) -> List[Dict[str, Any]]:
        """è¿‡æ»¤å’Œæ’åºæœç´¢ç»“æœ"""
        if not results:
            return []
        
        # å…ˆè¿‡æ»¤æ‰ä¸åˆé€‚çš„é“¾æ¥
        filtered_results = []
        for result in results:
            url = result.get('url', '').lower()
            title = result.get('title', '').lower()
            
            # è·³è¿‡PDFæ–‡ä»¶
            if url.endswith('.pdf') or 'pdf' in url:
                self.logger.debug(f"è·³è¿‡PDFé“¾æ¥: {url}")
                continue
            
            # è·³è¿‡ä¸‹è½½é“¾æ¥å’Œé™„ä»¶
            if any(keyword in url for keyword in ['download', 'attachment', 'file', '.doc', '.docx']):
                self.logger.debug(f"è·³è¿‡ä¸‹è½½é“¾æ¥: {url}")
                continue
            
            # è·³è¿‡æ˜æ˜¾ä¸ç›¸å…³çš„é¡µé¢ - ä½†è¦ç¡®ä¿ä¸è¯¯æ€æ­£ç¡®çš„æ³•è§„
            irrelevant_keywords = [
                'é¦–é¡µ', 'å¯¼èˆª', 'æœç´¢', 'ç™»å½•', 'æ³¨å†Œ', 'ç›®å½•', 'ç´¢å¼•'
            ]
            
            # å¯¹äº"æ¸…å•"ç±»é¡µé¢ï¼Œéœ€è¦æ›´ç²¾ç¡®çš„åˆ¤æ–­
            if any(keyword in title for keyword in irrelevant_keywords):
                self.logger.debug(f"è·³è¿‡ä¸ç›¸å…³é¡µé¢: {title}")
                continue
            
            # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæ˜¯"æ£€æŸ¥äº‹é¡¹æ¸…å•"ç­‰æ˜æ˜¾ä¸æ˜¯æ³•è§„æœ¬èº«çš„é¡µé¢
            if ('æ£€æŸ¥äº‹é¡¹' in title or 'æ¶‰ä¼æ£€æŸ¥' in title or 'å·¥ä½œæ¸…å•' in title or 'è´£ä»»æ¸…å•' in title) and \
               not any(core_word in title for core_word in ['æ‹›æ ‡æŠ•æ ‡ç®¡ç†åŠæ³•', 'æ–½å·¥æ‹›æ ‡æŠ•æ ‡']):
                self.logger.debug(f"è·³è¿‡æ£€æŸ¥æ¸…å•é¡µé¢: {title}")
                continue
            
            filtered_results.append(result)
        
        # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
        scored_results = []
        clean_law_name = re.sub(r'[ï¼ˆ(].*?[ï¼‰)]', '', law_name).lower()
        
        for result in filtered_results:
            score = 0
            title = result['title'].lower()
            snippet = result['snippet'].lower()
            url = result['url'].lower()
            
            # æ ‡é¢˜åŒ¹é…åŠ åˆ† - æ›´ç²¾ç¡®çš„åŒ¹é…
            title_clean = re.sub(r'[ï¼ˆ(].*?[ï¼‰)]', '', title).strip()
            
            # å®Œå…¨åŒ¹é…ï¼ˆå»æ‰æ‹¬å·åï¼‰
            if clean_law_name == title_clean:
                score += 25
            # é«˜åº¦åŒ¹é…ï¼ˆç›®æ ‡åœ¨æ ‡é¢˜ä¸­ï¼‰
            elif clean_law_name in title:
                score += 20
            # æ ‡é¢˜åœ¨ç›®æ ‡ä¸­ï¼ˆå¯èƒ½æ˜¯ç®€ç§°ï¼‰
            elif title_clean in clean_law_name and len(title_clean) > 6:
                score += 15
            
            # å…³é”®è¯åŒ¹é… - æå–æ›´å¤šå…³é”®è¯è¿›è¡ŒåŒ¹é…
            keywords = self._extract_keywords(law_name)
            title_keyword_matches = 0
            for keyword in keywords[:5]:  # æ£€æŸ¥å‰5ä¸ªå…³é”®è¯
                if keyword.lower() in title:
                    title_keyword_matches += 1
                    score += 2
                if keyword.lower() in snippet:
                    score += 1
            
            # å…³é”®è¯åŒ¹é…åº¦å¥–åŠ±
            if title_keyword_matches >= 3:
                score += 5  # å¤šä¸ªå…³é”®è¯åŒ¹é…å¥–åŠ±
            
            # ç‰¹æ®Šå¥–åŠ±ï¼šå¦‚æœæ ‡é¢˜åŒ…å«æ³•è§„çš„æ ¸å¿ƒåç§°ï¼ˆå»æ‰ä¿®è®¢å¹´ä»½ï¼‰
            core_name = re.sub(r'[ï¼ˆ(].*?[ï¼‰)]', '', law_name).replace('ä¸­åäººæ°‘å…±å’Œå›½', '').strip()
            if core_name and len(core_name) > 4 and core_name in title:
                score += 8
            
            # URLæƒå¨æ€§åŠ åˆ† - ä¼˜å…ˆä¸­å¤®æ”¿åºœç½‘ç«™
            if 'www.gov.cn' in url:  # ä¸­å›½æ”¿åºœç½‘ï¼ˆä¸­å¤®ï¼‰
                score += 15
            elif 'gov.cn' in url and not any(city in url for city in ['yueyang', 'beijing', 'shanghai', 'guangzhou', 'shenzhen']):
                score += 10  # å…¶ä»–gov.cnä½†éåœ°æ–¹æ”¿åºœ
            elif 'gov.cn' in url:  # åœ°æ–¹æ”¿åºœç½‘ç«™
                score += 5
            
            # URLç‰¹å¾åŠ åˆ†
            if 'gongbao' in url:  # æ”¿åºœå…¬æŠ¥
                score += 8
            elif 'zhengce' in url:  # æ”¿ç­–æ–‡ä»¶
                score += 6
            elif 'content' in url:  # å†…å®¹é¡µé¢
                score += 4
            
            # ä¼˜å…ˆé€‰æ‹©HTMLé¡µé¢
            if any(path in url for path in ['content', 'zhengce', 'gongbao', 'flcaw']):
                score += 3
            
            # åŒ…å«å¹´ä»½ä¿¡æ¯
            if re.search(r'20\d{2}', title + snippet):
                score += 1
            
            scored_results.append({
                **result,
                'relevance_score': score
            })
        
        # æŒ‰åˆ†æ•°æ’åº
        scored_results.sort(key=lambda x: x['relevance_score'], reverse=True)
        
        # å»é‡ - æ ¹æ®URL
        seen_urls = set()
        unique_results = []
        for result in scored_results:
            url_key = result['url'].split('?')[0]  # å»æ‰æŸ¥è¯¢å‚æ•°
            if url_key not in seen_urls:
                seen_urls.add(url_key)
                unique_results.append(result)
        
        return unique_results
    
    async def get_law_detail_from_url(self, url: str) -> Dict[str, Any]:
        """ä»URLè·å–æ³•è§„è¯¦ç»†ä¿¡æ¯"""
        await self._ensure_session()
        
        try:
            # æ£€æŸ¥æ˜¯å¦æ˜¯PDFæ–‡ä»¶
            if url.lower().endswith('.pdf'):
                self.logger.warning(f"è·³è¿‡PDFæ–‡ä»¶: {url}")
                return {}
            
            async with self.session.get(url) as response:
                if response.status == 200:
                    # æ£€æŸ¥Content-Type
                    content_type = response.headers.get('content-type', '').lower()
                    if 'application/pdf' in content_type:
                        self.logger.warning(f"è·³è¿‡PDFå†…å®¹: {url}")
                        return {}
                    
                    # å°è¯•è·å–æ–‡æœ¬å†…å®¹ï¼Œå¤„ç†ç¼–ç é—®é¢˜
                    try:
                        html = await response.text()
                    except UnicodeDecodeError:
                        # å¦‚æœUTF-8è§£ç å¤±è´¥ï¼Œå°è¯•å…¶ä»–ç¼–ç 
                        try:
                            content = await response.read()
                            # å°è¯•å¸¸è§çš„ä¸­æ–‡ç¼–ç 
                            for encoding in ['gb2312', 'gbk', 'gb18030', 'utf-8', 'latin1']:
                                try:
                                    html = content.decode(encoding)
                                    self.logger.debug(f"ä½¿ç”¨ç¼–ç  {encoding} æˆåŠŸè§£ç : {url}")
                                    break
                                except:
                                    continue
                            else:
                                self.logger.warning(f"æ— æ³•è§£ç é¡µé¢å†…å®¹: {url}")
                                return {}
                        except Exception as decode_error:
                            self.logger.warning(f"è§£ç å¤±è´¥: {url} - {decode_error}")
                            return {}
                    
                    return self._extract_law_details_from_html(html, url)
                else:
                    self.logger.warning(f"è·å–è¯¦æƒ…é¡µé¢å¤±è´¥: {url} - HTTP {response.status}")
                    
        except Exception as e:
            self.logger.error(f"è·å–è¯¦æƒ…é¡µé¢å¼‚å¸¸: {url} - {e}")
        
        return {}
    
    def _extract_law_details_from_html(self, html: str, url: str) -> Dict[str, Any]:
        """ä»HTMLæå–æ³•è§„è¯¦ç»†ä¿¡æ¯"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # æå–å®Œæ•´å†…å®¹
            content_selectors = [
                'div.pages_content',
                'div.TRS_Editor',
                'div.content',
                'div.article_content',
                'div.main_content'
            ]
            
            content = ""
            for selector in content_selectors:
                content_div = soup.select_one(selector)
                if content_div:
                    content = content_div.get_text(strip=True)
                    break
            
            # å¦‚æœæ²¡æ‰¾åˆ°ä¸“é—¨çš„å†…å®¹åŒºåŸŸï¼Œè·å–bodyæ–‡æœ¬
            if not content:
                body = soup.find('body')
                content = body.get_text(strip=True) if body else ""
            
            # åŸºç¡€ä¿¡æ¯
            result = {
                'content': content[:5000],  # é™åˆ¶å†…å®¹é•¿åº¦
                'source_url': url,
                'publish_date': '',
                'valid_from': '',
                'valid_to': '',
                'office': '',
                'issuing_authority': '',
                'document_number': '',
                'law_level': 'éƒ¨é—¨è§„ç« ',
                'status': 'æœ‰æ•ˆ'
            }
            
            # æ­£åˆ™æå–å…³é”®ä¿¡æ¯ - ç¡®ä¿reæ¨¡å—åœ¨å±€éƒ¨ä½œç”¨åŸŸå¯ç”¨
            import re
            
            # 1. æå–å®æ–½æ—¥æœŸ/æ–½è¡Œæ—¥æœŸ - ä¼˜å…ˆçº§æœ€é«˜
            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰"è‡ªå‘å¸ƒä¹‹æ—¥èµ·æ–½è¡Œ"çš„æƒ…å†µ
            if re.search(r'æœ¬åŠæ³•è‡ªå‘å¸ƒä¹‹æ—¥èµ·æ–½è¡Œ|è‡ªå‘å¸ƒä¹‹æ—¥èµ·æ–½è¡Œ', content, re.IGNORECASE):
                self.logger.debug("å‘ç°'è‡ªå‘å¸ƒä¹‹æ—¥èµ·æ–½è¡Œ'ï¼Œå®æ–½æ—¥æœŸå°†è®¾ä¸ºå‘å¸ƒæ—¥æœŸ")
                result['implement_from_publish'] = True
            else:
                # æ­£å¸¸æå–å®æ–½æ—¥æœŸ
                implement_patterns = [
                    r'æœ¬åŠæ³•è‡ª(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)èµ·æ–½è¡Œ',
                    r'æœ¬è§„å®šè‡ª(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)èµ·æ–½è¡Œ',
                    r'æœ¬æ¡ä¾‹è‡ª(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)èµ·æ–½è¡Œ',
                    r'è‡ª(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)èµ·æ–½è¡Œ',
                    r'å®æ–½æ—¥æœŸ[ï¼š:](\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',
                    r'æ–½è¡Œæ—¥æœŸ[ï¼š:](\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',
                    r'ç”Ÿæ•ˆæ—¥æœŸ[ï¼š:](\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',
                    # æ”¯æŒæ¨ªçº¿æ ¼å¼
                    r'æœ¬åŠæ³•è‡ª(\d{4}-\d{1,2}-\d{1,2})èµ·æ–½è¡Œ',
                    r'æœ¬è§„å®šè‡ª(\d{4}-\d{1,2}-\d{1,2})èµ·æ–½è¡Œ', 
                    r'æœ¬æ¡ä¾‹è‡ª(\d{4}-\d{1,2}-\d{1,2})èµ·æ–½è¡Œ',
                    r'è‡ª(\d{4}-\d{1,2}-\d{1,2})èµ·æ–½è¡Œ'
                ]
                
                for pattern in implement_patterns:
                    matches = re.findall(pattern, content, re.IGNORECASE)
                    if matches:
                        result['valid_from'] = matches[0]
                        self.logger.debug(f"æå–åˆ°å®æ–½æ—¥æœŸ: {matches[0]}")
                        break
            
            # 2. æå–å‘å¸ƒæ—¥æœŸ - å¢å¼ºç‰ˆï¼Œå¤„ç†å¤æ‚çš„ä¿®æ­£æƒ…å†µ
            # é¦–å…ˆå°è¯•æå–å¤æ‚çš„å‘å¸ƒå’Œä¿®æ­£ä¿¡æ¯
            # ä¿®æ”¹æ­£åˆ™è¡¨è¾¾å¼ä»¥æ•è·æ‰€æœ‰ä¿®æ­£ä¿¡æ¯
            complex_publish_pattern = r'ï¼ˆ(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥).*?(ç¬¬\d+å·)å‘å¸ƒ.*?æ ¹æ®.*?(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥).*?(ç¬¬\d+å·).*?ä¿®æ­£.*?æ ¹æ®.*?(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥).*?(ç¬¬\d+å·).*?ä¿®æ­£'
            complex_matches = re.findall(complex_publish_pattern, content[:2000], re.DOTALL)
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¤šæ¬¡ä¿®æ­£ï¼Œå°è¯•å•æ¬¡ä¿®æ­£
            if not complex_matches:
                simple_revision_pattern = r'ï¼ˆ(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥).*?(ç¬¬\d+å·)å‘å¸ƒ.*?æ ¹æ®(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥).*?(ç¬¬\d+å·).*?ä¿®æ­£'
                simple_matches = re.findall(simple_revision_pattern, content[:2000], re.DOTALL)
                if simple_matches:
                    # è½¬æ¢ä¸ºå¤æ‚åŒ¹é…æ ¼å¼ï¼ˆæ·»åŠ ç©ºçš„ç¬¬äºŒæ¬¡ä¿®æ­£ï¼‰
                    original_date, original_number, revision_date, revision_number = simple_matches[-1]
                    complex_matches = [(original_date, original_number, revision_date, revision_number, '', '')]
            
            if complex_matches:
                # å¤„ç†å¤æ‚æƒ…å†µï¼šæœ‰åŸå§‹å‘å¸ƒæ—¥æœŸå’Œä¿®æ­£æ—¥æœŸ
                match = complex_matches[-1]  # å–æœ€åä¸€ä¸ªåŒ¹é…
                if len(match) == 6:  # å¤šæ¬¡ä¿®æ­£
                    original_date, original_number, first_revision_date, first_revision_number, latest_date, latest_number = match
                    result['publish_date'] = original_date
                    result['document_number'] = original_number
                    result['latest_revision_date'] = latest_date if latest_date else first_revision_date
                    result['latest_revision_number'] = latest_number if latest_number else first_revision_number
                    self.logger.debug(f"æå–åˆ°å¤æ‚å‘å¸ƒä¿¡æ¯ - åŸå§‹: {original_date} {original_number}, æœ€æ–°ä¿®æ­£: {result['latest_revision_date']} {result['latest_revision_number']}")
                else:  # å•æ¬¡ä¿®æ­£
                    original_date, original_number, latest_date, latest_number = match[:4]
                    result['publish_date'] = original_date
                    result['document_number'] = original_number
                    result['latest_revision_date'] = latest_date
                    result['latest_revision_number'] = latest_number
                    self.logger.debug(f"æå–åˆ°å‘å¸ƒä¿¡æ¯ - åŸå§‹: {original_date} {original_number}, ä¿®æ­£: {latest_date} {latest_number}")
            else:
                # å¸¸è§„å‘å¸ƒæ—¥æœŸæå–
                publish_patterns = [
                    r'å‘å¸ƒæ—¥æœŸ[ï¼š:](\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',
                    r'é¢å¸ƒæ—¥æœŸ[ï¼š:](\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',
                    r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)å‘å¸ƒ',
                    r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)é¢å¸ƒ',
                    # æ”¯æŒæ¨ªçº¿æ ¼å¼  
                    r'å‘å¸ƒæ—¥æœŸ[ï¼š:](\d{4}-\d{1,2}-\d{1,2})',
                    r'é¢å¸ƒæ—¥æœŸ[ï¼š:](\d{4}-\d{1,2}-\d{1,2})',
                    r'(\d{4}-\d{1,2}-\d{1,2})å‘å¸ƒ',
                    r'(\d{4}-\d{1,2}-\d{1,2})é¢å¸ƒ',
                    # ä»æ³•è§„å¼€å¤´çš„å¤æ‚æè¿°ä¸­æå–åŸå§‹å‘å¸ƒæ—¥æœŸ
                    r'ï¼ˆ(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥).*?å‘å¸ƒ',
                    r'ï¼ˆ(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥).*?ä»¤.*?å‘å¸ƒ'
                ]
                
                for pattern in publish_patterns:
                    matches = re.findall(pattern, content[:1500], re.IGNORECASE)
                    if matches:
                        result['publish_date'] = matches[0]
                        self.logger.debug(f"æå–åˆ°å‘å¸ƒæ—¥æœŸ: {matches[0]}")
                        break
                
                # å¦‚æœæ²¡æœ‰ä¸“é—¨çš„å‘å¸ƒæ—¥æœŸï¼Œä»å‰éƒ¨åˆ†æ‰¾ä¸€èˆ¬æ—¥æœŸ
                if not result['publish_date']:
                    general_date_patterns = [
                        r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',
                        r'(\d{4}-\d{1,2}-\d{1,2})',
                        r'(\d{4}\.\d{1,2}\.\d{1,2})'
                    ]
                    
                    for pattern in general_date_patterns:
                        matches = re.findall(pattern, content[:1000])
                        if matches:
                            result['publish_date'] = matches[0]
                            break
            
            # 3. æå–æ–‡å· - å¢å¼ºç‰ˆï¼Œå¤„ç†å¤æ‚çš„éƒ¨é—¨ä»¤æ ¼å¼
            # å¦‚æœåœ¨å¤æ‚å‘å¸ƒä¿¡æ¯ä¸­å·²ç»æå–åˆ°æ–‡å·ï¼Œè·³è¿‡è¿™ä¸€æ­¥
            if not result.get('document_number'):
                number_patterns = [
                    # å®Œæ•´çš„éƒ¨é—¨ä»¤æ ¼å¼ - å¢å¼ºç‰ˆ
                    r'(ä¸­åäººæ°‘å…±å’Œå›½.*?éƒ¨ä»¤ç¬¬\d+å·)',
                    r'(ä½æˆ¿å’ŒåŸä¹¡å»ºè®¾éƒ¨ä»¤ç¬¬\d+å·)',
                    r'(å»ºè®¾éƒ¨ä»¤ç¬¬\d+å·)',
                    r'(.*?éƒ¨ä»¤ç¬¬\d+å·)',
                    r'(.*?æ€»å±€ä»¤ç¬¬\d+å·)',
                    r'(.*?å§”å‘˜ä¼šä»¤ç¬¬\d+å·)',
                    # ä»å¤æ‚æè¿°ä¸­æå–æœ€æ–°çš„æ–‡å·
                    r'æ ¹æ®.*?(ç¬¬\d+å·).*?ä¿®æ­£',
                    r'æ ¹æ®.*?ä»¤(ç¬¬\d+å·)',
                    # æ ‡å‡†æ ¼å¼
                    r'ç¬¬(\d+)å·ä»¤',
                    r'ä»¤ç¬¬(\d+)å·',
                    r'ç¬¬(\d+)å·',
                    r'(\d{4}å¹´ç¬¬\d+å·)',
                    r'ä»¤.*?ç¬¬?(\d+)å·',
                    # å…¶ä»–æ ¼å¼
                    r'æ–‡å·[ï¼š:](.+?)\s',
                    r'æ–‡ä»¶ç¼–å·[ï¼š:](.+?)\s'
                ]
                
                for pattern in number_patterns:
                    matches = re.findall(pattern, content[:1500])
                    if matches:
                        doc_num = matches[0]
                        # å¦‚æœå·²ç»æ˜¯å®Œæ•´æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
                        if 'ä»¤' in doc_num or 'å·' in doc_num:
                            result['document_number'] = doc_num
                        elif doc_num.isdigit():
                            result['document_number'] = f"ç¬¬{doc_num}å·"
                        else:
                            result['document_number'] = doc_num
                        self.logger.debug(f"æå–åˆ°æ–‡å·: {result['document_number']}")
                        break
            
            # 4. æå–å‘å¸ƒæœºå…³/é¢å¸ƒæœºå…³ - å¢å¼ºç‰ˆï¼Œå¤„ç†å¤æ‚ä¿®æ­£æƒ…å†µ
            authority_patterns = [
                # ç›´æ¥æåŠå‘å¸ƒæœºå…³
                r'å‘å¸ƒæœºå…³[ï¼š:](.+?)(?:\s|å‘å¸ƒæ—¥æœŸ|é¢å¸ƒæ—¥æœŸ|å®æ–½æ—¥æœŸ)',
                r'é¢å¸ƒæœºå…³[ï¼š:](.+?)(?:\s|å‘å¸ƒæ—¥æœŸ|é¢å¸ƒæ—¥æœŸ|å®æ–½æ—¥æœŸ)',
                r'åˆ¶å®šæœºå…³[ï¼š:](.+?)(?:\s|å‘å¸ƒæ—¥æœŸ|é¢å¸ƒæ—¥æœŸ|å®æ–½æ—¥æœŸ)',
                
                # ä»å¤æ‚çš„å‘å¸ƒæè¿°ä¸­æå–åŸå§‹å’Œæœ€æ–°å‘å¸ƒæœºå…³
                r'ï¼ˆ\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥(ä¸­åäººæ°‘å…±å’Œå›½.*?éƒ¨)ä»¤ç¬¬\d+å·å‘å¸ƒ',
                r'æ ¹æ®\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥(ä¸­åäººæ°‘å…±å’Œå›½.*?éƒ¨)ä»¤ç¬¬\d+å·',
                
                # ä»å…·ä½“æè¿°ä¸­æå– - æ”¹è¿›ç‰ˆ
                r'ä¸­åäººæ°‘å…±å’Œå›½(.*?éƒ¨)(?:ä»¤|è§„ç« |åŠæ³•)',
                r'(ä½æˆ¿å’ŒåŸä¹¡å»ºè®¾éƒ¨)(?:ä»¤|è§„ç« |åŠæ³•)',
                r'(å»ºè®¾éƒ¨)(?:ä»¤|è§„ç« |åŠæ³•)',
                r'(äº¤é€šè¿è¾“éƒ¨)(?:ä»¤|è§„ç« |åŠæ³•)',
                r'(å·¥ä¸šå’Œä¿¡æ¯åŒ–éƒ¨)(?:ä»¤|è§„ç« |åŠæ³•)',
                r'(å›½å®¶å¸‚åœºç›‘ç£ç®¡ç†æ€»å±€)(?:ä»¤|è§„ç« |åŠæ³•)',
                r'(å›½å®¶.*?å±€)ä»¤',
                r'(.*?éƒ¨)ä»¤',
                r'(å›½åŠ¡é™¢.*?)ä»¤',
                r'(.*?å§”å‘˜ä¼š)ä»¤',
                r'(.*?æ€»å±€)ä»¤',
                r'(.*?ç›‘ç£ç®¡ç†å±€)ä»¤',
                
                # ä»åºŸæ­¢ä¿¡æ¯ä¸­æå–
                r'åŸ(å›½å®¶.*?å±€)ä»¤',
                r'åŸ(.*?éƒ¨)ä»¤',
                r'åŸ(.*?æ€»å±€)ä»¤',
                
                # ç‰¹æ®Šæƒ…å†µ
                r'(å›½å®¶è´¨é‡ç›‘ç£æ£€éªŒæ£€ç–«æ€»å±€)',
                r'(å¸‚åœºç›‘ç£ç®¡ç†æ€»å±€)',
                r'(ä½æˆ¿å’ŒåŸä¹¡å»ºè®¾éƒ¨)',
                r'(å»ºè®¾éƒ¨)',
                r'(äº¤é€šè¿è¾“éƒ¨)',
                r'(å·¥ä¸šå’Œä¿¡æ¯åŒ–éƒ¨)'
            ]
            
            for pattern in authority_patterns:
                matches = re.findall(pattern, content[:2000], re.IGNORECASE)
                if matches:
                    authority = matches[0].strip()
                    # æ¸…ç†å¸¸è§åç¼€å’Œå‰ç¼€
                    authority = re.sub(r'(ä»¤|ç¬¬.*?å·|å‘å¸ƒ|é¢å¸ƒ)$', '', authority).strip()
                    authority = re.sub(r'^(é¦–é¡µ|å…¬å¼€|æ”¿ç­–|è§„ç« åº“|ä¸‹è½½|æ–‡å­—ç‰ˆ|å›¾ç‰‡ç‰ˆ|>)+', '', authority).strip()
                    # æ¸…ç†å¯¼èˆªæ–‡æœ¬å’Œå¤šä½™ä¿¡æ¯
                    authority = re.sub(r'.*?>(.*?)(?:è§„ç« |åŠæ³•|ä»¤|ä¸‹è½½|ç‰ˆ|é¦–é¡µ)', r'\1', authority).strip()
                    # æå–æ ¸å¿ƒéƒ¨é—¨åç§°
                    if 'ä¸­åäººæ°‘å…±å’Œå›½' in authority:
                        authority = re.sub(r'.*ä¸­åäººæ°‘å…±å’Œå›½(.*)', r'\1', authority).strip()
                    
                    # ç‰¹æ®Šå¤„ç†ï¼šå»ºè®¾éƒ¨ -> ä½æˆ¿å’ŒåŸä¹¡å»ºè®¾éƒ¨ï¼ˆå†å²å˜æ›´ï¼‰
                    if authority == 'å»ºè®¾éƒ¨':
                        authority = 'ä½æˆ¿å’ŒåŸä¹¡å»ºè®¾éƒ¨'
                        result['historical_authority'] = 'å»ºè®¾éƒ¨'
                    
                    if len(authority) > 2:  # é¿å…æå–åˆ°è¿‡çŸ­çš„æ–‡æœ¬
                        result['issuing_authority'] = authority
                        result['office'] = authority  # åŒæ—¶è®¾ç½®officeå­—æ®µ
                        self.logger.debug(f"æå–åˆ°å‘å¸ƒæœºå…³: {authority}")
                        break
            
            # 5. æå–åºŸæ­¢ä¿¡æ¯ä¸­çš„é¢å¤–ç»†èŠ‚
            revoke_pattern = r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)(.*?)(ç¬¬\d+å·)(.*?)åŒæ—¶åºŸæ­¢'
            revoke_matches = re.findall(revoke_pattern, content)
            if revoke_matches:
                revoke_date, revoke_authority, revoke_number, revoke_doc = revoke_matches[0]
                # å¦‚æœè¿˜æ²¡æ‰¾åˆ°å‘å¸ƒæœºå…³ï¼Œä»åºŸæ­¢ä¿¡æ¯ä¸­æå–
                if not result['issuing_authority'] and revoke_authority:
                    cleaned_authority = re.sub(r'(åŸ|å‘å¸ƒçš„|ä»¤)', '', revoke_authority).strip()
                    if len(cleaned_authority) > 2:
                        result['issuing_authority'] = cleaned_authority
                        result['office'] = cleaned_authority
            
            # 6. å¤„ç†"è‡ªå‘å¸ƒä¹‹æ—¥èµ·æ–½è¡Œ"çš„æƒ…å†µ
            if result.get('implement_from_publish') and result.get('publish_date'):
                result['valid_from'] = result['publish_date']
                self.logger.debug(f"è®¾ç½®å®æ–½æ—¥æœŸä¸ºå‘å¸ƒæ—¥æœŸ: {result['publish_date']}")
            
            # 7. æ™ºèƒ½æ¨æ–­æ³•è§„çº§åˆ«
            authority = result.get('issuing_authority', '')
            if 'å›½åŠ¡é™¢' in authority:
                result['law_level'] = 'è¡Œæ”¿æ³•è§„'
            elif any(keyword in authority for keyword in ['éƒ¨', 'å§”å‘˜ä¼š', 'æ€»å±€', 'å±€']):
                result['law_level'] = 'éƒ¨é—¨è§„ç« '
            else:
                result['law_level'] = 'éƒ¨é—¨è§„ç« '  # é»˜è®¤
            
            return result
            
        except Exception as e:
            self.logger.error(f"æå–æ³•è§„è¯¦æƒ…å¤±è´¥: {e}")
            return {}
    
    async def crawl_law(self, law_name: str, law_number: str = None, strict_mode: bool = False, force_selenium: bool = False) -> Optional[Dict[str, Any]]:
        """çˆ¬å–å•ä¸ªæ³•è§„ - å¸¦è¶…æ—¶æ§åˆ¶å’Œååçˆ¬æœºåˆ¶
        
        Args:
            law_name: æ³•è§„åç§°
            law_number: æ³•è§„ç¼–å·ï¼ˆå¯é€‰ï¼‰
            strict_mode: ä¸¥æ ¼æ¨¡å¼ï¼ŒTrueæ—¶ä»…ä½¿ç”¨HTTPæœç´¢ï¼Œç¦ç”¨è‡ªåŠ¨åˆ‡æ¢
            force_selenium: å¼ºåˆ¶ä½¿ç”¨Seleniumæœç´¢ï¼ˆç­–ç•¥3ä¸“ç”¨ï¼‰
        """
        start_time = time.time()
        
        if strict_mode:
            self.logger.info(f"æœç´¢å¼•æ“ä¸¥æ ¼æ¨¡å¼çˆ¬å–: {law_name} (ä»…HTTPæœç´¢)")
        elif force_selenium:
            self.logger.info(f"æœç´¢å¼•æ“Seleniumæ¨¡å¼çˆ¬å–: {law_name} (ä»…Seleniumæœç´¢)")
        else:
            self.logger.info(f"æœç´¢å¼•æ“æ™ºèƒ½æ¨¡å¼çˆ¬å–: {law_name}")
        
        try:
            # æ ¹æ®æ¨¡å¼é€‰æ‹©æœç´¢æ–¹æ³•
            if force_selenium:
                # ç­–ç•¥3ï¼šå¼ºåˆ¶ä½¿ç”¨Seleniumæœç´¢ï¼Œä½†ç®€åŒ–å¤„ç†é¿å…è¶…æ—¶
                if not hasattr(self, 'selenium_engine') or not self.selenium_engine:
                    self.selenium_engine = SeleniumSearchEngine(self.anti_detection)
                search_task = self.selenium_engine.search_with_selenium(law_name, engine="baidu")
            elif strict_mode:
                # ç­–ç•¥2ï¼šä¸¥æ ¼æ¨¡å¼ï¼Œä»…HTTPæœç´¢
                search_task = self.search_law_via_engines(law_name)
            else:
                # æ™ºèƒ½æ¨¡å¼ï¼šHTTP + å¯èƒ½çš„Seleniumè¡¥å……
                search_task = self.search_law_via_engines(law_name)
            
            # 1. æœç´¢è·å–å€™é€‰ç»“æœ (å¸¦è¶…æ—¶æ§åˆ¶)
            
            try:
                if force_selenium:
                    # Seleniumæœç´¢ç›´æ¥è¿”å›ç»“æœï¼Œè®¾ç½®è¾ƒçŸ­è¶…æ—¶é¿å…å¡ä½
                    search_results = await asyncio.wait_for(search_task, timeout=20)
                    # è½¬æ¢Seleniumç»“æœæ ¼å¼ä¸ºç»Ÿä¸€æ ¼å¼
                    if search_results:
                        search_results = self._filter_and_rank_results(search_results, law_name)
                else:
                    search_results = await asyncio.wait_for(
                        search_task, 
                        timeout=min(self.timeout_config['single_law_timeout'] + 10, 60)  # å¢åŠ 10ç§’ç¼“å†²ï¼Œæœ€å¤§60ç§’
                    )
            except asyncio.TimeoutError:
                elapsed = time.time() - start_time
                self.logger.warning(f"æœç´¢å¼•æ“çˆ¬å–è¶…æ—¶ ({elapsed:.1f}s > {self.timeout_config['single_law_timeout']}s): {law_name}")
                return None
            
            if not search_results:
                elapsed = time.time() - start_time
                self.logger.warning(f"æœç´¢å¼•æ“æœªæ‰¾åˆ°ç»“æœ (è€—æ—¶ {elapsed:.1f}s): {law_name}")
                return None
            
            # 2. æ£€æŸ¥å‰©ä½™æ—¶é—´
            elapsed = time.time() - start_time
            remaining_time = self.timeout_config['single_law_timeout'] - elapsed
            
            if remaining_time <= 2:  # å‰©ä½™æ—¶é—´ä¸è¶³2ç§’
                self.logger.warning(f"å‰©ä½™æ—¶é—´ä¸è¶³ï¼Œè¿”å›åŸºæœ¬ä¿¡æ¯: {law_name}")
                best_result = search_results[0]
                return {
                    'success': True,
                    'name': best_result['title'],
                    'title': best_result['title'],
                    'target_name': law_name,
                    'search_keyword': law_name,
                    'crawl_time': datetime.now().isoformat(),
                    'source': 'æœç´¢å¼•æ“(æ”¿åºœç½‘)',
                    'source_url': best_result['url'],
                    'crawler_strategy': 'search_engine',
                    'search_engine': best_result.get('source', 'unknown'),
                    'timeout_limited': True,
                    'elapsed_time': elapsed
                }
            
            # 3. è·å–æœ€ä½³åŒ¹é…çš„è¯¦ç»†ä¿¡æ¯ (å¸¦å‰©ä½™æ—¶é—´é™åˆ¶)
            best_result = search_results[0]
            
            try:
                detail_task = self.get_law_detail_from_url(best_result['url'])
                # è·å–è¯¦ç»†ä¿¡æ¯ï¼Œè€ƒè™‘å‰©ä½™æ—¶é—´ï¼Œä½†ä¿è¯æœ€å°æ—¶é—´
                remaining_time = max(15, self.timeout_config['single_law_timeout'] - elapsed)  # æœ€å°‘15ç§’
                detail_info = await asyncio.wait_for(detail_task, timeout=remaining_time)
            except asyncio.TimeoutError:
                elapsed = time.time() - start_time
                self.logger.warning(f"è¯¦ç»†ä¿¡æ¯è·å–è¶…æ—¶ (æ€»è€—æ—¶ {elapsed:.1f}s): {law_name}")
                return {
                    'success': True,
                    'name': best_result['title'],
                    'title': best_result['title'],
                    'target_name': law_name,
                    'search_keyword': law_name,
                    'crawl_time': datetime.now().isoformat(),
                    'source': 'æœç´¢å¼•æ“(æ”¿åºœç½‘)',
                    'source_url': best_result['url'],
                    'crawler_strategy': 'search_engine',
                    'search_engine': best_result.get('source', 'unknown'),
                    'detail_timeout': True,
                    'elapsed_time': elapsed
                }
            
            if not detail_info:
                elapsed = time.time() - start_time
                self.logger.warning(f"æ— æ³•è·å–è¯¦ç»†ä¿¡æ¯ (è€—æ—¶ {elapsed:.1f}s): {law_name}")
                return None
            
            # 4. æ•´åˆç»“æœ
            elapsed = time.time() - start_time
            result = {
                'success': True,
                'name': best_result['title'],
                'title': best_result['title'],
                'target_name': law_name,
                'search_keyword': law_name,
                'crawl_time': datetime.now().isoformat(),
                'source': 'æœç´¢å¼•æ“(æ”¿åºœç½‘)',
                'source_url': best_result['url'],
                'crawler_strategy': 'search_engine',
                'search_engine': best_result.get('source', 'unknown'),
                'search_rank': 1,
                'elapsed_time': elapsed,
                **detail_info
            }
            
            self.logger.success(f"æœç´¢å¼•æ“çˆ¬å–æˆåŠŸ (è€—æ—¶ {elapsed:.1f}s): {law_name}")
            return result
            
        except Exception as e:
            elapsed = time.time() - start_time
            self.logger.error(f"æœç´¢å¼•æ“çˆ¬å–å¤±è´¥ (è€—æ—¶ {elapsed:.1f}s): {law_name} - {e}")
            return None
        finally:
            # ä¿æŒä¼šè¯æ‰“å¼€ä»¥ä¾¿å¤ç”¨
            pass
    
    async def close(self):
        """å…³é—­ä¼šè¯å’ŒSeleniumé©±åŠ¨"""
        # å…³é—­Seleniumé©±åŠ¨
        if hasattr(self, 'selenium_engine') and self.selenium_engine:
            try:
                self.selenium_engine.close()
                self.logger.debug("Seleniumæœç´¢å¼•æ“å·²å…³é—­")
            except Exception as e:
                self.logger.warning(f"å…³é—­Seleniumæœç´¢å¼•æ“æ—¶å‡ºé”™: {e}")
        
        # å…³é—­HTTPä¼šè¯
        if self.session and not self.session.closed:
            try:
                await self.session.close()
                self.session = None
                self.logger.debug("æœç´¢å¼•æ“çˆ¬è™«ä¼šè¯å·²å…³é—­")
            except Exception as e:
                self.logger.warning(f"å…³é—­æœç´¢å¼•æ“çˆ¬è™«ä¼šè¯æ—¶å‡ºé”™: {e}")
                self.session = None
    
    def __del__(self):
        """ææ„å‡½æ•°"""
        if hasattr(self, 'session') and self.session and not self.session.closed:
            try:
                # ç›´æ¥è®¾ç½®ä¸ºNoneï¼Œé¿å…åœ¨ææ„æ—¶è¿›è¡Œå¤æ‚çš„å¼‚æ­¥æ“ä½œ
                self.session = None
            except:
                pass

    async def search(self, law_name: str, law_number: Optional[str] = None) -> List[Dict[str, Any]]:
        """æœç´¢æ³•è§„ - å®ç°æŠ½è±¡æ–¹æ³•"""
        return await self.search_law_via_engines(law_name)
    
    async def get_detail(self, law_id: str) -> Dict[str, Any]:
        """è·å–æ³•è§„è¯¦æƒ… - å®ç°æŠ½è±¡æ–¹æ³•"""
        return await self.get_law_detail_from_url(law_id)
    
    async def download_file(self, url: str, save_path: str) -> bool:
        """ä¸‹è½½æ–‡ä»¶ - å®ç°æŠ½è±¡æ–¹æ³•"""
        try:
            await self._ensure_session()
            
            async with self.session.get(url) as response:
                if response.status == 200:
                    content = await response.read()
                    with open(save_path, 'wb') as f:
                        f.write(content)
                    return True
            return False
        except Exception as e:
            self.logger.error(f"ä¸‹è½½æ–‡ä»¶å¤±è´¥: {e}")
            return False

    async def _search_with_waf_protection(self, query: str, max_retries: int = 3) -> List[Dict]:
        """
        å¸¦WAFä¿æŠ¤çš„æœç´¢
        
        Args:
            query: æœç´¢å…³é”®è¯
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        results = []
        
        for attempt in range(max_retries):
            try:
                # æ£€æŸ¥æ˜¯å¦éœ€è¦è½®æ¢IP
                if (attempt > 0 or 
                    self.consecutive_failures >= self.proxy_rotation_threshold):
                    
                    logger.info(f"ğŸ”„ ç¬¬{attempt+1}æ¬¡å°è¯•ï¼Œè½®æ¢IPä¸­...")
                    await self._rotate_proxy_for_waf()
                
                # æ‰§è¡Œæœç´¢
                search_results = await self._execute_protected_search(query)
                
                if search_results:
                    results.extend(search_results)
                    self.consecutive_failures = 0  # é‡ç½®å¤±è´¥è®¡æ•°
                    logger.info(f"âœ… æœç´¢æˆåŠŸï¼Œè·å¾— {len(search_results)} ä¸ªç»“æœ")
                    break
                else:
                    self.consecutive_failures += 1
                    logger.warning(f"âš ï¸ æœç´¢æ— ç»“æœï¼Œè¿ç»­å¤±è´¥ {self.consecutive_failures} æ¬¡")
                
            except Exception as e:
                self.consecutive_failures += 1
                error_msg = str(e)
                
                # æ£€æµ‹WAFé˜»æ–­
                is_waf_blocked = any(keyword in error_msg.lower() for keyword in [
                    '403', 'forbidden', 'access denied', 'blocked', 
                    'captcha', 'security check', 'éªŒè¯ç ', 'å®‰å…¨éªŒè¯'
                ])
                
                if is_waf_blocked:
                    logger.warning(f"ğŸ›¡ï¸ æ£€æµ‹åˆ°WAFé˜»æ–­: {error_msg}")
                    
                    # å¤„ç†WAFæ£€æµ‹
                    if self.current_proxy and self.enhanced_proxy_pool:
                        await self.enhanced_proxy_pool.handle_waf_detection(
                            self.current_proxy, error_msg
                        )
                    
                    # ç­‰å¾…åé‡è¯•
                    await asyncio.sleep(self.waf_retry_delay)
                else:
                    logger.error(f"âŒ æœç´¢å¼‚å¸¸: {error_msg}")
                
                if attempt == max_retries - 1:
                    logger.error(f"ğŸ’¥ æœç´¢å®Œå…¨å¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡")
        
        return results

    async def _rotate_proxy_for_waf(self):
        """ä¸ºWAFå¯¹æŠ—è½®æ¢ä»£ç†"""
        try:
            if self.enhanced_proxy_pool:
                # è·å–ä¸“é—¨ç”¨äºç»•è¿‡WAFçš„ä»£ç†
                new_proxy = await self.enhanced_proxy_pool.get_proxy_for_waf_bypass()
                if new_proxy:
                    old_proxy_name = self.current_proxy.name if self.current_proxy else "æ— "
                    self.current_proxy = new_proxy
                    logger.info(f"ğŸŒ IPè½®æ¢: {old_proxy_name} â†’ {new_proxy.name}")
                    
                    # çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                    await asyncio.sleep(random.uniform(2, 5))
                else:
                    logger.warning("âš ï¸ æ— å¯ç”¨ä»£ç†è¿›è¡Œè½®æ¢")
            else:
                logger.warning("âš ï¸ ä»£ç†æ± æœªåˆå§‹åŒ–ï¼Œæ— æ³•è½®æ¢")
                
        except Exception as e:
            logger.error(f"âŒ ä»£ç†è½®æ¢å¤±è´¥: {e}")

    async def _execute_protected_search(self, query: str) -> List[Dict]:
        """æ‰§è¡Œå—ä¿æŠ¤çš„æœç´¢"""
        results = []
        
        for search_engine in self.search_engines:
            try:
                logger.info(f"ğŸ” ä½¿ç”¨ {search_engine} æœç´¢: {query}")
                
                # æ„å»ºæœç´¢URL
                search_url = f"{search_engine}{query}"
                
                # ä½¿ç”¨å½“å‰ä»£ç†å‘èµ·è¯·æ±‚
                response = await self._make_protected_request(search_url)
                
                if response:
                    # æ£€æµ‹å“åº”æ˜¯å¦åŒ…å«WAFç‰¹å¾
                    if await self._detect_waf_response(response):
                        logger.warning(f"ğŸ›¡ï¸ {search_engine} å“åº”åŒ…å«WAFç‰¹å¾")
                        continue
                    
                    # è§£ææœç´¢ç»“æœ
                    engine_results = await self._parse_search_results(response, search_engine)
                    results.extend(engine_results)
                    
                    logger.info(f"âœ… {search_engine} è¿”å› {len(engine_results)} ä¸ªç»“æœ")
                
            except Exception as e:
                logger.error(f"âŒ {search_engine} æœç´¢å¤±è´¥: {e}")
                continue
        
        return results

    async def _make_protected_request(self, url: str, timeout: int = 30) -> str:
        """å‘èµ·å—ä¿æŠ¤çš„è¯·æ±‚"""
        # æ„å»ºè¯·æ±‚headers
        headers = self._get_stealth_headers()
        
        # è·å–ä»£ç†é…ç½®
        proxy_dict = None
        if self.current_proxy:
            proxy_dict = self.current_proxy.proxy_dict
        
        # å‘èµ·è¯·æ±‚
        async with aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=timeout),
            headers=headers
        ) as session:
            
            async with session.get(url, proxy=proxy_dict.get('http') if proxy_dict else None) as response:
                if response.status == 200:
                    return await response.text()
                else:
                    raise Exception(f"HTTP {response.status}: {await response.text()}")

    async def _detect_waf_response(self, response_text: str) -> bool:
        """æ£€æµ‹å“åº”æ˜¯å¦åŒ…å«WAFç‰¹å¾"""
        waf_indicators = [
            'cloudflare', 'access denied', '403 forbidden',
            'security check', 'captcha', 'blocked',
            'éªŒè¯ç ', 'å®‰å…¨éªŒè¯', 'è®¿é—®è¢«æ‹’ç»', 'é˜²ç«å¢™'
        ]
        
        response_lower = response_text.lower()
        return any(indicator in response_lower for indicator in waf_indicators)

    def _get_stealth_headers(self) -> Dict[str, str]:
        """è·å–éšç§˜æ€§è¯·æ±‚å¤´"""
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0'
        ]
        
        return {
            'User-Agent': random.choice(user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        }

    def initialize_proxy_pools(self):
        """åˆå§‹åŒ–ä»£ç†æ±  - å®Œå…¨ç¦ç”¨ï¼Œä½¿ç”¨ç›´è¿æ¨¡å¼"""
        self.logger.info("ğŸš€ æœç´¢å¼•æ“çˆ¬è™«ä½¿ç”¨ç›´è¿æ¨¡å¼ï¼Œè·³è¿‡ä»£ç†æ± åˆå§‹åŒ–")
        self._initialized = True
        return
        
        # # åŸä»£ç†æ± åˆå§‹åŒ–é€»è¾‘å·²ç¦ç”¨
        # if self._initialized:
        #     return
        # 
        # try:
        #     # Enhancedä»£ç†æ± 
        #     settings = get_settings()
        #     if settings.proxy_pool.enabled:
        #         try:
        #             self.enhanced_proxy_pool = await get_enhanced_proxy_pool()
        #             self.logger.success("Enhancedä»£ç†æ± åˆå§‹åŒ–æˆåŠŸ")
        #         except Exception as e:
        #             self.logger.warning(f"Enhancedä»£ç†æ± åˆå§‹åŒ–å¤±è´¥: {e}")
        # 
        #     # IPæ± 
        #     if settings.ip_pool.enabled:
        #         try:
        #             self.ip_pool = await get_ip_pool()
        #             self.logger.success("IPæ± åˆå§‹åŒ–æˆåŠŸ")
        #         except Exception as e:
        #             self.logger.warning(f"IPæ± åˆå§‹åŒ–å¤±è´¥: {e}")
        # 
        #     self._initialized = True
        # 
        # except Exception as e:
        #     self.logger.error(f"ä»£ç†æ± åˆå§‹åŒ–å¼‚å¸¸: {e}")
        #     self._initialized = True


def create_search_engine_crawler():
    """åˆ›å»ºæœç´¢å¼•æ“çˆ¬è™«å®ä¾‹"""
    return SearchEngineCrawler() 