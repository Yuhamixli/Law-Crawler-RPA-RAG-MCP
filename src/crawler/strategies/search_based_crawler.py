#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
åŸºäºæœç´¢çš„æ³•è§„é‡‡é›†å™¨ç­–ç•¥
å‚è€ƒç¤ºä¾‹é¡¹ç›®çš„æˆåŠŸæ–¹æ³•ï¼Œä½¿ç”¨æœç´¢API + è¯¦æƒ…APIçš„ç»„åˆæ–¹æ¡ˆ
"""

import json
import requests
import time
from datetime import datetime
from typing import List, Dict, Optional, Any
import re
from loguru import logger

from ..base_crawler import BaseCrawler


def normalize_date_format(date_str: str) -> str:
    """
    å°†å„ç§æ—¥æœŸæ ¼å¼ç»Ÿä¸€è½¬æ¢ä¸º yyyy-mm-dd æ ¼å¼
    æ”¯æŒçš„è¾“å…¥æ ¼å¼ï¼š
    - 2013å¹´2æœˆ4æ—¥ -> 2013-02-04
    - 2013-2-4 -> 2013-02-04
    - 2013.2.4 -> 2013-02-04
    - 2025-05-29 00:00:00 -> 2025-05-29
    """
    if not date_str or date_str.strip() == '':
        return ''
    
    import re
    from datetime import datetime
    
    date_str = str(date_str).strip()
    
    try:
        # æ ¼å¼1: 2013å¹´2æœˆ4æ—¥
        match = re.match(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', date_str)
        if match:
            year, month, day = match.groups()
            return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
        
        # æ ¼å¼2: 2013-2-4 æˆ– 2013/2/4 æˆ– 2013.2.4
        match = re.match(r'(\d{4})[-/.](\d{1,2})[-/.](\d{1,2})', date_str)
        if match:
            year, month, day = match.groups()
            return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
        
        # æ ¼å¼3: 2025-05-29 00:00:00 (å¸¦æ—¶é—´)
        match = re.match(r'(\d{4}-\d{2}-\d{2})\s+\d{2}:\d{2}:\d{2}', date_str)
        if match:
            return match.group(1)
        
        # æ ¼å¼4: å·²ç»æ˜¯ yyyy-mm-dd æ ¼å¼
        match = re.match(r'\d{4}-\d{2}-\d{2}$', date_str)
        if match:
            return date_str
        
        # æ ¼å¼5: å°è¯•ä½¿ç”¨datetimeè§£æ
        for fmt in ['%Y-%m-%d', '%Y/%m/%d', '%Y.%m.%d', '%Yå¹´%mæœˆ%dæ—¥']:
            try:
                dt = datetime.strptime(date_str, fmt)
                return dt.strftime('%Y-%m-%d')
            except:
                continue
        
        # å¦‚æœéƒ½æ— æ³•è§£æï¼Œè¿”å›åŸå§‹å­—ç¬¦ä¸²
        logger.warning(f"æ— æ³•è§£ææ—¥æœŸæ ¼å¼: {date_str}")
        return date_str
        
    except Exception as e:
        logger.warning(f"æ—¥æœŸæ ¼å¼åŒ–å¤±è´¥: {date_str}, é”™è¯¯: {e}")
        return date_str


class SearchBasedCrawler(BaseCrawler):
    """åŸºäºæœç´¢çš„æ³•è§„é‡‡é›†å™¨"""
    
    def __init__(self):
        super().__init__("search_api")
        self.setup_headers()
        self.logger = logger
        
    def setup_headers(self):
        """è®¾ç½®è¯·æ±‚å¤´ï¼Œå‚è€ƒç¤ºä¾‹é¡¹ç›®çš„æˆåŠŸé…ç½®"""
        self.session = requests.Session()
        self.session.headers.update({
            "authority": "flk.npc.gov.cn",
            "sec-ch-ua": '" Not A;Brand";v="99", "Chromium";v="99", "Microsoft Edge";v="99"',
            "accept": "application/json, text/javascript, */*; q=0.01",
            "x-requested-with": "XMLHttpRequest",
            "sec-ch-ua-mobile": "?0",
            "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36 Edg/99.0.1150.39",
            "sec-ch-ua-platform": '"macOS"',
            "sec-fetch-site": "same-origin",
            "sec-fetch-mode": "cors",
            "sec-fetch-dest": "empty",
            "referer": "https://flk.npc.gov.cn/fl.html",
            "accept-language": "en-AU,en-GB;q=0.9,en;q=0.8,en-US;q=0.7,zh-CN;q=0.6,zh;q=0.5",
            "cookie": "yfx_c_g_u_id_10006696=_ck22022520424713255117764923111; cna=NdafGk8tiAgCAd9IPxhfROag; yfx_f_l_v_t_10006696=f_t_1645792967326__r_t_1646401808964__v_t_1646401808964__r_c_5; Hm_lvt_54434aa6770b6d9fef104d146430b53b=1646407223,1646570042,1646666110,1647148584; acw_tc=75a1461516471485843844814eb808af266b8ede0e0502ec1c46ab1581; Hm_lpvt_54434aa6770b6d9fef104d146430b53b=1647148626",
        })
    
    async def search(self, law_name: str, law_number: Optional[str] = None) -> List[Dict[str, Any]]:
        """æœç´¢æ³•è§„ - å®ç°æŠ½è±¡æ–¹æ³•"""
        return self.search_law(law_name)
    
    async def get_detail(self, law_id: str) -> Dict[str, Any]:
        """è·å–æ³•è§„è¯¦æƒ… - å®ç°æŠ½è±¡æ–¹æ³•"""
        result = self.get_law_detail(law_id)
        return result or {}
    
    async def download_file(self, url: str, save_path: str) -> bool:
        """ä¸‹è½½æ–‡ä»¶ - å®ç°æŠ½è±¡æ–¹æ³•"""
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            with open(save_path, 'wb') as f:
                f.write(response.content)
            return True
        except Exception as e:
            self.logger.error(f"ä¸‹è½½æ–‡ä»¶å¤±è´¥: {str(e)}")
            return False
    
    def search_law(self, keyword: str) -> List[Dict[str, Any]]:
        """æœç´¢æ³•è§„"""
        params = [
            ("searchType", "title;accurate;1,3"),
            ("sortTr", "f_bbrq_s;desc"),
            ("gbrqStart", ""),
            ("gbrqEnd", ""),
            ("sxrqStart", ""),
            ("sxrqEnd", ""),
            ("sort", "true"),
            ("page", "1"),
            ("size", "20"),
            ("fgbt", keyword),  # æœç´¢å…³é”®è¯
            ("_", int(time.time() * 1000)),
        ]
        
        try:
            response = self.session.get(
                "https://flk.npc.gov.cn/api/",
                params=params,
                timeout=10
            )
            
            if response.status_code == 200:
                result = response.json()
                if result.get('success') and result.get('result', {}).get('data'):
                    return result['result']['data']
            
            return []
            
        except Exception as e:
            self.logger.error(f"æœç´¢æ³•è§„å¤±è´¥: {str(e)}")
            return []
    
    def get_law_detail(self, law_id: str) -> Optional[Dict[str, Any]]:
        """è·å–æ³•è§„è¯¦æƒ…"""
        try:
            response = self.session.post(
                "https://flk.npc.gov.cn/api/detail",
                data={"id": law_id},
                timeout=10
            )
            
            if response.status_code == 200:
                result = response.json()
                if result.get('success') and result.get('result'):
                    return result['result']
            
            return None
            
        except Exception as e:
            self.logger.error(f"è·å–æ³•è§„è¯¦æƒ…å¤±è´¥: {str(e)}")
            return None
    
    def normalize_law_name(self, law_name: str) -> str:
        """æ ‡å‡†åŒ–æ³•è§„åç§°"""
        # ç§»é™¤æ‹¬å·å†…å®¹
        normalized = re.sub(r'[ï¼ˆ(].*?[ï¼‰)]', '', law_name)
        # ç§»é™¤"ä¿®è®¢"ã€"ä¿®æ­£"ç­‰åç¼€
        normalized = re.sub(r'ï¼ˆ\d{4}.*?ä¿®è®¢.*?ï¼‰', '', normalized)
        normalized = re.sub(r'ï¼ˆ\d{4}.*?ä¿®æ­£.*?ï¼‰', '', normalized)
        # ç§»é™¤ä¸»å¸­ä»¤ç­‰ç¼–å·
        normalized = re.sub(r'ï¼ˆ.*?ä¸»å¸­ä»¤.*?ï¼‰', '', normalized)
        normalized = re.sub(r'ï¼ˆ.*?ä»¤.*?ï¼‰', '', normalized)
        # æ¸…ç†å¤šä½™ç©ºæ ¼
        normalized = re.sub(r'\s+', '', normalized)
        
        return normalized.strip()
    
    def calculate_match_score(self, target: str, result: str) -> float:
        """è®¡ç®—åŒ¹é…åˆ†æ•°"""
        if target == result:
            return 1.0
        
        # è®¡ç®—åŒ…å«å…³ç³»
        if target in result:
            return 0.8 + (len(target) / len(result)) * 0.2
        if result in target:
            return 0.8 + (len(result) / len(target)) * 0.2
        
        # è®¡ç®—å…¬å…±å­ä¸²
        common_length = 0
        for i in range(min(len(target), len(result))):
            if target[i] == result[i]:
                common_length += 1
            else:
                break
        
        if common_length > 0:
            return common_length / max(len(target), len(result))
        
        return 0.0
    
    def find_best_match(self, target_name: str, search_results: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """åœ¨æœç´¢ç»“æœä¸­æ‰¾åˆ°æœ€ä½³åŒ¹é…"""
        if not search_results:
            return None
        
        # æ ‡å‡†åŒ–ç›®æ ‡åç§°
        target_normalized = self.normalize_law_name(target_name)
        
        best_match = None
        best_score = 0
        
        for law in search_results:
            law_title = law.get('title', '')
            law_normalized = self.normalize_law_name(law_title)
            
            # è®¡ç®—åŒ¹é…åˆ†æ•°
            score = self.calculate_match_score(target_normalized, law_normalized)
            
            if score > best_score:
                best_score = score
                best_match = law
        
        # åªæœ‰åŒ¹é…åˆ†æ•°è¶³å¤Ÿé«˜æ‰è¿”å›
        if best_score >= 0.7:  # 70%åŒ¹é…åº¦
            return best_match
        
        return None
    
    def generate_search_keywords(self, law_name: str) -> List[str]:
        """ç”Ÿæˆæœç´¢å…³é”®è¯"""
        keywords = []
        
        # 1. å®Œæ•´åç§°
        keywords.append(law_name)
        
        # 2. ç§»é™¤"ä¸­åäººæ°‘å…±å’Œå›½"å‰ç¼€
        if law_name.startswith("ä¸­åäººæ°‘å…±å’Œå›½"):
            keywords.append(law_name.replace("ä¸­åäººæ°‘å…±å’Œå›½", ""))
        
        # 3. æå–ä¸»å¹²åç§°ï¼ˆç§»é™¤æ‹¬å·å†…å®¹ï¼‰
        main_name = re.sub(r'[ï¼ˆ(].*?[ï¼‰)]', '', law_name)
        if main_name != law_name:
            keywords.append(main_name.strip())
        
        # 4. æå–æ ¸å¿ƒè¯æ±‡
        if "æ³•" in law_name:
            # æå–"æ³•"å‰é¢çš„éƒ¨åˆ†
            parts = law_name.split("æ³•")
            if parts[0]:
                keywords.append(parts[0] + "æ³•")
        
        # å»é‡å¹¶è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
        keywords = list(set([k for k in keywords if k.strip()]))
        
        return keywords
    
    def extract_document_number(self, detail: dict) -> str:
        """æå–æ–‡å·"""
        try:
            # æ–¹æ³•1: ä»otherFileåˆ—è¡¨ä¸­çš„ä¸»å¸­ä»¤æ–‡ä»¶åæå–æ–‡å·
            other_files = detail.get('otherFile', [])
            if isinstance(other_files, list):
                for file_info in other_files:
                    if isinstance(file_info, dict):
                        file_name = file_info.get('name', '')
                        if 'ä¸»å¸­ä»¤' in file_name:
                            # æå–ä¸»å¸­ä»¤å·ç 
                            import re
                            # åŒ¹é…å„ç§ä¸»å¸­ä»¤æ ¼å¼
                            patterns = [
                                r'ä¸»å¸­ä»¤ï¼ˆç¬¬(\w+)å·ï¼‰',  # ä¸»å¸­ä»¤ï¼ˆç¬¬ä¸‰åä¸€å·ï¼‰
                                r'ä¸»å¸­ä»¤ç¬¬(\w+)å·',      # ä¸»å¸­ä»¤ç¬¬ä¸€äºŒã€‡å·
                                r'ä¸»å¸­ä»¤.*?(\d+)å·',     # åŒ…å«æ•°å­—çš„ä¸»å¸­ä»¤
                            ]
                            for pattern in patterns:
                                match = re.search(pattern, file_name)
                                if match:
                                    return f"ä¸»å¸­ä»¤ç¬¬{match.group(1)}å·"
            
            # æ–¹æ³•2: ä»æ ‡é¢˜ä¸­æå–ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
            title = detail.get('title', '')
            if title:
                # ç®€å•çš„æ–‡å·æå–é€»è¾‘ï¼Œå¯ä»¥æ ¹æ®éœ€è¦æ‰©å±•
                return ""
            
            return ""
        except Exception as e:
            self.logger.error(f"æå–æ–‡å·æ—¶å‡ºé”™: {e}")
            return ""
    
    def crawl_law_by_search(self, law_name: str) -> Optional[Dict[str, Any]]:
        """é€šè¿‡æœç´¢é‡‡é›†å•ä¸ªæ³•è§„"""
        self.logger.info(f"æœç´¢æ³•è§„: {law_name}")
        
        # ç”Ÿæˆæœç´¢å…³é”®è¯
        keywords = self.generate_search_keywords(law_name)
        self.logger.debug(f"æœç´¢å…³é”®è¯: {keywords}")
        
        for keyword in keywords:
            self.logger.debug(f"  å°è¯•å…³é”®è¯: {keyword}")
            
            search_results = self.search_law(keyword)
            
            if search_results:
                self.logger.debug(f"    æ‰¾åˆ° {len(search_results)} ä¸ªç»“æœ")
                
                # æ‰¾åˆ°æœ€ä½³åŒ¹é…
                best_match = self.find_best_match(law_name, search_results)
                
                if best_match:
                    self.logger.info(f"    âœ… æ‰¾åˆ°åŒ¹é…: {best_match.get('title', '')}")
                    self.logger.debug(f"    ID: {best_match.get('id', '')}")
                    
                    # è·å–è¯¦ç»†ä¿¡æ¯
                    detail = self.get_law_detail(best_match['id'])
                    if not detail:
                        self.logger.error(f"  âŒ æ— æ³•è·å–è¯¦ç»†ä¿¡æ¯")
                        return None
                    
                    # æå–å’Œæ•´ç†æ–‡ä»¶ä¿¡æ¯
                    body_files = detail.get('body', [])
                    other_files = detail.get('otherFile', [])
                    
                    # æ•´ç†æ­£æ–‡æ–‡ä»¶ä¿¡æ¯
                    formatted_body_files = []
                    if isinstance(body_files, list):
                        for file_info in body_files:
                            if isinstance(file_info, dict):
                                formatted_body_files.append({
                                    'type': file_info.get('type', ''),
                                    'path': file_info.get('path', ''),
                                    'url': file_info.get('url', ''),
                                    'mobile_url': file_info.get('mobile', ''),
                                    'addr': file_info.get('addr', '')
                                })
                    
                    # æ•´ç†å…¶ä»–æ–‡ä»¶ä¿¡æ¯
                    formatted_other_files = []
                    if isinstance(other_files, list):
                        for file_info in other_files:
                            if isinstance(file_info, dict):
                                formatted_other_files.append({
                                    'name': file_info.get('name', ''),
                                    'type': file_info.get('type', ''),
                                    'hdfs_path': file_info.get('hdfsPath', ''),
                                    'oss_path': file_info.get('ossPath', ''),
                                    'order': file_info.get('order', '')
                                })
                    
                    result_data = {
                        # åŸºæœ¬ä¿¡æ¯
                        'law_id': best_match.get('id', ''),
                        'target_name': law_name,
                        'search_keyword': keyword,
                        'title': detail.get('title', ''),
                        'document_number': self.extract_document_number(detail),
                        
                        # æ—¥æœŸä¿¡æ¯
                        'publish_date': normalize_date_format(detail.get('publish', '')),
                        'implement_date': normalize_date_format(detail.get('expiry', '')),
                        'invalidate_date': '',  # APIä¸­æ²¡æœ‰å¤±æ•ˆæ—¥æœŸå­—æ®µ
                        
                        # æœºå…³å’Œåˆ†ç±»ä¿¡æ¯
                        'office': detail.get('office', ''),
                        'level': detail.get('level', ''),  # æ³•è§„çº§åˆ«ï¼šæ³•å¾‹ã€è¡Œæ”¿æ³•è§„ã€å¸æ³•è§£é‡Šç­‰
                        'status': 'æœ‰æ•ˆ' if detail.get('status', '') == '1' else 'å¤±æ•ˆ',
                        
                        # æ–‡ä»¶ä¿¡æ¯
                        'body_files': formatted_body_files,  # æ­£æ–‡æ–‡ä»¶ï¼ˆWORD/PDF/HTMLï¼‰
                        'other_files': formatted_other_files,  # å…¶ä»–æ–‡ä»¶ï¼ˆä¸»å¸­ä»¤ç­‰ï¼‰
                        
                        # åŸå§‹æ•°æ®
                        'raw_api_response': detail,  # ä¿å­˜å®Œæ•´çš„APIå“åº”
                        
                        # å…ƒæ•°æ®
                        'source_url': f"https://flk.npc.gov.cn/detail2.html?id={best_match.get('id', '')}",
                        'crawl_date': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        'crawler_version': '1.0.0'
                    }
                    
                    self.logger.info(f"    ğŸ”¢ æ–‡å·: {result_data['document_number']}")
                    self.logger.info(f"    ğŸ“… å‘å¸ƒæ—¥æœŸ: {result_data['publish_date']}")
                    self.logger.info(f"    ğŸ“… å®æ–½æ—¥æœŸ: {result_data['implement_date']}")
                    self.logger.info(f"    ğŸš¦ çŠ¶æ€: {result_data['status']} (statusåŸå€¼: {detail.get('status', '')})")
                    self.logger.info(f"    ğŸ›ï¸ å‘å¸ƒæœºå…³: {result_data['office']}")
                    self.logger.info(f"    ğŸ“‹ æ³•è§„çº§åˆ«: {result_data['level']}")
                    
                    return result_data
                else:
                    self.logger.warning(f"    âŒ æœªæ‰¾åˆ°åŒ¹é…")
            else:
                self.logger.warning(f"    âŒ æœç´¢æ— ç»“æœ")
            
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        self.logger.error(f"  âŒ æ‰€æœ‰å…³é”®è¯éƒ½æœªæ‰¾åˆ°åŒ¹é…")
        return None
    
    async def crawl_law(self, law_name: str, law_number: str = None) -> Optional[Dict]:
        """çˆ¬å–å•ä¸ªæ³•å¾‹ï¼ˆå®ç°CrawlerManageréœ€è¦çš„æ¥å£ï¼‰"""
        try:
            self.logger.info(f"äººå¤§ç½‘çˆ¬å–: {law_name}")
            
            result = self.crawl_law_by_search(law_name)
            
            if result:
                # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                return {
                    'law_id': result.get('law_id', ''),
                    'name': result.get('title', ''),
                    'number': result.get('document_number', ''),
                    'source': 'search_api',
                    'success': True,
                    'source_url': result.get('source_url', ''),
                    'publish_date': result.get('publish_date', ''),
                    'valid_from': result.get('implement_date', ''),
                    'valid_to': result.get('invalidate_date', ''),
                    'office': result.get('office', ''),
                    'status': result.get('status', ''),
                    'level': result.get('level', ''),
                    'crawl_time': result.get('crawl_date', ''),
                    'raw_data': result
                }
            else:
                return None
                
        except Exception as e:
            self.logger.error(f"äººå¤§ç½‘çˆ¬å–å¤±è´¥: {law_name}, é”™è¯¯: {e}")
            return None
    
    def crawl_laws(self, target_laws: List[str]) -> List[Dict[str, Any]]:
        """æ‰¹é‡é‡‡é›†æ³•è§„"""
        self.logger.info("=== åŸºäºæœç´¢çš„æ³•è§„é‡‡é›†å™¨ ===")
        self.logger.info(f"ç›®æ ‡æ³•è§„æ•°: {len(target_laws)}")
        
        results = []
        success_count = 0
        
        for i, law_name in enumerate(target_laws, 1):
            self.logger.info(f"è¿›åº¦: {i}/{len(target_laws)}")
            
            result = self.crawl_law_by_search(law_name)
            
            if result:
                results.append(result)
                success_count += 1
                self.logger.info(f"âœ… æˆåŠŸé‡‡é›†: {result['title']}")
            else:
                self.logger.error(f"âŒ é‡‡é›†å¤±è´¥: {law_name}")
            
            self.logger.info("-" * 50)
        
        self.logger.info(f"\n=== é‡‡é›†å®Œæˆ ===")
        self.logger.info(f"æˆåŠŸ: {success_count}/{len(target_laws)}")
        self.logger.info(f"æˆåŠŸç‡: {success_count/len(target_laws)*100:.1f}%")
        
        return results 