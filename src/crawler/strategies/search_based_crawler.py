#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
åŸºäºæœç´¢çš„æ³•è§„é‡‡é›†å™¨ç­–ç•¥
å‚è€ƒç¤ºä¾‹é¡¹ç›®çš„æˆåŠŸæ–¹æ³•ï¼Œä½¿ç”¨æœç´¢API + è¯¦æƒ…APIçš„ç»„åˆæ–¹æ¡ˆ
"""

import json
import requests
import time
from datetime import datetime
from typing import List, Dict, Optional, Any
import re
from loguru import logger
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, WebDriverException
import sys
sys.path.append('..')
from ..base_crawler import BaseCrawler
import random
from urllib.parse import urljoin
import asyncio
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup
from ..utils.enhanced_proxy_pool import get_enhanced_proxy_pool, EnhancedProxyPool
from ..utils.ip_pool import get_ip_pool, SmartIPPool
from config.settings import get_settings


def normalize_date_format(date_str: str) -> str:
    """
    å°†å„ç§æ—¥æœŸæ ¼å¼ç»Ÿä¸€è½¬æ¢ä¸º yyyy-mm-dd æ ¼å¼
    æ”¯æŒçš„è¾“å…¥æ ¼å¼ï¼š
    - 2013å¹´2æœˆ4æ—¥ -> 2013-02-04
    - 2013-2-4 -> 2013-02-04
    - 2013.2.4 -> 2013-02-04
    - 2025-05-29 00:00:00 -> 2025-05-29
    """
    if not date_str or date_str.strip() == '':
        return ''
    
    import re
    from datetime import datetime
    
    date_str = str(date_str).strip()
    
    try:
        # æ ¼å¼1: 2013å¹´2æœˆ4æ—¥
        match = re.match(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', date_str)
        if match:
            year, month, day = match.groups()
            return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
        
        # æ ¼å¼2: 2013-2-4 æˆ– 2013/2/4 æˆ– 2013.2.4
        match = re.match(r'(\d{4})[-/.](\d{1,2})[-/.](\d{1,2})', date_str)
        if match:
            year, month, day = match.groups()
            return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
        
        # æ ¼å¼3: 2025-05-29 00:00:00 (å¸¦æ—¶é—´)
        match = re.match(r'(\d{4}-\d{2}-\d{2})\s+\d{2}:\d{2}:\d{2}', date_str)
        if match:
            return match.group(1)
        
        # æ ¼å¼4: å·²ç»æ˜¯ yyyy-mm-dd æ ¼å¼
        match = re.match(r'\d{4}-\d{2}-\d{2}$', date_str)
        if match:
            return date_str
        
        # æ ¼å¼5: å°è¯•ä½¿ç”¨datetimeè§£æ
        for fmt in ['%Y-%m-%d', '%Y/%m/%d', '%Y.%m.%d', '%Yå¹´%mæœˆ%dæ—¥']:
            try:
                dt = datetime.strptime(date_str, fmt)
                return dt.strftime('%Y-%m-%d')
            except:
                continue
        
        # å¦‚æœéƒ½æ— æ³•è§£æï¼Œè¿”å›åŸå§‹å­—ç¬¦ä¸²
        logger.warning(f"æ— æ³•è§£ææ—¥æœŸæ ¼å¼: {date_str}")
        return date_str
        
    except Exception as e:
        logger.warning(f"æ—¥æœŸæ ¼å¼åŒ–å¤±è´¥: {date_str}, é”™è¯¯: {e}")
        return date_str


class SearchBasedCrawler(BaseCrawler):
    """åŸºäºæœç´¢çš„æ³•è§„é‡‡é›†å™¨"""
    
    def __init__(self):
        super().__init__("search_api")
        self.logger = logger
        self.session = requests.Session()
        
        # ä»£ç†æ± ç›¸å…³
        self.enhanced_proxy_pool: Optional[EnhancedProxyPool] = None
        self.ip_pool: Optional[SmartIPPool] = None
        self.current_proxy = None
        self.proxy_failures = 0
        self.max_proxy_failures = 3
        self._proxy_pools_initialized = False
        
        self.setup_headers()
        self.base_url = "https://flk.npc.gov.cn"
        
        # WAFçŠ¶æ€è¿½è¸ª
        self.waf_triggered = False
        self.consecutive_waf_count = 0
        self.last_successful_time = time.time()
        
        # åˆå§‹åŒ–è®¿é—®ï¼Œè·å–å¿…è¦çš„cookies
        self._initialize_session()
    
    def setup_headers(self):
        """è®¾ç½®è¯·æ±‚å¤´ - ä½¿ç”¨åŸå§‹ç¤ºä¾‹é¡¹ç›®æˆåŠŸçš„headersé…ç½®"""
        # åŸºäºåŸå§‹ç¤ºä¾‹é¡¹ç›®ä¸­æˆåŠŸçš„REQUEST_HEADERé…ç½®
        self.session.headers.update({
            "authority": "flk.npc.gov.cn",
            "sec-ch-ua": '" Not A;Brand";v="99", "Chromium";v="99", "Microsoft Edge";v="99"',
            "accept": "application/json, text/javascript, */*; q=0.01",
            "x-requested-with": "XMLHttpRequest", 
            "sec-ch-ua-mobile": "?0",
            "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36 Edg/99.0.1150.39",
            "sec-ch-ua-platform": '"macOS"',
            "sec-fetch-site": "same-origin",
            "sec-fetch-mode": "cors", 
            "sec-fetch-dest": "empty",
            "referer": "https://flk.npc.gov.cn/fl.html",
            "accept-language": "en-AU,en-GB;q=0.9,en;q=0.8,en-US;q=0.7,zh-CN;q=0.6,zh;q=0.5",
            # å…³é”®ï¼šä½¿ç”¨ç¤ºä¾‹é¡¹ç›®ä¸­æˆåŠŸçš„cookieï¼ˆæ¨¡æ‹ŸçœŸå®ä¼šè¯ï¼‰
            "cookie": "yfx_c_g_u_id_10006696=_ck22022520424713255117764923111; cna=NdafGk8tiAgCAd9IPxhfROag; yfx_f_l_v_t_10006696=f_t_1645792967326__r_t_1646401808964__v_t_1646401808964__r_c_5; Hm_lvt_54434aa6770b6d9fef104d146430b53b=1646407223,1646570042,1646666110,1647148584; acw_tc=75a1461516471485843844814eb808af266b8ede0e0502ec1c46ab1581; Hm_lpvt_54434aa6770b6d9fef104d146430b53b=1647148626",
        })
        
        # è®¾ç½®Sessionçº§åˆ«çš„é…ç½®
        self.session.verify = True
        self.session.allow_redirects = True
        self.session.max_redirects = 5
    
    def _initialize_session(self):
        """åˆå§‹åŒ–sessionï¼Œè®¿é—®é¦–é¡µè·å–cookies"""
        try:
            # å…ˆè®¿é—®é¦–é¡µ
            home_response = self.session.get(
                "https://flk.npc.gov.cn/",
                timeout=10,
                allow_redirects=True
            )
            self.logger.debug(f"é¦–é¡µè®¿é—®çŠ¶æ€ç : {home_response.status_code}")
            
            # å†è®¿é—®æ³•è§„æœç´¢é¡µé¢
            search_page_response = self.session.get(
                "https://flk.npc.gov.cn/fl.html",
                timeout=10,
                allow_redirects=True
            )
            self.logger.debug(f"æœç´¢é¡µé¢è®¿é—®çŠ¶æ€ç : {search_page_response.status_code}")
            
            # ç­‰å¾…ä¸€ä¸‹ï¼Œæ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸º
            time.sleep(1)
            
        except Exception as e:
            self.logger.warning(f"åˆå§‹åŒ–sessionå¤±è´¥: {str(e)}")
            # ä¸ä¸­æ–­ï¼Œç»§ç»­å°è¯•
    
    async def search(self, law_name: str, law_number: Optional[str] = None) -> List[Dict[str, Any]]:
        """æœç´¢æ³•è§„ - å®ç°æŠ½è±¡æ–¹æ³•"""
        return self.search_law(law_name)
    
    async def get_detail(self, law_id: str) -> Dict[str, Any]:
        """è·å–æ³•è§„è¯¦æƒ… - å®ç°æŠ½è±¡æ–¹æ³•"""
        result = self.get_law_detail(law_id)
        return result or {}
    
    async def download_file(self, url: str, save_path: str) -> bool:
        """ä¸‹è½½æ–‡ä»¶ - å®ç°æŠ½è±¡æ–¹æ³•"""
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            with open(save_path, 'wb') as f:
                f.write(response.content)
            return True
        except Exception as e:
            self.logger.error(f"ä¸‹è½½æ–‡ä»¶å¤±è´¥: {str(e)}")
            return False
    
    def search_law_selenium(self, keyword: str) -> List[Dict[str, Any]]:
        """ä½¿ç”¨Seleniumæœç´¢æ³•è§„ - æ¨¡æ‹Ÿé¦–é¡µæœç´¢ï¼Œæ”¯æŒä»£ç†"""
        driver = None
        try:
            # é…ç½®Chromeé€‰é¡¹
            chrome_options = Options()
            chrome_options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36')
            
            # é…ç½®ä»£ç†ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            if self.current_proxy:
                proxy_url = self.current_proxy
                if proxy_url.startswith('http://'):
                    proxy_url = proxy_url[7:]  # ç§»é™¤http://å‰ç¼€
                elif proxy_url.startswith('https://'):
                    proxy_url = proxy_url[8:]  # ç§»é™¤https://å‰ç¼€
                
                chrome_options.add_argument(f'--proxy-server={proxy_url}')
                self.logger.debug(f"ğŸŒ Seleniumä½¿ç”¨ä»£ç†: {proxy_url}")
            
            # å¯åŠ¨æµè§ˆå™¨
            driver = webdriver.Chrome(options=chrome_options)
            wait = WebDriverWait(driver, 10)
            
            self.logger.debug(f"    Seleniumè®¿é—®é¦–é¡µè¿›è¡Œæœç´¢...")
            
            # è®¿é—®é¦–é¡µ
            driver.get("https://flk.npc.gov.cn/")
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            time.sleep(2)
            
            # æŸ¥æ‰¾æœç´¢è¾“å…¥æ¡†ï¼ˆæ ¹æ®HTMLç»“æ„ï¼‰
            search_input = wait.until(
                EC.presence_of_element_located((By.ID, "flfgTitle"))
            )
            
            # è¾“å…¥æœç´¢å…³é”®è¯
            search_input.clear()
            search_input.send_keys(keyword)
            
            self.logger.debug(f"    å·²è¾“å…¥å…³é”®è¯: {keyword}")
            
            # ç‚¹å‡»æœç´¢æŒ‰é’®ï¼ˆæ ¹æ®HTMLï¼Œæœç´¢æŒ‰é’®é€šè¿‡confirmFilter()å‡½æ•°è§¦å‘ï¼‰
            search_button = wait.until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, "li[onclick='confirmFilter()']"))
            )
            search_button.click()
            
            self.logger.debug(f"    å·²ç‚¹å‡»æœç´¢æŒ‰é’®")
            
            # ç­‰å¾…æœç´¢ç»“æœåŠ è½½
            time.sleep(3)
            
            # ç­‰å¾…ç»“æœè¡¨æ ¼å‡ºç°
            results_table = wait.until(
                EC.presence_of_element_located((By.ID, "flData"))
            )
            
            # è§£ææœç´¢ç»“æœ
            results = []
            rows = results_table.find_elements(By.CSS_SELECTOR, "tr.list-b")
            
            for row in rows:
                try:
                    # æå–æ ‡é¢˜å’Œé“¾æ¥
                    title_element = row.find_element(By.CSS_SELECTOR, ".l-wen")
                    title = title_element.get_attribute("title") or title_element.text.strip()
                    
                    # æå–è¯¦æƒ…é“¾æ¥å’ŒID
                    onclick_attr = title_element.get_attribute("onclick")
                    detail_url = None
                    law_id = ""
                    if onclick_attr and "showDetail" in onclick_attr:
                        # ä»onclickå±æ€§ä¸­æå–URL
                        import re
                        match = re.search(r"showDetail\('([^']+)'\)", onclick_attr)
                        if match:
                            detail_url = urljoin("https://flk.npc.gov.cn/", match.group(1))
                            # ä»URLä¸­æå–æ³•è§„IDï¼ˆé€šå¸¸åœ¨é—®å·åé¢ï¼‰
                            if "?" in detail_url:
                                law_id = detail_url.split("?")[1]
                    
                    # æå–åˆ¶å®šæœºå…³
                    agency_elements = row.find_elements(By.CSS_SELECTOR, ".l-sx2 .l-wen1")
                    agency = agency_elements[0].text.strip() if agency_elements else ""
                    
                    # æå–æ³•å¾‹æ€§è´¨
                    type_elements = row.find_elements(By.CSS_SELECTOR, ".l-sx3 .l-wen1")
                    law_type = type_elements[0].text.strip() if len(type_elements) > 0 else ""
                    
                    # æå–æ—¶æ•ˆæ€§å¹¶è½¬æ¢ä¸ºæ•°å­—çŠ¶æ€
                    status_text = type_elements[1].text.strip() if len(type_elements) > 1 else ""
                    status = 1  # é»˜è®¤æœ‰æ•ˆ
                    if "å·²ä¿®æ”¹" in status_text:
                        status = 5
                    elif "å·²åºŸæ­¢" in status_text:
                        status = 9
                    elif "å°šæœªç”Ÿæ•ˆ" in status_text:
                        status = 3
                    
                    # æå–å…¬å¸ƒæ—¥æœŸå¹¶æ ¼å¼åŒ–
                    date_elements = row.find_elements(By.CSS_SELECTOR, ".l-sx4 .l-wen1")
                    publish_date = ""
                    if date_elements:
                        date_text = date_elements[0].text.strip()
                        # ç§»é™¤æ–¹æ‹¬å·ï¼š[2024-12-25] -> 2024-12-25
                        publish_date = date_text.replace("[", "").replace("]", "")
                    
                    if title and law_id:
                        # æ„å»ºä¸HTTP APIä¸€è‡´çš„è¿”å›æ ¼å¼
                        results.append({
                            'id': law_id,
                            'title': title,
                            'link': detail_url,
                            'publish_date': publish_date,
                            'status': status,
                            'agency': agency,
                            'type': law_type,
                            'score': 1.0  # Seleniumæœç´¢çš„ç»“æœéƒ½è®¤ä¸ºæ˜¯é«˜åŒ¹é…åº¦
                        })
                        
                except Exception as e:
                    self.logger.debug(f"    è§£æè¡Œæ•°æ®å¤±è´¥: {e}")
                    continue
            
            self.logger.debug(f"    Seleniumæœç´¢æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
            return results
            
        except TimeoutException:
            self.logger.debug(f"    Seleniumæœç´¢è¶…æ—¶")
            return []
        except WebDriverException as e:
            self.logger.debug(f"    Selenium WebDriveré”™è¯¯: {e}")
            return []
        except Exception as e:
            self.logger.debug(f"    Seleniumæœç´¢å¤±è´¥: {e}")
            return []
        finally:
            if driver:
                try:
                    driver.quit()
                except:
                    pass
    
    def _initialize_proxy_pools_sync(self):
        """åˆå§‹åŒ–ä»£ç†æ±  - å®Œå…¨ç¦ç”¨"""
        # å®Œå…¨ç¦ç”¨ä»£ç†æ± åˆå§‹åŒ–ï¼Œä½¿ç”¨ç›´è¿æ¨¡å¼
        self.logger.info("ğŸš€ ä½¿ç”¨ç›´è¿æ¨¡å¼ï¼Œè·³è¿‡ä»£ç†æ± åˆå§‹åŒ–")
        return
        
        # # åŸä»£ç†æ± åˆå§‹åŒ–é€»è¾‘å·²ç¦ç”¨
        # if self._proxy_pools_initialized:
        #     return
        # 
        # self.logger.info("ğŸ”„ å¼€å§‹åˆå§‹åŒ–ä»£ç†æ± ...")
        # 
        # try:
        #     # 1. ä¼˜å…ˆä½¿ç”¨enhanced_proxy_pool
        #     if not self.enhanced_proxy_pool:
        #         settings = get_settings()
        #         if settings.proxy_pool.enabled:
        #             loop = asyncio.new_event_loop()
        #             asyncio.set_event_loop(loop)
        #             try:
        #                 from ..utils.enhanced_proxy_pool import get_enhanced_proxy_pool
        #                 self.enhanced_proxy_pool = loop.run_until_complete(
        #                     get_enhanced_proxy_pool(settings.proxy_pool.config_file)
        #                 )
        #                 self.logger.success("ğŸ”„ Enhancedä»£ç†æ± åˆå§‹åŒ–æˆåŠŸ")
        #             except Exception as e:
        #                 self.logger.warning(f"Enhancedä»£ç†æ± åˆå§‹åŒ–å¤±è´¥: {e}")
        #             finally:
        #                 loop.close()
        # 
        # except Exception as e:
        #     self.logger.warning(f"ä»£ç†æ± åˆå§‹åŒ–å¤±è´¥: {e}")
        # 
        # self._proxy_pools_initialized = True

    def _get_proxy_for_request_sync(self):
        """è·å–ä»£ç† - å®Œå…¨ç¦ç”¨ï¼Œå¼ºåˆ¶ç›´è¿"""
        # å®Œå…¨ç¦ç”¨ä»£ç†ï¼Œä½¿ç”¨ç›´è¿æ¨¡å¼æé«˜é€Ÿåº¦å’Œç¨³å®šæ€§
        return None
        
        # # åŸä»£ç†é€»è¾‘å·²ç¦ç”¨
        # if self.enhanced_proxy_pool:
        #     try:
        #         proxy_info = self.enhanced_proxy_pool.get_proxy_sync(prefer_paid=True)
        #         if proxy_info:
        #             self.logger.debug(f"ğŸŒ ä½¿ç”¨Enhancedä»£ç†: {proxy_info.name}")
        #             return proxy_info.proxy_url
        #     except Exception as e:
        #         self.logger.debug(f"Enhancedä»£ç†è·å–å¤±è´¥: {e}")
        # 
        # if self.ip_pool:
        #     try:
        #         proxy = self.ip_pool.get_proxy_sync()
        #         if proxy:
        #             self.logger.debug(f"ğŸŒ ä½¿ç”¨IPæ± ä»£ç†: {proxy.ip}:{proxy.port}")
        #             return proxy.proxy_url
        #     except Exception as e:
        #         self.logger.debug(f"IPæ± ä»£ç†è·å–å¤±è´¥: {e}")
        # 
        # return None

    def _configure_session_proxy(self, proxy_url: str = None):
        """é…ç½®sessionçš„ä»£ç†"""
        if proxy_url:
            # è§£æä»£ç†URL
            if proxy_url.startswith('http://') or proxy_url.startswith('https://'):
                proxies = {
                    'http': proxy_url,
                    'https': proxy_url,
                }
            elif proxy_url.startswith('socks5://'):
                proxies = {
                    'http': proxy_url,
                    'https': proxy_url,
                }
            else:
                # é»˜è®¤HTTPä»£ç†
                proxies = {
                    'http': f'http://{proxy_url}',
                    'https': f'http://{proxy_url}',
                }
            
            self.session.proxies.update(proxies)
            self.current_proxy = proxy_url
            self.logger.info(f"âœ… Sessionä»£ç†å·²é…ç½®: {proxy_url}")
        else:
            # æ¸…é™¤ä»£ç†ï¼Œä½¿ç”¨ç›´è¿
            if self.session.proxies:
                self.session.proxies.clear()
                self.current_proxy = None
                self.logger.debug("ğŸ”„ ä½¿ç”¨ç›´è¿æ¨¡å¼ï¼Œä»£ç†å·²æ¸…é™¤")

    def _rotate_proxy_on_waf_sync(self):
        """WAFæ£€æµ‹æ—¶è½®æ¢ä»£ç† - åŒæ­¥ç‰ˆæœ¬"""
        self.proxy_failures += 1
        
        if self.proxy_failures >= self.max_proxy_failures:
            self.logger.warning(f"ğŸ”„ ä»£ç†è¿ç»­å¤±è´¥{self.proxy_failures}æ¬¡ï¼Œå¼ºåˆ¶è½®æ¢")
            
            # æ ‡è®°å½“å‰ä»£ç†å¤±è´¥
            if self.current_proxy and self.enhanced_proxy_pool:
                try:
                    # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå¼‚æ­¥æ“ä½œ
                    import concurrent.futures
                    
                    def mark_failed():
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                        try:
                            return loop.run_until_complete(
                                self.enhanced_proxy_pool.mark_proxy_failed(self.current_proxy)
                            )
                        finally:
                            loop.close()
                    
                    with concurrent.futures.ThreadPoolExecutor() as executor:
                        future = executor.submit(mark_failed)
                        future.result(timeout=5)
                        
                except Exception as e:
                    self.logger.debug(f"æ ‡è®°ä»£ç†å¤±è´¥æ—¶å‡ºé”™: {e}")
            
            # è·å–æ–°ä»£ç†
            new_proxy = self._get_proxy_for_request_sync()
            self._configure_session_proxy(new_proxy)
            
            self.proxy_failures = 0  # é‡ç½®å¤±è´¥è®¡æ•°
            
            # æ·»åŠ è½®æ¢å»¶è¿Ÿ
            delay = random.uniform(3, 8)
            self.logger.info(f"â±ï¸ ä»£ç†è½®æ¢å»¶è¿Ÿ: {delay:.1f}ç§’")
            time.sleep(delay)

    def search_law(self, keyword: str) -> List[Dict[str, Any]]:
        """æœç´¢æ³•è§„ - æ™ºèƒ½ç‰ˆï¼šä¼˜å…ˆAPIï¼ŒWAFæ¿€æ´»æ—¶è‡ªåŠ¨åˆ‡æ¢Seleniumï¼Œæ”¯æŒä»£ç†è½®æ¢"""
        
        # å¦‚æœWAFå·²æ¿€æ´»ï¼Œç›´æ¥ä½¿ç”¨Selenium
        if self.waf_triggered:
            self.logger.debug(f"    WAFå·²æ¿€æ´»ï¼Œç›´æ¥ä½¿ç”¨Seleniumæœç´¢")
            # ç¡®ä¿Seleniumä¹Ÿä½¿ç”¨ä»£ç†
            if not self.current_proxy:
                proxy_url = self._get_proxy_for_request_sync()
                self._configure_session_proxy(proxy_url)
                if proxy_url:
                    self.logger.info(f"ğŸŒ ä¸ºSeleniumè·å–ä»£ç†: {proxy_url}")
            return self.search_law_selenium(keyword)
        
        # å°è¯•HTTP APIæœç´¢
        results = self._search_law_http(keyword)
        
        # æ£€æŸ¥ç»“æœå¹¶å¤„ç†WAFçŠ¶æ€
        if results:
            self._handle_waf_detection(False)  # æˆåŠŸï¼Œé‡ç½®WAFçŠ¶æ€
            return results
        else:
            # APIå¤±è´¥ï¼Œå¯èƒ½æ˜¯WAFæ‹¦æˆªï¼Œå°è¯•Seleniumå¤‡ç”¨
            self.logger.debug(f"    HTTP APIå¤±è´¥ï¼Œå°è¯•Seleniumå¤‡ç”¨æœç´¢")
            # ç¡®ä¿Seleniumä¹Ÿä½¿ç”¨ä»£ç†
            if not self.current_proxy:
                proxy_url = self._get_proxy_for_request_sync()
                self._configure_session_proxy(proxy_url)
                if proxy_url:
                    self.logger.info(f"ğŸŒ ä¸ºSeleniumè·å–ä»£ç†: {proxy_url}")
            
            selenium_results = self.search_law_selenium(keyword)
            
            # å¦‚æœSeleniumæˆåŠŸè€ŒAPIå¤±è´¥ï¼Œè¯´æ˜å¯èƒ½æ˜¯WAFé—®é¢˜
            if selenium_results:
                self._handle_waf_detection(True)  # æ ‡è®°å¯èƒ½çš„WAFæ‹¦æˆª
            
            return selenium_results
    
    def _search_law_http(self, keywords, search_type="title;vague"):
        """HTTP APIæœç´¢æ³•è§„"""
        strategies = [
            "title;vague",
            "title;accurate;1,3",
            "title;accurate;2,4", 
            "all;accurate;1,3"
        ]
        
        if search_type not in strategies:
            strategies = [search_type] + strategies
        
        for strategy in strategies:
            self.logger.debug(f"    ğŸ” å°è¯•APIæœç´¢ç­–ç•¥: {strategy}")
            
            # å®Œå…¨ä½¿ç”¨ç›´è¿æ¨¡å¼ï¼Œç¦ç”¨ä»£ç†
            self._configure_session_proxy(None)  # å¼ºåˆ¶æ¸…é™¤ä»£ç†
            
            # æé€Ÿä¼˜åŒ–å»¶è¿Ÿï¼š0.1-0.5ç§’
            delay = random.uniform(0.1, 0.5)
            self.logger.debug(f"    â±ï¸ æ·»åŠ éšæœºå»¶è¿Ÿ: {delay:.1f}ç§’")
            time.sleep(delay)
            
            try:
                # æ„å»ºæŸ¥è¯¢å‚æ•°
                params = {
                    "type": "flfg",
                    "searchType": strategy,
                    "sortTr": "f_bbrq_s;desc",
                    "gbrqStart": "",
                    "gbrqEnd": "",
                    "sxrqStart": "",
                    "sxrqEnd": "",
                    "sort": "true",
                    "page": "1",
                    "size": "20",
                    "fgbt": keywords,
                    "_": str(int(time.time() * 1000))
                }
                
                url = f"{self.base_url}/api/"
                
                # ç›´è¿è¯·æ±‚ï¼Œè¶…æ—¶ä¼˜åŒ–
                response = self.session.get(
                    url, 
                    params=params,
                    timeout=(3, 8),  # è¿æ¥è¶…æ—¶3ç§’ï¼Œè¯»å–è¶…æ—¶8ç§’
                    verify=False
                )
                
                if response.status_code == 200:
                    try:
                        data = response.json()
                        if data.get("result") and data["result"].get("data"):
                            results = data["result"]["data"]
                            self.logger.success(f"    âœ… ç›´è¿APIæœç´¢æˆåŠŸ (ç­–ç•¥: {strategy}): æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
                            return results
                        else:
                            self.logger.debug(f"    ğŸ“‹ APIæœç´¢æ— ç»“æœ (ç­–ç•¥: {strategy})")
                    except ValueError as e:
                        self.logger.warning(f"    âš ï¸ APIå“åº”JSONè§£æå¤±è´¥ (ç­–ç•¥: {strategy}): {e}")
                else:
                    self.logger.debug(f"    âŒ APIè¯·æ±‚å¤±è´¥ (ç­–ç•¥: {strategy}): HTTP {response.status_code}")
                    
            except Exception as e:
                self.logger.debug(f"    âŒ APIè¯·æ±‚å¼‚å¸¸ (ç­–ç•¥: {strategy}): {e}")
                continue
        
        self.logger.debug(f"    ğŸš« æ‰€æœ‰APIæœç´¢ç­–ç•¥éƒ½å¤±è´¥")
        return []
    
    def _check_waf_response(self, response) -> bool:
        """æ£€æŸ¥å“åº”æ˜¯å¦è¢«WAFæ‹¦æˆª"""
        # æ£€æŸ¥Content-Type
        content_type = response.headers.get('Content-Type', '').lower()
        if 'text/html' in content_type:
            return True
        
        # æ£€æŸ¥WAFæ ‡è¯†
        if 'WZWS-RAY' in response.headers:
            return True
        
        # æ£€æŸ¥å“åº”å†…å®¹
        if "<!DOCTYPE HTML>" in response.text and "JavaScript" in response.text:
            return True
        
        return False
    
    def _handle_waf_detection(self, waf_detected: bool):
        """å¤„ç†WAFæ£€æµ‹ç»“æœ"""
        if waf_detected:
            self.consecutive_waf_count += 1
            if self.consecutive_waf_count >= 2:  # è¿ç»­2æ¬¡è¢«æ‹¦æˆªæ‰è®¤ä¸ºWAFæ¿€æ´»
                self.waf_triggered = True
                self.logger.warning(f"ğŸš« WAFå·²æ¿€æ´»ï¼Œè¿ç»­æ‹¦æˆª{self.consecutive_waf_count}æ¬¡ï¼Œåˆ‡æ¢åˆ°Seleniumæ¨¡å¼")
        else:
            # æˆåŠŸè¯·æ±‚ï¼Œé‡ç½®è®¡æ•°å™¨
            self.consecutive_waf_count = 0
            self.last_successful_time = time.time()
            if self.waf_triggered:
                self.logger.info("âœ… APIæ¢å¤æ­£å¸¸ï¼ŒWAFå¯èƒ½å·²è§£é™¤")
                self.waf_triggered = False
    
    def get_law_detail(self, law_id: str) -> Optional[Dict[str, Any]]:
        """è·å–æ³•è§„è¯¦æƒ…"""
        try:
            response = self.session.post(
                "https://flk.npc.gov.cn/api/detail",
                data={"id": law_id},
                timeout=10
            )
            
            if response.status_code == 200:
                result = response.json()
                if result.get('success') and result.get('result'):
                    return result['result']
            
            return None
            
        except Exception as e:
            self.logger.error(f"è·å–æ³•è§„è¯¦æƒ…å¤±è´¥: {str(e)}")
            return None
    
    def normalize_law_name(self, law_name: str) -> str:
        """æ ‡å‡†åŒ–æ³•è§„åç§°"""
        # ç§»é™¤æ‹¬å·å†…å®¹
        normalized = re.sub(r'[ï¼ˆ(].*?[ï¼‰)]', '', law_name)
        # ç§»é™¤"ä¿®è®¢"ã€"ä¿®æ­£"ç­‰åç¼€
        normalized = re.sub(r'ï¼ˆ\d{4}.*?ä¿®è®¢.*?ï¼‰', '', normalized)
        normalized = re.sub(r'ï¼ˆ\d{4}.*?ä¿®æ­£.*?ï¼‰', '', normalized)
        # ç§»é™¤ä¸»å¸­ä»¤ç­‰ç¼–å·
        normalized = re.sub(r'ï¼ˆ.*?ä¸»å¸­ä»¤.*?ï¼‰', '', normalized)
        normalized = re.sub(r'ï¼ˆ.*?ä»¤.*?ï¼‰', '', normalized)
        # æ¸…ç†å¤šä½™ç©ºæ ¼
        normalized = re.sub(r'\s+', '', normalized)
        
        return normalized.strip()
    
    def calculate_match_score(self, target: str, result: str) -> float:
        """è®¡ç®—åŒ¹é…åˆ†æ•° - æ”¹è¿›ç‰ˆ"""
        if not target or not result:
            return 0.0
            
        # å®Œå…¨åŒ¹é…
        if target == result:
            return 1.0
        
        # æ ‡å‡†åŒ–å¤„ç†
        target_clean = re.sub(r'[ï¼ˆ(].*?[ï¼‰)]', '', target).strip()
        result_clean = re.sub(r'[ï¼ˆ(].*?[ï¼‰)]', '', result).strip()
        
        # å»æ‰ä¿®è®¢å¹´ä»½åçš„åŒ¹é…
        if target_clean == result_clean:
            return 0.95
        
        # åŒ…å«å…³ç³» - ä½†è¦é¿å…åŒ¹é…åˆ°å¸æ³•è§£é‡Š
        if "è§£é‡Š" in result and "è§£é‡Š" not in target:
            return 0.1  # å¤§å¹…é™ä½å¸æ³•è§£é‡Šçš„åŒ¹é…åˆ†æ•°
        
        if "æ„è§" in result and "æ„è§" not in target:
            return 0.1  # å¤§å¹…é™ä½æ„è§ç±»æ–‡ä»¶çš„åŒ¹é…åˆ†æ•°
            
        # æ£€æŸ¥æ ¸å¿ƒå…³é”®è¯åŒ¹é…
        target_core = target_clean.replace("ä¸­åäººæ°‘å…±å’Œå›½", "")
        result_core = result_clean.replace("ä¸­åäººæ°‘å…±å’Œå›½", "")
        
        if target_core and result_core:
            # å®Œå…¨åŒ¹é…æ ¸å¿ƒéƒ¨åˆ†
            if target_core == result_core:
                return 0.9
            
            # åŒ…å«å…³ç³»
            if target_core in result_core:
                ratio = len(target_core) / len(result_core)
                return 0.7 + ratio * 0.2
            if result_core in target_core:
                ratio = len(result_core) / len(target_core)
                return 0.7 + ratio * 0.2
        
        # è®¡ç®—å…¬å…±å­ä¸²
        common_length = 0
        min_len = min(len(target), len(result))
        for i in range(min_len):
            if target[i] == result[i]:
                common_length += 1
            else:
                break
        
        if common_length > 0:
            base_score = common_length / max(len(target), len(result))
            # å¦‚æœå…¬å…±å‰ç¼€å¾ˆé•¿ï¼Œç»™æ›´é«˜åˆ†æ•°
            if common_length >= 6:
                return min(0.8, base_score * 1.2)
            return base_score
        
        return 0.0
    
    def find_best_match(self, target_name: str, search_results: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """åœ¨æœç´¢ç»“æœä¸­æ‰¾åˆ°æœ€ä½³åŒ¹é… - ä¼˜åŒ–ç‰ˆï¼Œä¼˜å…ˆé€‰æ‹©æœ‰æ•ˆæ³•è§„"""
        if not search_results:
            return None
        
        # æ ‡å‡†åŒ–ç›®æ ‡åç§°
        target_normalized = self.normalize_law_name(target_name)
        
        # è®°å½•æ‰€æœ‰å€™é€‰é¡¹çš„åˆ†æ•°å’ŒçŠ¶æ€
        candidates = []
        
        for law in search_results:
            law_title = law.get('title', '')
            law_normalized = self.normalize_law_name(law_title)
            
            # è®¡ç®—åŒ¹é…åˆ†æ•°
            score = self.calculate_match_score(target_normalized, law_normalized)
            
            # è·å–æ³•è§„çŠ¶æ€ (1=æœ‰æ•ˆ, 5=å¤±æ•ˆ) - å¤„ç†å­—ç¬¦ä¸²ç±»å‹
            status = law.get('status', 0)
            # ç¡®ä¿çŠ¶æ€æ˜¯æ•´æ•°ç±»å‹
            try:
                status_int = int(status)
            except (ValueError, TypeError):
                status_int = 0
            
            is_valid = (status_int == 1)
            
            candidates.append({
                'title': law_title,
                'score': score,
                'law': law,
                'status': status_int,
                'is_valid': is_valid,
                'status_text': 'æœ‰æ•ˆ' if is_valid else 'å¤±æ•ˆ'
            })
        
        # æŒ‰åˆ†æ•°æ’åº
        candidates.sort(key=lambda x: x['score'], reverse=True)
        
        # è°ƒè¯•ä¿¡æ¯
        self.logger.debug(f"    åŒ¹é…å€™é€‰é¡¹:")
        for candidate in candidates[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
            self.logger.debug(f"      {candidate['title']}: {candidate['score']:.3f} ({candidate['status_text']})")
        
        # è®¾ç½®åŒ¹é…é˜ˆå€¼
        threshold = 0.75
        
        # ç­›é€‰å‡ºè¾¾åˆ°é˜ˆå€¼çš„å€™é€‰é¡¹
        qualified_candidates = [c for c in candidates if c['score'] >= threshold]
        
        if not qualified_candidates:
            self.logger.debug(f"    æ²¡æœ‰å€™é€‰é¡¹è¾¾åˆ°é˜ˆå€¼ {threshold}")
            return None
        
        # ä¼˜å…ˆçº§é€‰æ‹©é€»è¾‘
        # 1. ä¼˜å…ˆé€‰æ‹©æœ‰æ•ˆçš„æ³•è§„
        valid_candidates = [c for c in qualified_candidates if c['is_valid']]
        
        if valid_candidates:
            # æœ‰æœ‰æ•ˆæ³•è§„ï¼Œé€‰æ‹©åˆ†æ•°æœ€é«˜çš„æœ‰æ•ˆæ³•è§„
            best_candidate = valid_candidates[0]  # å·²æŒ‰åˆ†æ•°æ’åº
            self.logger.debug(f"    ä¼˜å…ˆé€‰æ‹©æœ‰æ•ˆæ³•è§„: {best_candidate['title']} (åˆ†æ•°: {best_candidate['score']:.3f}, çŠ¶æ€: {best_candidate['status_text']})")
            return best_candidate['law']
        else:
            # æ²¡æœ‰æœ‰æ•ˆæ³•è§„ï¼Œé€‰æ‹©åˆ†æ•°æœ€é«˜çš„å¤±æ•ˆæ³•è§„
            best_candidate = qualified_candidates[0]  # å·²æŒ‰åˆ†æ•°æ’åº
            self.logger.debug(f"    æ— æœ‰æ•ˆæ³•è§„ï¼Œé€‰æ‹©å¤±æ•ˆæ³•è§„: {best_candidate['title']} (åˆ†æ•°: {best_candidate['score']:.3f}, çŠ¶æ€: {best_candidate['status_text']})")
            return best_candidate['law']
    
    def generate_search_keywords(self, law_name: str) -> List[str]:
        """ç”Ÿæˆæœç´¢å…³é”®è¯"""
        keywords = []
        
        # 1. å®Œæ•´åç§°
        keywords.append(law_name)
        
        # 2. ç§»é™¤"ä¸­åäººæ°‘å…±å’Œå›½"å‰ç¼€
        if law_name.startswith("ä¸­åäººæ°‘å…±å’Œå›½"):
            keywords.append(law_name.replace("ä¸­åäººæ°‘å…±å’Œå›½", ""))
        
        # 3. æå–ä¸»å¹²åç§°ï¼ˆç§»é™¤æ‹¬å·å†…å®¹ï¼‰
        main_name = re.sub(r'[ï¼ˆ(].*?[ï¼‰)]', '', law_name)
        if main_name != law_name and main_name.strip():
            keywords.append(main_name.strip())
        
        # 4. æå–æ ¸å¿ƒè¯æ±‡
        if "æ³•" in law_name:
            # æå–"æ³•"å‰é¢çš„éƒ¨åˆ†
            parts = law_name.split("æ³•")
            if parts[0]:
                core_name = parts[0] + "æ³•"
                if core_name not in keywords:
                    keywords.append(core_name)
        
        # 5. å¯¹äºåŠæ³•ã€æ¡ä¾‹ç­‰ï¼Œå°è¯•ä¸åŒçš„æœç´¢ç­–ç•¥
        if any(word in law_name for word in ["åŠæ³•", "æ¡ä¾‹", "è§„å®š", "ç»†åˆ™"]):
            # æå–å…³é”®è¯ç»„åˆ
            for suffix in ["åŠæ³•", "æ¡ä¾‹", "è§„å®š", "ç»†åˆ™"]:
                if suffix in law_name:
                    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå…³é”®è¯
                    parts = law_name.split(suffix)
                    if parts[0]:
                        # å°è¯•ä¸åŒé•¿åº¦çš„å…³é”®è¯
                        base = parts[0].strip()
                        # ç§»é™¤ä¿®è®¢å¹´ä»½
                        base = re.sub(r'[ï¼ˆ(].*?[ï¼‰)]', '', base).strip()
                        if base and len(base) >= 4:  # è‡³å°‘4ä¸ªå­—ç¬¦
                            keywords.append(base + suffix)
                            # å°è¯•æ›´çŸ­çš„å…³é”®è¯
                            if len(base) > 6:
                                keywords.append(base[-6:] + suffix)
        
        # å»é‡å¹¶è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
        keywords = list(dict.fromkeys([k for k in keywords if k.strip()]))  # ä¿æŒé¡ºåºçš„å»é‡
        
        return keywords
    
    def extract_document_number(self, detail: dict) -> str:
        """æå–æ–‡å·"""
        try:
            # æ–¹æ³•1: ä»otherFileåˆ—è¡¨ä¸­çš„ä¸»å¸­ä»¤æ–‡ä»¶åæå–æ–‡å·
            other_files = detail.get('otherFile', [])
            if isinstance(other_files, list):
                for file_info in other_files:
                    if isinstance(file_info, dict):
                        file_name = file_info.get('name', '')
                        if 'ä¸»å¸­ä»¤' in file_name:
                            # æå–ä¸»å¸­ä»¤å·ç 
                            import re
                            # åŒ¹é…å„ç§ä¸»å¸­ä»¤æ ¼å¼
                            patterns = [
                                r'ä¸»å¸­ä»¤ï¼ˆç¬¬(\w+)å·ï¼‰',  # ä¸»å¸­ä»¤ï¼ˆç¬¬ä¸‰åä¸€å·ï¼‰
                                r'ä¸»å¸­ä»¤ç¬¬(\w+)å·',      # ä¸»å¸­ä»¤ç¬¬ä¸€äºŒã€‡å·
                                r'ä¸»å¸­ä»¤.*?(\d+)å·',     # åŒ…å«æ•°å­—çš„ä¸»å¸­ä»¤
                            ]
                            for pattern in patterns:
                                match = re.search(pattern, file_name)
                                if match:
                                    return f"ä¸»å¸­ä»¤ç¬¬{match.group(1)}å·"
            
            # æ–¹æ³•2: ä»æ ‡é¢˜ä¸­æå–ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
            title = detail.get('title', '')
            if title:
                # ç®€å•çš„æ–‡å·æå–é€»è¾‘ï¼Œå¯ä»¥æ ¹æ®éœ€è¦æ‰©å±•
                return ""
            
            return ""
        except Exception as e:
            self.logger.error(f"æå–æ–‡å·æ—¶å‡ºé”™: {e}")
            return ""
    
    def crawl_law_by_search(self, law_name: str) -> Optional[Dict[str, Any]]:
        """é€šè¿‡æœç´¢é‡‡é›†å•ä¸ªæ³•è§„"""
        self.logger.info(f"æœç´¢æ³•è§„: {law_name}")
        
        # ç”Ÿæˆæœç´¢å…³é”®è¯
        keywords = self.generate_search_keywords(law_name)
        self.logger.debug(f"æœç´¢å…³é”®è¯: {keywords}")
        
        for keyword in keywords:
            self.logger.debug(f"  å°è¯•å…³é”®è¯: {keyword}")
            
            search_results = self.search_law(keyword)
            
            if search_results:
                self.logger.debug(f"    æ‰¾åˆ° {len(search_results)} ä¸ªç»“æœ")
                
                # æ‰¾åˆ°æœ€ä½³åŒ¹é…
                best_match = self.find_best_match(law_name, search_results)
                
                if best_match:
                    self.logger.info(f"    âœ… æ‰¾åˆ°åŒ¹é…: {best_match.get('title', '')}")
                    self.logger.debug(f"    ID: {best_match.get('id', '')}")
                    
                    # è·å–è¯¦ç»†ä¿¡æ¯
                    detail = self.get_law_detail(best_match['id'])
                    if not detail:
                        self.logger.error(f"  âŒ æ— æ³•è·å–è¯¦ç»†ä¿¡æ¯")
                        return None
                    
                    # æå–å’Œæ•´ç†æ–‡ä»¶ä¿¡æ¯
                    body_files = detail.get('body', [])
                    other_files = detail.get('otherFile', [])
                    
                    # æ•´ç†æ­£æ–‡æ–‡ä»¶ä¿¡æ¯
                    formatted_body_files = []
                    if isinstance(body_files, list):
                        for file_info in body_files:
                            if isinstance(file_info, dict):
                                formatted_body_files.append({
                                    'type': file_info.get('type', ''),
                                    'path': file_info.get('path', ''),
                                    'url': file_info.get('url', ''),
                                    'mobile_url': file_info.get('mobile', ''),
                                    'addr': file_info.get('addr', '')
                                })
                    
                    # æ•´ç†å…¶ä»–æ–‡ä»¶ä¿¡æ¯
                    formatted_other_files = []
                    if isinstance(other_files, list):
                        for file_info in other_files:
                            if isinstance(file_info, dict):
                                formatted_other_files.append({
                                    'name': file_info.get('name', ''),
                                    'type': file_info.get('type', ''),
                                    'hdfs_path': file_info.get('hdfsPath', ''),
                                    'oss_path': file_info.get('ossPath', ''),
                                    'order': file_info.get('order', '')
                                })
                    
                    result_data = {
                        # åŸºæœ¬ä¿¡æ¯
                        'law_id': best_match.get('id', ''),
                        'target_name': law_name,
                        'search_keyword': keyword,
                        'title': detail.get('title', ''),
                        'document_number': self.extract_document_number(detail),
                        
                        # æ—¥æœŸä¿¡æ¯
                        'publish_date': normalize_date_format(detail.get('publish', '')),
                        'implement_date': normalize_date_format(detail.get('expiry', '')),
                        'invalidate_date': '',  # APIä¸­æ²¡æœ‰å¤±æ•ˆæ—¥æœŸå­—æ®µ
                        
                        # æœºå…³å’Œåˆ†ç±»ä¿¡æ¯
                        'office': detail.get('office', ''),
                        'level': detail.get('level', ''),  # æ³•è§„çº§åˆ«ï¼šæ³•å¾‹ã€è¡Œæ”¿æ³•è§„ã€å¸æ³•è§£é‡Šç­‰
                        'status': 'æœ‰æ•ˆ' if detail.get('status', '') == '1' else 'å¤±æ•ˆ',
                        
                        # æ–‡ä»¶ä¿¡æ¯
                        'body_files': formatted_body_files,  # æ­£æ–‡æ–‡ä»¶ï¼ˆWORD/PDF/HTMLï¼‰
                        'other_files': formatted_other_files,  # å…¶ä»–æ–‡ä»¶ï¼ˆä¸»å¸­ä»¤ç­‰ï¼‰
                        
                        # åŸå§‹æ•°æ®
                        'raw_api_response': detail,  # ä¿å­˜å®Œæ•´çš„APIå“åº”
                        
                        # å…ƒæ•°æ®
                        'source_url': f"https://flk.npc.gov.cn/detail2.html?id={best_match.get('id', '')}",
                        'crawl_date': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        'crawler_version': '1.0.0'
                    }
                    
                    self.logger.info(f"    ğŸ”¢ æ–‡å·: {result_data['document_number']}")
                    self.logger.info(f"    ğŸ“… å‘å¸ƒæ—¥æœŸ: {result_data['publish_date']}")
                    self.logger.info(f"    ğŸ“… å®æ–½æ—¥æœŸ: {result_data['implement_date']}")
                    self.logger.info(f"    ğŸš¦ çŠ¶æ€: {result_data['status']} (statusåŸå€¼: {detail.get('status', '')})")
                    self.logger.info(f"    ğŸ›ï¸ å‘å¸ƒæœºå…³: {result_data['office']}")
                    self.logger.info(f"    ğŸ“‹ æ³•è§„çº§åˆ«: {result_data['level']}")
                    
                    return result_data
                else:
                    self.logger.warning(f"    âŒ æœªæ‰¾åˆ°åŒ¹é…")
            else:
                self.logger.warning(f"    âŒ æœç´¢æ— ç»“æœ")
            
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        self.logger.error(f"  âŒ æ‰€æœ‰å…³é”®è¯éƒ½æœªæ‰¾åˆ°åŒ¹é…")
        return None
    
    async def crawl_law(self, law_name: str, law_number: str = None) -> Optional[Dict]:
        """çˆ¬å–å•ä¸ªæ³•å¾‹ï¼ˆå®ç°CrawlerManageréœ€è¦çš„æ¥å£ï¼‰"""
        try:
            self.logger.info(f"äººå¤§ç½‘çˆ¬å–: {law_name}")
            
            result = self.crawl_law_by_search(law_name)
            
            if result:
                # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                return {
                    'law_id': result.get('law_id', ''),
                    'name': result.get('title', ''),
                    'number': result.get('document_number', ''),
                    'source': 'search_api',
                    'success': True,
                    'source_url': result.get('source_url', ''),
                    'publish_date': result.get('publish_date', ''),
                    'valid_from': result.get('implement_date', ''),
                    'valid_to': result.get('invalidate_date', ''),
                    'office': result.get('office', ''),
                    'status': result.get('status', ''),
                    'level': result.get('level', ''),
                    'crawl_time': result.get('crawl_date', ''),
                    'raw_data': result
                }
            else:
                return None
                
        except Exception as e:
            self.logger.error(f"äººå¤§ç½‘çˆ¬å–å¤±è´¥: {law_name}, é”™è¯¯: {e}")
            return None
    
    def crawl_laws(self, target_laws: List[str]) -> List[Dict[str, Any]]:
        """æ‰¹é‡é‡‡é›†æ³•è§„"""
        self.logger.info("=== åŸºäºæœç´¢çš„æ³•è§„é‡‡é›†å™¨ ===")
        self.logger.info(f"ç›®æ ‡æ³•è§„æ•°: {len(target_laws)}")
        
        results = []
        success_count = 0
        
        for i, law_name in enumerate(target_laws, 1):
            self.logger.info(f"è¿›åº¦: {i}/{len(target_laws)}")
            
            result = self.crawl_law_by_search(law_name)
            
            if result:
                results.append(result)
                success_count += 1
                self.logger.info(f"âœ… æˆåŠŸé‡‡é›†: {result['title']}")
            else:
                self.logger.error(f"âŒ é‡‡é›†å¤±è´¥: {law_name}")
            
            self.logger.info("-" * 50)
        
        self.logger.info(f"\n=== é‡‡é›†å®Œæˆ ===")
        self.logger.info(f"æˆåŠŸ: {success_count}/{len(target_laws)}")
        self.logger.info(f"æˆåŠŸç‡: {success_count/len(target_laws)*100:.1f}%")
        
        return results 