#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
åŸºäºæœç´¢çš„æ³•è§„é‡‡é›†å™¨ç­–ç•¥
å‚è€ƒç¤ºä¾‹é¡¹ç›®çš„æˆåŠŸæ–¹æ³•ï¼Œä½¿ç”¨æœç´¢API + è¯¦æƒ…APIçš„ç»„åˆæ–¹æ¡ˆ
"""

import json
import requests
import time
from datetime import datetime
from typing import List, Dict, Optional, Any
import re
from loguru import logger
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, WebDriverException
import sys
sys.path.append('..')
from ..base_crawler import BaseCrawler
import random


def normalize_date_format(date_str: str) -> str:
    """
    å°†å„ç§æ—¥æœŸæ ¼å¼ç»Ÿä¸€è½¬æ¢ä¸º yyyy-mm-dd æ ¼å¼
    æ”¯æŒçš„è¾“å…¥æ ¼å¼ï¼š
    - 2013å¹´2æœˆ4æ—¥ -> 2013-02-04
    - 2013-2-4 -> 2013-02-04
    - 2013.2.4 -> 2013-02-04
    - 2025-05-29 00:00:00 -> 2025-05-29
    """
    if not date_str or date_str.strip() == '':
        return ''
    
    import re
    from datetime import datetime
    
    date_str = str(date_str).strip()
    
    try:
        # æ ¼å¼1: 2013å¹´2æœˆ4æ—¥
        match = re.match(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', date_str)
        if match:
            year, month, day = match.groups()
            return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
        
        # æ ¼å¼2: 2013-2-4 æˆ– 2013/2/4 æˆ– 2013.2.4
        match = re.match(r'(\d{4})[-/.](\d{1,2})[-/.](\d{1,2})', date_str)
        if match:
            year, month, day = match.groups()
            return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
        
        # æ ¼å¼3: 2025-05-29 00:00:00 (å¸¦æ—¶é—´)
        match = re.match(r'(\d{4}-\d{2}-\d{2})\s+\d{2}:\d{2}:\d{2}', date_str)
        if match:
            return match.group(1)
        
        # æ ¼å¼4: å·²ç»æ˜¯ yyyy-mm-dd æ ¼å¼
        match = re.match(r'\d{4}-\d{2}-\d{2}$', date_str)
        if match:
            return date_str
        
        # æ ¼å¼5: å°è¯•ä½¿ç”¨datetimeè§£æ
        for fmt in ['%Y-%m-%d', '%Y/%m/%d', '%Y.%m.%d', '%Yå¹´%mæœˆ%dæ—¥']:
            try:
                dt = datetime.strptime(date_str, fmt)
                return dt.strftime('%Y-%m-%d')
            except:
                continue
        
        # å¦‚æœéƒ½æ— æ³•è§£æï¼Œè¿”å›åŸå§‹å­—ç¬¦ä¸²
        logger.warning(f"æ— æ³•è§£ææ—¥æœŸæ ¼å¼: {date_str}")
        return date_str
        
    except Exception as e:
        logger.warning(f"æ—¥æœŸæ ¼å¼åŒ–å¤±è´¥: {date_str}, é”™è¯¯: {e}")
        return date_str


class SearchBasedCrawler(BaseCrawler):
    """åŸºäºæœç´¢çš„æ³•è§„é‡‡é›†å™¨"""
    
    def __init__(self):
        super().__init__("search_api")
        self.logger = logger
        self.session = requests.Session()
        self.setup_headers()
        self.base_url = "https://flk.npc.gov.cn"
        
        # WAFçŠ¶æ€è¿½è¸ª
        self.waf_triggered = False
        self.consecutive_waf_count = 0
        self.last_successful_time = time.time()
        
        # åˆå§‹åŒ–è®¿é—®ï¼Œè·å–å¿…è¦çš„cookies
        self._initialize_session()
    
    def setup_headers(self):
        """è®¾ç½®è¯·æ±‚å¤´ - å¢å¼ºç‰ˆï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨"""
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/javascript, */*; q=0.01',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Referer': 'https://flk.npc.gov.cn/',
            'Origin': 'https://flk.npc.gov.cn',
            'X-Requested-With': 'XMLHttpRequest',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-origin',
            'Sec-Ch-Ua': '"Google Chrome";v="131", "Chromium";v="131", "Not_A Brand";v="24"',
            'Sec-Ch-Ua-Mobile': '?0',
            'Sec-Ch-Ua-Platform': '"Windows"',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache'
        })
        
        # è®¾ç½®Sessionçº§åˆ«çš„é…ç½®
        self.session.verify = True
        self.session.allow_redirects = True
        self.session.max_redirects = 5
    
    def _initialize_session(self):
        """åˆå§‹åŒ–sessionï¼Œè®¿é—®é¦–é¡µè·å–cookies"""
        try:
            # å…ˆè®¿é—®é¦–é¡µ
            home_response = self.session.get(
                "https://flk.npc.gov.cn/",
                timeout=10,
                allow_redirects=True
            )
            self.logger.debug(f"é¦–é¡µè®¿é—®çŠ¶æ€ç : {home_response.status_code}")
            
            # å†è®¿é—®æ³•è§„æœç´¢é¡µé¢
            search_page_response = self.session.get(
                "https://flk.npc.gov.cn/fl.html",
                timeout=10,
                allow_redirects=True
            )
            self.logger.debug(f"æœç´¢é¡µé¢è®¿é—®çŠ¶æ€ç : {search_page_response.status_code}")
            
            # ç­‰å¾…ä¸€ä¸‹ï¼Œæ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸º
            time.sleep(1)
            
        except Exception as e:
            self.logger.warning(f"åˆå§‹åŒ–sessionå¤±è´¥: {str(e)}")
            # ä¸ä¸­æ–­ï¼Œç»§ç»­å°è¯•
    
    async def search(self, law_name: str, law_number: Optional[str] = None) -> List[Dict[str, Any]]:
        """æœç´¢æ³•è§„ - å®ç°æŠ½è±¡æ–¹æ³•"""
        return self.search_law(law_name)
    
    async def get_detail(self, law_id: str) -> Dict[str, Any]:
        """è·å–æ³•è§„è¯¦æƒ… - å®ç°æŠ½è±¡æ–¹æ³•"""
        result = self.get_law_detail(law_id)
        return result or {}
    
    async def download_file(self, url: str, save_path: str) -> bool:
        """ä¸‹è½½æ–‡ä»¶ - å®ç°æŠ½è±¡æ–¹æ³•"""
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            with open(save_path, 'wb') as f:
                f.write(response.content)
            return True
        except Exception as e:
            self.logger.error(f"ä¸‹è½½æ–‡ä»¶å¤±è´¥: {str(e)}")
            return False
    
    def search_law_selenium(self, keyword: str) -> List[Dict[str, Any]]:
        """ä½¿ç”¨Seleniumæœç´¢æ³•è§„ - WAFç»•è¿‡æ–¹æ¡ˆ"""
        driver = None
        try:
            # é…ç½®Chromeé€‰é¡¹
            chrome_options = Options()
            chrome_options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36')
            
            # å¯åŠ¨æµè§ˆå™¨
            driver = webdriver.Chrome(options=chrome_options)
            wait = WebDriverWait(driver, 10)
            
            self.logger.debug(f"    ä½¿ç”¨Seleniumæœç´¢: {keyword}")
            
            # è®¿é—®æœç´¢é¡µé¢
            driver.get("https://flk.npc.gov.cn/fl.html")
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            wait.until(EC.presence_of_element_located((By.ID, "fgbt")))
            
            # è¾“å…¥æœç´¢å…³é”®è¯
            search_input = driver.find_element(By.ID, "fgbt")
            search_input.clear()
            search_input.send_keys(keyword)
            
            # ç‚¹å‡»æœç´¢æŒ‰é’®
            search_button = driver.find_element(By.CSS_SELECTOR, ".search_btn")
            search_button.click()
            
            # ç­‰å¾…æœç´¢ç»“æœ
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, ".search_result")))
            
            # è§£ææœç´¢ç»“æœ
            results = []
            result_items = driver.find_elements(By.CSS_SELECTOR, ".search_result .result_item")
            
            for item in result_items:
                try:
                    title_element = item.find_element(By.CSS_SELECTOR, ".title a")
                    title = title_element.text.strip()
                    link = title_element.get_attribute("href")
                    
                    # æå–æ³•è§„ID
                    law_id = ""
                    if "id=" in link:
                        law_id = link.split("id=")[1].split("&")[0]
                    
                    # è·å–å…¶ä»–ä¿¡æ¯
                    info_elements = item.find_elements(By.CSS_SELECTOR, ".info span")
                    publish_date = ""
                    status = 1  # é»˜è®¤æœ‰æ•ˆ
                    
                    for info_elem in info_elements:
                        text = info_elem.text.strip()
                        if "å‘å¸ƒæ—¥æœŸ" in text:
                            publish_date = text.replace("å‘å¸ƒæ—¥æœŸï¼š", "")
                        elif "å¤±æ•ˆ" in text:
                            status = 5
                    
                    result = {
                        'id': law_id,
                        'title': title,
                        'link': link,
                        'publish_date': publish_date,
                        'status': status
                    }
                    results.append(result)
                    
                except Exception as e:
                    self.logger.warning(f"è§£ææœç´¢ç»“æœé¡¹å¤±è´¥: {str(e)}")
                    continue
            
            self.logger.debug(f"    Seleniumæœç´¢æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
            return results
            
        except Exception as e:
            self.logger.error(f"Seleniumæœç´¢å¤±è´¥: {str(e)}")
            return []
        finally:
            if driver:
                driver.quit()
    
    def search_law(self, keyword: str) -> List[Dict[str, Any]]:
        """æœç´¢æ³•è§„ - æ™ºèƒ½ç‰ˆï¼šä¼˜å…ˆAPIï¼ŒWAFæ¿€æ´»æ—¶è‡ªåŠ¨åˆ‡æ¢Selenium"""
        
        # å¦‚æœWAFå·²æ¿€æ´»ï¼Œç›´æ¥ä½¿ç”¨Selenium
        if self.waf_triggered:
            self.logger.debug(f"    WAFå·²æ¿€æ´»ï¼Œç›´æ¥ä½¿ç”¨Seleniumæœç´¢")
            return self.search_law_selenium(keyword)
        
        # å°è¯•HTTP APIæœç´¢
        results = self._search_law_http(keyword)
        
        # æ£€æŸ¥ç»“æœå¹¶å¤„ç†WAFçŠ¶æ€
        if results:
            self._handle_waf_detection(False)  # æˆåŠŸï¼Œé‡ç½®WAFçŠ¶æ€
            return results
        else:
            # APIå¤±è´¥ï¼Œå¯èƒ½æ˜¯WAFæ‹¦æˆªï¼Œå°è¯•Seleniumå¤‡ç”¨
            self.logger.debug(f"    HTTP APIå¤±è´¥ï¼Œå°è¯•Seleniumå¤‡ç”¨æœç´¢")
            selenium_results = self.search_law_selenium(keyword)
            
            # å¦‚æœSeleniumæˆåŠŸè€ŒAPIå¤±è´¥ï¼Œè¯´æ˜å¯èƒ½æ˜¯WAFé—®é¢˜
            if selenium_results:
                self._handle_waf_detection(True)  # æ ‡è®°å¯èƒ½çš„WAFæ‹¦æˆª
            
            return selenium_results
    
    def _search_law_http(self, keyword: str) -> List[Dict[str, Any]]:
        """HTTP APIæœç´¢æ³•è§„ - å¢å¼ºWAFæ£€æµ‹"""
        search_strategies = [
            ("title;vague", keyword),  # æ ‡é¢˜æ¨¡ç³Šæœç´¢
            ("title;accurate;1,3", keyword),  # æ ‡é¢˜ç²¾ç¡®æœç´¢ï¼ˆæ³•å¾‹+è¡Œæ”¿æ³•è§„ï¼‰
        ]
        
        for search_type, search_keyword in search_strategies:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦WAFå†·å´ç­‰å¾…
            self._should_wait_for_waf()
            
            params = [
                ("type", search_type),
                ("searchType", "title;vague" if "vague" in search_type else "title;accurate"),
                ("sortTr", "f_bbrq_s;desc"),
                ("gbrqStart", ""),
                ("gbrqEnd", ""),
                ("sxrqStart", ""),
                ("sxrqEnd", ""),
                ("sort", "true"),
                ("page", "1"),
                ("size", "20"),
                ("fgbt", keyword),  # æœç´¢å…³é”®è¯
                ("_", int(time.time() * 1000)),
            ]
            
            try:
                self.logger.debug(f"    å°è¯•APIæœç´¢ç­–ç•¥: {search_type}")
                
                # æ·»åŠ éšæœºå»¶è¿Ÿï¼Œé¿å…è§¦å‘WAF
                if self.consecutive_waf_count > 0:
                    delay = random.uniform(2, 5)
                    self.logger.debug(f"    æ·»åŠ éšæœºå»¶è¿Ÿ: {delay:.1f}ç§’")
                    time.sleep(delay)
                
                response = self.session.get(
                    "https://flk.npc.gov.cn/api/",
                    params=params,
                    timeout=15
                )
                
                # è¯¦ç»†çš„å“åº”æ£€æŸ¥
                self.logger.debug(f"    APIå“åº”çŠ¶æ€ç : {response.status_code}")
                content_type = response.headers.get('Content-Type', '')
                self.logger.debug(f"    APIå“åº”Content-Type: {content_type}")
                
                if response.status_code == 200:
                    # æ£€æŸ¥æ˜¯å¦è¢«WAFæ‹¦æˆª
                    waf_detected = self._check_waf_response(response)
                    
                    if waf_detected:
                        self.logger.debug(f"    æ£€æµ‹åˆ°WAFæ‹¦æˆª (ç­–ç•¥: {search_type})")
                        self._handle_waf_detection(True)
                        continue  # å°è¯•ä¸‹ä¸€ä¸ªç­–ç•¥
                    
                    # æ­£å¸¸JSONå“åº”
                    if not response.text.strip():
                        self.logger.debug(f"    APIè¿”å›ç©ºå†…å®¹ (ç­–ç•¥: {search_type})")
                        continue
                        
                    try:
                        result = response.json()
                        if result.get('success') and result.get('result', {}).get('data'):
                            results = result['result']['data']
                            self.logger.debug(f"    APIæœç´¢æˆåŠŸ: {len(results)} ä¸ªç»“æœ (ç­–ç•¥: {search_type})")
                            self._handle_waf_detection(False)  # æˆåŠŸï¼Œé‡ç½®WAFçŠ¶æ€
                            return results
                        else:
                            self.logger.debug(f"    APIæœç´¢æ— ç»“æœ (ç­–ç•¥: {search_type})")
                    except json.JSONDecodeError as e:
                        self.logger.debug(f"    JSONè§£æå¤±è´¥ (ç­–ç•¥: {search_type}): {str(e)}")
                        # å¯èƒ½æ˜¯WAFè¿”å›çš„HTML
                        self._handle_waf_detection(True)
                        continue
                else:
                    self.logger.debug(f"    API HTTPé”™è¯¯ (ç­–ç•¥: {search_type}): {response.status_code}")
                    
            except Exception as e:
                self.logger.debug(f"    APIè¯·æ±‚å¼‚å¸¸ (ç­–ç•¥: {search_type}): {str(e)}")
                continue
        
        self.logger.debug(f"    æ‰€æœ‰APIæœç´¢ç­–ç•¥éƒ½å¤±è´¥")
        return []
    
    def _check_waf_response(self, response) -> bool:
        """æ£€æŸ¥å“åº”æ˜¯å¦è¢«WAFæ‹¦æˆª"""
        # æ£€æŸ¥Content-Type
        content_type = response.headers.get('Content-Type', '').lower()
        if 'text/html' in content_type:
            return True
        
        # æ£€æŸ¥WAFæ ‡è¯†
        if 'WZWS-RAY' in response.headers:
            return True
        
        # æ£€æŸ¥å“åº”å†…å®¹
        if "<!DOCTYPE HTML>" in response.text and "JavaScript" in response.text:
            return True
        
        return False
    
    def _handle_waf_detection(self, waf_detected: bool):
        """å¤„ç†WAFæ£€æµ‹ç»“æœ"""
        if waf_detected:
            self.consecutive_waf_count += 1
            if self.consecutive_waf_count >= 2:  # è¿ç»­2æ¬¡è¢«æ‹¦æˆªæ‰è®¤ä¸ºWAFæ¿€æ´»
                self.waf_triggered = True
                self.logger.warning(f"ğŸš« WAFå·²æ¿€æ´»ï¼Œè¿ç»­æ‹¦æˆª{self.consecutive_waf_count}æ¬¡ï¼Œåˆ‡æ¢åˆ°Seleniumæ¨¡å¼")
        else:
            # æˆåŠŸè¯·æ±‚ï¼Œé‡ç½®è®¡æ•°å™¨
            self.consecutive_waf_count = 0
            self.last_successful_time = time.time()
            if self.waf_triggered:
                self.logger.info("âœ… APIæ¢å¤æ­£å¸¸ï¼ŒWAFå¯èƒ½å·²è§£é™¤")
                self.waf_triggered = False
    
    def _should_wait_for_waf(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦ç­‰å¾…WAFå†·å´"""
        if not self.waf_triggered:
            return False
        
        # å¦‚æœWAFè¢«è§¦å‘ï¼Œç­‰å¾…ä¸€å®šæ—¶é—´å†å°è¯•
        time_since_last_success = time.time() - self.last_successful_time
        if time_since_last_success < 60:  # 60ç§’å†·å´æ—¶é—´
            wait_time = 60 - time_since_last_success
            self.logger.info(f"â±ï¸ WAFå†·å´ä¸­ï¼Œç­‰å¾… {wait_time:.1f} ç§’...")
            time.sleep(wait_time)
        
        return True
    
    def get_law_detail(self, law_id: str) -> Optional[Dict[str, Any]]:
        """è·å–æ³•è§„è¯¦æƒ…"""
        try:
            response = self.session.post(
                "https://flk.npc.gov.cn/api/detail",
                data={"id": law_id},
                timeout=10
            )
            
            if response.status_code == 200:
                result = response.json()
                if result.get('success') and result.get('result'):
                    return result['result']
            
            return None
            
        except Exception as e:
            self.logger.error(f"è·å–æ³•è§„è¯¦æƒ…å¤±è´¥: {str(e)}")
            return None
    
    def normalize_law_name(self, law_name: str) -> str:
        """æ ‡å‡†åŒ–æ³•è§„åç§°"""
        # ç§»é™¤æ‹¬å·å†…å®¹
        normalized = re.sub(r'[ï¼ˆ(].*?[ï¼‰)]', '', law_name)
        # ç§»é™¤"ä¿®è®¢"ã€"ä¿®æ­£"ç­‰åç¼€
        normalized = re.sub(r'ï¼ˆ\d{4}.*?ä¿®è®¢.*?ï¼‰', '', normalized)
        normalized = re.sub(r'ï¼ˆ\d{4}.*?ä¿®æ­£.*?ï¼‰', '', normalized)
        # ç§»é™¤ä¸»å¸­ä»¤ç­‰ç¼–å·
        normalized = re.sub(r'ï¼ˆ.*?ä¸»å¸­ä»¤.*?ï¼‰', '', normalized)
        normalized = re.sub(r'ï¼ˆ.*?ä»¤.*?ï¼‰', '', normalized)
        # æ¸…ç†å¤šä½™ç©ºæ ¼
        normalized = re.sub(r'\s+', '', normalized)
        
        return normalized.strip()
    
    def calculate_match_score(self, target: str, result: str) -> float:
        """è®¡ç®—åŒ¹é…åˆ†æ•° - æ”¹è¿›ç‰ˆ"""
        if not target or not result:
            return 0.0
            
        # å®Œå…¨åŒ¹é…
        if target == result:
            return 1.0
        
        # æ ‡å‡†åŒ–å¤„ç†
        target_clean = re.sub(r'[ï¼ˆ(].*?[ï¼‰)]', '', target).strip()
        result_clean = re.sub(r'[ï¼ˆ(].*?[ï¼‰)]', '', result).strip()
        
        # å»æ‰ä¿®è®¢å¹´ä»½åçš„åŒ¹é…
        if target_clean == result_clean:
            return 0.95
        
        # åŒ…å«å…³ç³» - ä½†è¦é¿å…åŒ¹é…åˆ°å¸æ³•è§£é‡Š
        if "è§£é‡Š" in result and "è§£é‡Š" not in target:
            return 0.1  # å¤§å¹…é™ä½å¸æ³•è§£é‡Šçš„åŒ¹é…åˆ†æ•°
        
        if "æ„è§" in result and "æ„è§" not in target:
            return 0.1  # å¤§å¹…é™ä½æ„è§ç±»æ–‡ä»¶çš„åŒ¹é…åˆ†æ•°
            
        # æ£€æŸ¥æ ¸å¿ƒå…³é”®è¯åŒ¹é…
        target_core = target_clean.replace("ä¸­åäººæ°‘å…±å’Œå›½", "")
        result_core = result_clean.replace("ä¸­åäººæ°‘å…±å’Œå›½", "")
        
        if target_core and result_core:
            # å®Œå…¨åŒ¹é…æ ¸å¿ƒéƒ¨åˆ†
            if target_core == result_core:
                return 0.9
            
            # åŒ…å«å…³ç³»
            if target_core in result_core:
                ratio = len(target_core) / len(result_core)
                return 0.7 + ratio * 0.2
            if result_core in target_core:
                ratio = len(result_core) / len(target_core)
                return 0.7 + ratio * 0.2
        
        # è®¡ç®—å…¬å…±å­ä¸²
        common_length = 0
        min_len = min(len(target), len(result))
        for i in range(min_len):
            if target[i] == result[i]:
                common_length += 1
            else:
                break
        
        if common_length > 0:
            base_score = common_length / max(len(target), len(result))
            # å¦‚æœå…¬å…±å‰ç¼€å¾ˆé•¿ï¼Œç»™æ›´é«˜åˆ†æ•°
            if common_length >= 6:
                return min(0.8, base_score * 1.2)
            return base_score
        
        return 0.0
    
    def find_best_match(self, target_name: str, search_results: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """åœ¨æœç´¢ç»“æœä¸­æ‰¾åˆ°æœ€ä½³åŒ¹é… - ä¼˜åŒ–ç‰ˆï¼Œä¼˜å…ˆé€‰æ‹©æœ‰æ•ˆæ³•è§„"""
        if not search_results:
            return None
        
        # æ ‡å‡†åŒ–ç›®æ ‡åç§°
        target_normalized = self.normalize_law_name(target_name)
        
        # è®°å½•æ‰€æœ‰å€™é€‰é¡¹çš„åˆ†æ•°å’ŒçŠ¶æ€
        candidates = []
        
        for law in search_results:
            law_title = law.get('title', '')
            law_normalized = self.normalize_law_name(law_title)
            
            # è®¡ç®—åŒ¹é…åˆ†æ•°
            score = self.calculate_match_score(target_normalized, law_normalized)
            
            # è·å–æ³•è§„çŠ¶æ€ (1=æœ‰æ•ˆ, 5=å¤±æ•ˆ) - å¤„ç†å­—ç¬¦ä¸²ç±»å‹
            status = law.get('status', 0)
            # ç¡®ä¿çŠ¶æ€æ˜¯æ•´æ•°ç±»å‹
            try:
                status_int = int(status)
            except (ValueError, TypeError):
                status_int = 0
            
            is_valid = (status_int == 1)
            
            candidates.append({
                'title': law_title,
                'score': score,
                'law': law,
                'status': status_int,
                'is_valid': is_valid,
                'status_text': 'æœ‰æ•ˆ' if is_valid else 'å¤±æ•ˆ'
            })
        
        # æŒ‰åˆ†æ•°æ’åº
        candidates.sort(key=lambda x: x['score'], reverse=True)
        
        # è°ƒè¯•ä¿¡æ¯
        self.logger.debug(f"    åŒ¹é…å€™é€‰é¡¹:")
        for candidate in candidates[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
            self.logger.debug(f"      {candidate['title']}: {candidate['score']:.3f} ({candidate['status_text']})")
        
        # è®¾ç½®åŒ¹é…é˜ˆå€¼
        threshold = 0.75
        
        # ç­›é€‰å‡ºè¾¾åˆ°é˜ˆå€¼çš„å€™é€‰é¡¹
        qualified_candidates = [c for c in candidates if c['score'] >= threshold]
        
        if not qualified_candidates:
            self.logger.debug(f"    æ²¡æœ‰å€™é€‰é¡¹è¾¾åˆ°é˜ˆå€¼ {threshold}")
            return None
        
        # ä¼˜å…ˆçº§é€‰æ‹©é€»è¾‘
        # 1. ä¼˜å…ˆé€‰æ‹©æœ‰æ•ˆçš„æ³•è§„
        valid_candidates = [c for c in qualified_candidates if c['is_valid']]
        
        if valid_candidates:
            # æœ‰æœ‰æ•ˆæ³•è§„ï¼Œé€‰æ‹©åˆ†æ•°æœ€é«˜çš„æœ‰æ•ˆæ³•è§„
            best_candidate = valid_candidates[0]  # å·²æŒ‰åˆ†æ•°æ’åº
            self.logger.debug(f"    ä¼˜å…ˆé€‰æ‹©æœ‰æ•ˆæ³•è§„: {best_candidate['title']} (åˆ†æ•°: {best_candidate['score']:.3f}, çŠ¶æ€: {best_candidate['status_text']})")
            return best_candidate['law']
        else:
            # æ²¡æœ‰æœ‰æ•ˆæ³•è§„ï¼Œé€‰æ‹©åˆ†æ•°æœ€é«˜çš„å¤±æ•ˆæ³•è§„
            best_candidate = qualified_candidates[0]  # å·²æŒ‰åˆ†æ•°æ’åº
            self.logger.debug(f"    æ— æœ‰æ•ˆæ³•è§„ï¼Œé€‰æ‹©å¤±æ•ˆæ³•è§„: {best_candidate['title']} (åˆ†æ•°: {best_candidate['score']:.3f}, çŠ¶æ€: {best_candidate['status_text']})")
            return best_candidate['law']
    
    def generate_search_keywords(self, law_name: str) -> List[str]:
        """ç”Ÿæˆæœç´¢å…³é”®è¯"""
        keywords = []
        
        # 1. å®Œæ•´åç§°
        keywords.append(law_name)
        
        # 2. ç§»é™¤"ä¸­åäººæ°‘å…±å’Œå›½"å‰ç¼€
        if law_name.startswith("ä¸­åäººæ°‘å…±å’Œå›½"):
            keywords.append(law_name.replace("ä¸­åäººæ°‘å…±å’Œå›½", ""))
        
        # 3. æå–ä¸»å¹²åç§°ï¼ˆç§»é™¤æ‹¬å·å†…å®¹ï¼‰
        main_name = re.sub(r'[ï¼ˆ(].*?[ï¼‰)]', '', law_name)
        if main_name != law_name and main_name.strip():
            keywords.append(main_name.strip())
        
        # 4. æå–æ ¸å¿ƒè¯æ±‡
        if "æ³•" in law_name:
            # æå–"æ³•"å‰é¢çš„éƒ¨åˆ†
            parts = law_name.split("æ³•")
            if parts[0]:
                core_name = parts[0] + "æ³•"
                if core_name not in keywords:
                    keywords.append(core_name)
        
        # 5. å¯¹äºåŠæ³•ã€æ¡ä¾‹ç­‰ï¼Œå°è¯•ä¸åŒçš„æœç´¢ç­–ç•¥
        if any(word in law_name for word in ["åŠæ³•", "æ¡ä¾‹", "è§„å®š", "ç»†åˆ™"]):
            # æå–å…³é”®è¯ç»„åˆ
            for suffix in ["åŠæ³•", "æ¡ä¾‹", "è§„å®š", "ç»†åˆ™"]:
                if suffix in law_name:
                    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå…³é”®è¯
                    parts = law_name.split(suffix)
                    if parts[0]:
                        # å°è¯•ä¸åŒé•¿åº¦çš„å…³é”®è¯
                        base = parts[0].strip()
                        # ç§»é™¤ä¿®è®¢å¹´ä»½
                        base = re.sub(r'[ï¼ˆ(].*?[ï¼‰)]', '', base).strip()
                        if base and len(base) >= 4:  # è‡³å°‘4ä¸ªå­—ç¬¦
                            keywords.append(base + suffix)
                            # å°è¯•æ›´çŸ­çš„å…³é”®è¯
                            if len(base) > 6:
                                keywords.append(base[-6:] + suffix)
        
        # å»é‡å¹¶è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
        keywords = list(dict.fromkeys([k for k in keywords if k.strip()]))  # ä¿æŒé¡ºåºçš„å»é‡
        
        return keywords
    
    def extract_document_number(self, detail: dict) -> str:
        """æå–æ–‡å·"""
        try:
            # æ–¹æ³•1: ä»otherFileåˆ—è¡¨ä¸­çš„ä¸»å¸­ä»¤æ–‡ä»¶åæå–æ–‡å·
            other_files = detail.get('otherFile', [])
            if isinstance(other_files, list):
                for file_info in other_files:
                    if isinstance(file_info, dict):
                        file_name = file_info.get('name', '')
                        if 'ä¸»å¸­ä»¤' in file_name:
                            # æå–ä¸»å¸­ä»¤å·ç 
                            import re
                            # åŒ¹é…å„ç§ä¸»å¸­ä»¤æ ¼å¼
                            patterns = [
                                r'ä¸»å¸­ä»¤ï¼ˆç¬¬(\w+)å·ï¼‰',  # ä¸»å¸­ä»¤ï¼ˆç¬¬ä¸‰åä¸€å·ï¼‰
                                r'ä¸»å¸­ä»¤ç¬¬(\w+)å·',      # ä¸»å¸­ä»¤ç¬¬ä¸€äºŒã€‡å·
                                r'ä¸»å¸­ä»¤.*?(\d+)å·',     # åŒ…å«æ•°å­—çš„ä¸»å¸­ä»¤
                            ]
                            for pattern in patterns:
                                match = re.search(pattern, file_name)
                                if match:
                                    return f"ä¸»å¸­ä»¤ç¬¬{match.group(1)}å·"
            
            # æ–¹æ³•2: ä»æ ‡é¢˜ä¸­æå–ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
            title = detail.get('title', '')
            if title:
                # ç®€å•çš„æ–‡å·æå–é€»è¾‘ï¼Œå¯ä»¥æ ¹æ®éœ€è¦æ‰©å±•
                return ""
            
            return ""
        except Exception as e:
            self.logger.error(f"æå–æ–‡å·æ—¶å‡ºé”™: {e}")
            return ""
    
    def crawl_law_by_search(self, law_name: str) -> Optional[Dict[str, Any]]:
        """é€šè¿‡æœç´¢é‡‡é›†å•ä¸ªæ³•è§„"""
        self.logger.info(f"æœç´¢æ³•è§„: {law_name}")
        
        # ç”Ÿæˆæœç´¢å…³é”®è¯
        keywords = self.generate_search_keywords(law_name)
        self.logger.debug(f"æœç´¢å…³é”®è¯: {keywords}")
        
        for keyword in keywords:
            self.logger.debug(f"  å°è¯•å…³é”®è¯: {keyword}")
            
            search_results = self.search_law(keyword)
            
            if search_results:
                self.logger.debug(f"    æ‰¾åˆ° {len(search_results)} ä¸ªç»“æœ")
                
                # æ‰¾åˆ°æœ€ä½³åŒ¹é…
                best_match = self.find_best_match(law_name, search_results)
                
                if best_match:
                    self.logger.info(f"    âœ… æ‰¾åˆ°åŒ¹é…: {best_match.get('title', '')}")
                    self.logger.debug(f"    ID: {best_match.get('id', '')}")
                    
                    # è·å–è¯¦ç»†ä¿¡æ¯
                    detail = self.get_law_detail(best_match['id'])
                    if not detail:
                        self.logger.error(f"  âŒ æ— æ³•è·å–è¯¦ç»†ä¿¡æ¯")
                        return None
                    
                    # æå–å’Œæ•´ç†æ–‡ä»¶ä¿¡æ¯
                    body_files = detail.get('body', [])
                    other_files = detail.get('otherFile', [])
                    
                    # æ•´ç†æ­£æ–‡æ–‡ä»¶ä¿¡æ¯
                    formatted_body_files = []
                    if isinstance(body_files, list):
                        for file_info in body_files:
                            if isinstance(file_info, dict):
                                formatted_body_files.append({
                                    'type': file_info.get('type', ''),
                                    'path': file_info.get('path', ''),
                                    'url': file_info.get('url', ''),
                                    'mobile_url': file_info.get('mobile', ''),
                                    'addr': file_info.get('addr', '')
                                })
                    
                    # æ•´ç†å…¶ä»–æ–‡ä»¶ä¿¡æ¯
                    formatted_other_files = []
                    if isinstance(other_files, list):
                        for file_info in other_files:
                            if isinstance(file_info, dict):
                                formatted_other_files.append({
                                    'name': file_info.get('name', ''),
                                    'type': file_info.get('type', ''),
                                    'hdfs_path': file_info.get('hdfsPath', ''),
                                    'oss_path': file_info.get('ossPath', ''),
                                    'order': file_info.get('order', '')
                                })
                    
                    result_data = {
                        # åŸºæœ¬ä¿¡æ¯
                        'law_id': best_match.get('id', ''),
                        'target_name': law_name,
                        'search_keyword': keyword,
                        'title': detail.get('title', ''),
                        'document_number': self.extract_document_number(detail),
                        
                        # æ—¥æœŸä¿¡æ¯
                        'publish_date': normalize_date_format(detail.get('publish', '')),
                        'implement_date': normalize_date_format(detail.get('expiry', '')),
                        'invalidate_date': '',  # APIä¸­æ²¡æœ‰å¤±æ•ˆæ—¥æœŸå­—æ®µ
                        
                        # æœºå…³å’Œåˆ†ç±»ä¿¡æ¯
                        'office': detail.get('office', ''),
                        'level': detail.get('level', ''),  # æ³•è§„çº§åˆ«ï¼šæ³•å¾‹ã€è¡Œæ”¿æ³•è§„ã€å¸æ³•è§£é‡Šç­‰
                        'status': 'æœ‰æ•ˆ' if detail.get('status', '') == '1' else 'å¤±æ•ˆ',
                        
                        # æ–‡ä»¶ä¿¡æ¯
                        'body_files': formatted_body_files,  # æ­£æ–‡æ–‡ä»¶ï¼ˆWORD/PDF/HTMLï¼‰
                        'other_files': formatted_other_files,  # å…¶ä»–æ–‡ä»¶ï¼ˆä¸»å¸­ä»¤ç­‰ï¼‰
                        
                        # åŸå§‹æ•°æ®
                        'raw_api_response': detail,  # ä¿å­˜å®Œæ•´çš„APIå“åº”
                        
                        # å…ƒæ•°æ®
                        'source_url': f"https://flk.npc.gov.cn/detail2.html?id={best_match.get('id', '')}",
                        'crawl_date': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        'crawler_version': '1.0.0'
                    }
                    
                    self.logger.info(f"    ğŸ”¢ æ–‡å·: {result_data['document_number']}")
                    self.logger.info(f"    ğŸ“… å‘å¸ƒæ—¥æœŸ: {result_data['publish_date']}")
                    self.logger.info(f"    ğŸ“… å®æ–½æ—¥æœŸ: {result_data['implement_date']}")
                    self.logger.info(f"    ğŸš¦ çŠ¶æ€: {result_data['status']} (statusåŸå€¼: {detail.get('status', '')})")
                    self.logger.info(f"    ğŸ›ï¸ å‘å¸ƒæœºå…³: {result_data['office']}")
                    self.logger.info(f"    ğŸ“‹ æ³•è§„çº§åˆ«: {result_data['level']}")
                    
                    return result_data
                else:
                    self.logger.warning(f"    âŒ æœªæ‰¾åˆ°åŒ¹é…")
            else:
                self.logger.warning(f"    âŒ æœç´¢æ— ç»“æœ")
            
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        self.logger.error(f"  âŒ æ‰€æœ‰å…³é”®è¯éƒ½æœªæ‰¾åˆ°åŒ¹é…")
        return None
    
    async def crawl_law(self, law_name: str, law_number: str = None) -> Optional[Dict]:
        """çˆ¬å–å•ä¸ªæ³•å¾‹ï¼ˆå®ç°CrawlerManageréœ€è¦çš„æ¥å£ï¼‰"""
        try:
            self.logger.info(f"äººå¤§ç½‘çˆ¬å–: {law_name}")
            
            result = self.crawl_law_by_search(law_name)
            
            if result:
                # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                return {
                    'law_id': result.get('law_id', ''),
                    'name': result.get('title', ''),
                    'number': result.get('document_number', ''),
                    'source': 'search_api',
                    'success': True,
                    'source_url': result.get('source_url', ''),
                    'publish_date': result.get('publish_date', ''),
                    'valid_from': result.get('implement_date', ''),
                    'valid_to': result.get('invalidate_date', ''),
                    'office': result.get('office', ''),
                    'status': result.get('status', ''),
                    'level': result.get('level', ''),
                    'crawl_time': result.get('crawl_date', ''),
                    'raw_data': result
                }
            else:
                return None
                
        except Exception as e:
            self.logger.error(f"äººå¤§ç½‘çˆ¬å–å¤±è´¥: {law_name}, é”™è¯¯: {e}")
            return None
    
    def crawl_laws(self, target_laws: List[str]) -> List[Dict[str, Any]]:
        """æ‰¹é‡é‡‡é›†æ³•è§„"""
        self.logger.info("=== åŸºäºæœç´¢çš„æ³•è§„é‡‡é›†å™¨ ===")
        self.logger.info(f"ç›®æ ‡æ³•è§„æ•°: {len(target_laws)}")
        
        results = []
        success_count = 0
        
        for i, law_name in enumerate(target_laws, 1):
            self.logger.info(f"è¿›åº¦: {i}/{len(target_laws)}")
            
            result = self.crawl_law_by_search(law_name)
            
            if result:
                results.append(result)
                success_count += 1
                self.logger.info(f"âœ… æˆåŠŸé‡‡é›†: {result['title']}")
            else:
                self.logger.error(f"âŒ é‡‡é›†å¤±è´¥: {law_name}")
            
            self.logger.info("-" * 50)
        
        self.logger.info(f"\n=== é‡‡é›†å®Œæˆ ===")
        self.logger.info(f"æˆåŠŸ: {success_count}/{len(target_laws)}")
        self.logger.info(f"æˆåŠŸç‡: {success_count/len(target_laws)*100:.1f}%")
        
        return results 