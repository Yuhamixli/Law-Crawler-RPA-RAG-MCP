#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
åŸºäºSeleniumçš„æ”¿åºœç½‘çˆ¬è™«
ä½¿ç”¨çœŸå®æµè§ˆå™¨ç¯å¢ƒç»•è¿‡åçˆ¬æœºåˆ¶
"""

import time
import json
from datetime import datetime
from typing import Dict, List, Optional, Any
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from loguru import logger
from bs4 import BeautifulSoup
import re

from ..base_crawler import BaseCrawler


def normalize_date_format(date_str: str) -> str:
    """
    å°†å„ç§æ—¥æœŸæ ¼å¼ç»Ÿä¸€è½¬æ¢ä¸º yyyy-mm-dd æ ¼å¼
    æ”¯æŒçš„è¾“å…¥æ ¼å¼ï¼š
    - 2013å¹´2æœˆ4æ—¥ -> 2013-02-04
    - 2013-2-4 -> 2013-02-04
    - 2013.2.4 -> 2013-02-04
    - 2025-05-29 00:00:00 -> 2025-05-29
    """
    if not date_str or date_str.strip() == '':
        return ''
    
    import re
    from datetime import datetime
    
    date_str = str(date_str).strip()
    
    try:
        # æ ¼å¼1: 2013å¹´2æœˆ4æ—¥
        match = re.match(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', date_str)
        if match:
            year, month, day = match.groups()
            return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
        
        # æ ¼å¼2: 2013-2-4 æˆ– 2013/2/4 æˆ– 2013.2.4
        match = re.match(r'(\d{4})[-/.](\d{1,2})[-/.](\d{1,2})', date_str)
        if match:
            year, month, day = match.groups()
            return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
        
        # æ ¼å¼3: 2025-05-29 00:00:00 (å¸¦æ—¶é—´)
        match = re.match(r'(\d{4}-\d{2}-\d{2})\s+\d{2}:\d{2}:\d{2}', date_str)
        if match:
            return match.group(1)
        
        # æ ¼å¼4: å·²ç»æ˜¯ yyyy-mm-dd æ ¼å¼
        match = re.match(r'\d{4}-\d{2}-\d{2}$', date_str)
        if match:
            return date_str
        
        # æ ¼å¼5: å°è¯•ä½¿ç”¨datetimeè§£æ
        for fmt in ['%Y-%m-%d', '%Y/%m/%d', '%Y.%m.%d', '%Yå¹´%mæœˆ%dæ—¥']:
            try:
                dt = datetime.strptime(date_str, fmt)
                return dt.strftime('%Y-%m-%d')
            except:
                continue
        
        # å¦‚æœéƒ½æ— æ³•è§£æï¼Œè¿”å›åŸå§‹å­—ç¬¦ä¸²
        logger.warning(f"æ— æ³•è§£ææ—¥æœŸæ ¼å¼: {date_str}")
        return date_str
        
    except Exception as e:
        logger.warning(f"æ—¥æœŸæ ¼å¼åŒ–å¤±è´¥: {date_str}, é”™è¯¯: {e}")
        return date_str


class SeleniumGovCrawler(BaseCrawler):
    """åŸºäºSeleniumçš„ä¸­å›½æ”¿åºœç½‘çˆ¬è™«"""
    
    def __init__(self):
        super().__init__("selenium_gov")
        self.driver = None
        self.logger = logger
        self.setup_driver()
    
    def setup_driver(self):
        """è®¾ç½®Chrome WebDriver - ä¼˜åŒ–ç‰ˆæœ¬"""
        try:
            chrome_options = Options()
            
            # æ€§èƒ½ä¼˜åŒ–è®¾ç½®
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--disable-web-security')
            chrome_options.add_argument('--allow-running-insecure-content')
            chrome_options.add_argument('--ignore-certificate-errors')
            chrome_options.add_argument('--ignore-ssl-errors')
            chrome_options.add_argument('--ignore-certificate-errors-spki-list')
            
            # å¯ç”¨æ— å¤´æ¨¡å¼ä»¥æé«˜æ•ˆç‡
            chrome_options.add_argument('--headless')  # å¯ç”¨æ— å¤´æ¨¡å¼
            
            # ç¦ç”¨ä¸å¿…è¦çš„åŠŸèƒ½ä»¥æé«˜é€Ÿåº¦
            chrome_options.add_argument('--disable-extensions')
            chrome_options.add_argument('--disable-plugins')
            chrome_options.add_argument('--disable-javascript')  # ç¦ç”¨JSåŠ å¿«åŠ è½½
            chrome_options.add_argument('--disable-background-timer-throttling')
            chrome_options.add_argument('--disable-backgrounding-occluded-windows')
            chrome_options.add_argument('--disable-renderer-backgrounding')
            chrome_options.add_argument('--disable-features=TranslateUI')
            chrome_options.add_argument('--disable-ipc-flooding-protection')
            
            # å†…å­˜å’ŒCPUä¼˜åŒ–
            chrome_options.add_argument('--memory-pressure-off')
            chrome_options.add_argument('--max_old_space_size=4096')
            chrome_options.add_argument('--aggressive-cache-discard')
            
            # ç”¨æˆ·ä»£ç†è®¾ç½®
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            
            # çª—å£å¤§å°
            chrome_options.add_argument('--window-size=1366,768')  # å‡å°çª—å£å¤§å°
            
            # ç¦ç”¨å›¾ç‰‡ã€CSSã€å­—ä½“åŠ è½½ä»¥æé«˜é€Ÿåº¦
            prefs = {
                "profile.managed_default_content_settings.images": 2,
                "profile.default_content_setting_values.notifications": 2,
                "profile.managed_default_content_settings.stylesheets": 2,
                "profile.managed_default_content_settings.cookies": 2,
                "profile.managed_default_content_settings.javascript": 1,  # å¯ç”¨JSï¼ŒæŸäº›é¡µé¢éœ€è¦
                "profile.managed_default_content_settings.plugins": 2,
                "profile.managed_default_content_settings.popups": 2,
                "profile.managed_default_content_settings.geolocation": 2,
                "profile.managed_default_content_settings.media_stream": 2,
            }
            chrome_options.add_experimental_option("prefs", prefs)
            
            # å°è¯•è‡ªåŠ¨ä¸‹è½½ChromeDriverï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨ç³»ç»Ÿè·¯å¾„
            try:
                service = Service(ChromeDriverManager().install())
                self.logger.info("ä½¿ç”¨è‡ªåŠ¨ä¸‹è½½çš„ChromeDriver")
            except Exception as download_error:
                self.logger.warning(f"ChromeDriverè‡ªåŠ¨ä¸‹è½½å¤±è´¥: {download_error}")
                # å°è¯•ä½¿ç”¨ç³»ç»ŸPATHä¸­çš„chromedriver
                service = Service()  # ä½¿ç”¨é»˜è®¤è·¯å¾„
                self.logger.info("å°è¯•ä½¿ç”¨ç³»ç»ŸPATHä¸­çš„ChromeDriver")
            
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            self.driver.implicitly_wait(5)  # å‡å°‘éšå¼ç­‰å¾…æ—¶é—´
            
            # è®¾ç½®é¡µé¢åŠ è½½è¶…æ—¶
            self.driver.set_page_load_timeout(20)  # 20ç§’é¡µé¢åŠ è½½è¶…æ—¶
            self.driver.set_script_timeout(10)     # 10ç§’è„šæœ¬æ‰§è¡Œè¶…æ—¶
            
            self.logger.info("Chrome WebDriveråˆå§‹åŒ–æˆåŠŸï¼ˆä¼˜åŒ–æ¨¡å¼ï¼‰")
            
        except Exception as e:
            self.logger.error(f"WebDriveråˆå§‹åŒ–å¤±è´¥: {e}")
            self.logger.info("å»ºè®®ï¼š")
            self.logger.info("1. ç¡®ä¿å·²å®‰è£…Chromeæµè§ˆå™¨")
            self.logger.info("2. æ£€æŸ¥ç½‘ç»œè¿æ¥")
            self.logger.info("3. æˆ–æ‰‹åŠ¨ä¸‹è½½ChromeDriverå¹¶æ·»åŠ åˆ°PATH")
            self.driver = None
    
    def ensure_driver(self):
        """ç¡®ä¿é©±åŠ¨å¯ç”¨ï¼Œå¦‚æœä¸å¯ç”¨åˆ™é‡æ–°åˆå§‹åŒ–"""
        if not self.driver:
            self.setup_driver()
        else:
            try:
                # æµ‹è¯•é©±åŠ¨æ˜¯å¦è¿˜æ´»ç€
                self.driver.current_url
            except:
                self.logger.warning("WebDriverå·²å¤±æ•ˆï¼Œé‡æ–°åˆå§‹åŒ–")
                self.close_driver()
                self.setup_driver()
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œç¡®ä¿æµè§ˆå™¨å…³é—­"""
        self.close_driver()
    
    def close_driver(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.driver:
            try:
                self.driver.quit()
                self.logger.info("æµè§ˆå™¨å·²å…³é—­")
            except:
                pass
            finally:
                self.driver = None
    
    async def search(self, law_name: str, law_number: Optional[str] = None) -> List[Dict[str, Any]]:
        """æœç´¢æ³•è§„ - å®ç°æŠ½è±¡æ–¹æ³•"""
        return self.search_law_with_browser(law_name)
    
    async def get_detail(self, law_id: str) -> Dict[str, Any]:
        """è·å–æ³•è§„è¯¦æƒ… - å®ç°æŠ½è±¡æ–¹æ³•"""
        # å¯¹äºæ”¿åºœç½‘ï¼Œlaw_idå®é™…ä¸Šæ˜¯URL
        return self.get_law_detail_from_url(law_id)
    
    async def download_file(self, url: str, save_path: str) -> bool:
        """ä¸‹è½½æ–‡ä»¶ - å®ç°æŠ½è±¡æ–¹æ³•"""
        try:
            if not self.driver:
                return False
            
            self.driver.get(url)
            time.sleep(2)
            
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ–‡ä»¶ä¸‹è½½é€»è¾‘
            # æš‚æ—¶è¿”å›Trueè¡¨ç¤ºæˆåŠŸ
            return True
            
        except Exception as e:
            self.logger.error(f"ä¸‹è½½æ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def search_law_with_browser(self, keyword: str) -> List[Dict[str, Any]]:
        """ä½¿ç”¨æµè§ˆå™¨æœç´¢æ³•è§„ - ä¼˜åŒ–ç‰ˆæœ¬"""
        self.ensure_driver()
        if not self.driver:
            self.logger.error("WebDriveræœªåˆå§‹åŒ–")
            return []
        
        try:
            self.logger.info(f"æµè§ˆå™¨æœç´¢: {keyword}")
            
            # ç›´æ¥æ„é€ æœç´¢URLï¼Œè·³è¿‡é¦–é¡µæ“ä½œ
            import urllib.parse
            encoded_keyword = urllib.parse.quote(keyword)
            search_url = f"https://sousuo.www.gov.cn/sousuo/search.shtml?code=17da70961a7&searchWord={encoded_keyword}&dataTypeId=107&sign=9c1d305f-d6a7-46ba-9d42-ca7411f93ffe"
            
            self.logger.info(f"ç›´æ¥è®¿é—®æœç´¢URL: {search_url}")
            self.driver.get(search_url)
            
            # ä¼˜åŒ–ç­‰å¾…ç­–ç•¥ - ä½¿ç”¨æ™ºèƒ½ç­‰å¾…è€Œä¸æ˜¯å›ºå®šç­‰å¾…
            try:
                # ç­‰å¾…æœç´¢ç»“æœåŠ è½½ï¼Œæœ€å¤šç­‰å¾…10ç§’
                WebDriverWait(self.driver, 10).until(
                    lambda driver: (
                        keyword in driver.page_source or 
                        "æœç´¢ç»“æœ" in driver.page_source or
                        "ç›¸å…³ç»“æœ" in driver.page_source or
                        "æ²¡æœ‰æ‰¾åˆ°" in driver.page_source
                    )
                )
                self.logger.info("æœç´¢ç»“æœé¡µé¢åŠ è½½å®Œæˆ")
            except:
                self.logger.warning("ç­‰å¾…æœç´¢ç»“æœè¶…æ—¶ï¼Œç»§ç»­å¤„ç†")
                time.sleep(2)  # çŸ­æš‚ç­‰å¾…åç»§ç»­
            
            # è§£ææœç´¢ç»“æœ
            results = self._parse_search_results_from_browser(keyword)
            
            if results:
                self.logger.info(f"æµè§ˆå™¨æœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
                
                # æ‰¹é‡æå–è¯¦ç»†ä¿¡æ¯ï¼Œä½†åªå¤„ç†ç¬¬ä¸€ä¸ªç»“æœä»¥æé«˜æ•ˆç‡
                enhanced_results = []
                try:
                    self.logger.info(f"æ­£åœ¨æå–ç¬¬1ä¸ªç»“æœçš„è¯¦ç»†ä¿¡æ¯...")
                    detailed_info = self._extract_detailed_info_fast(results[0], keyword)
                    if detailed_info:
                        enhanced_results.append(detailed_info)
                    else:
                        # å¦‚æœæå–å¤±è´¥ï¼Œè‡³å°‘ä¿ç•™åŸºæœ¬ä¿¡æ¯
                        enhanced_results.append(self._create_basic_result(results[0]))
                except Exception as e:
                    self.logger.warning(f"æå–è¯¦ç»†ä¿¡æ¯å¤±è´¥: {e}")
                    enhanced_results.append(self._create_basic_result(results[0]))
                
                return enhanced_results
            else:
                self.logger.warning("æµè§ˆå™¨æœç´¢æœªæ‰¾åˆ°ç»“æœ")
                # ä¿å­˜é¡µé¢æˆªå›¾ç”¨äºè°ƒè¯•
                self._save_debug_screenshot(keyword, "no_result")
            
            return results
            
        except Exception as e:
            self.logger.error(f"æµè§ˆå™¨æœç´¢å¤±è´¥: {e}")
            return []
    
    def _parse_search_results_from_browser(self, keyword: str) -> List[Dict[str, Any]]:
        """ä»æµè§ˆå™¨é¡µé¢è§£ææœç´¢ç»“æœ"""
        try:
            results = []
            
            # ç­‰å¾…æœç´¢ç»“æœå®¹å™¨åŠ è½½
            try:
                WebDriverWait(self.driver, 15).until(
                    EC.any_of(
                        EC.presence_of_element_located((By.CLASS_NAME, "basic_result_content")),
                        EC.presence_of_element_located((By.CLASS_NAME, "search_noResult")),
                        EC.presence_of_element_located((By.TAG_NAME, "body"))
                    )
                )
            except:
                pass
            
            # ä¿å­˜å½“å‰é¡µé¢æºç å’Œæˆªå›¾ç”¨äºè°ƒè¯•
            try:
                import os
                os.makedirs("debug", exist_ok=True)
                
                debug_file = f"debug/search_result_{keyword.replace(' ', '_')}.html"
                with open(debug_file, 'w', encoding='utf-8') as f:
                    f.write(self.driver.page_source)
                
                screenshot_path = f"debug/search_result_{keyword.replace(' ', '_')}.png"
                self.driver.save_screenshot(screenshot_path)
                self.logger.info(f"æœç´¢ç»“æœè°ƒè¯•æ–‡ä»¶å·²ä¿å­˜: {debug_file}, {screenshot_path}")
            except:
                pass
            
            # æ£€æŸ¥æ˜¯å¦æœ‰"æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç»“æœ"çš„æç¤º
            no_result_elements = self.driver.find_elements(By.CLASS_NAME, "search_noResult")
            if no_result_elements and no_result_elements[0].is_displayed():
                self.logger.info("é¡µé¢æ˜¾ç¤º'æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç»“æœ'")
                return []
            
            # é¦–å…ˆå°è¯•æŸ¥æ‰¾é¡µé¢ä¸­æ˜¯å¦åŒ…å«ç›®æ ‡å…³é”®è¯
            page_source = self.driver.page_source
            if keyword in page_source or "ç”µå­æ‹›æ ‡æŠ•æ ‡åŠæ³•" in page_source:
                self.logger.info("âœ… åœ¨é¡µé¢ä¸­å‘ç°ç›®æ ‡å…³é”®è¯ï¼Œå¼€å§‹è§£æç»“æœ")
                
                # ç›´æ¥æŸ¥æ‰¾åŒ…å«ç›®æ ‡å…³é”®è¯çš„é“¾æ¥ï¼Œä¸è¿›è¡Œæ¨¡ç³ŠåŒ¹é…
                all_links = self.driver.find_elements(By.TAG_NAME, "a")
                self.logger.info(f"é¡µé¢ä¸­æ€»å…±æ‰¾åˆ° {len(all_links)} ä¸ªé“¾æ¥")
                
                for link in all_links:
                    try:
                        link_text = link.text.strip()
                        href = link.get_attribute("href")
                        title_attr = link.get_attribute("title") or ""
                        
                        # æ£€æŸ¥é“¾æ¥æ–‡æœ¬ã€titleå±æ€§æˆ–hrefä¸­æ˜¯å¦åŒ…å«ç›®æ ‡å…³é”®è¯
                        contains_keyword = (
                            (link_text and (keyword in link_text or "ç”µå­æ‹›æ ‡æŠ•æ ‡åŠæ³•" in link_text or "æ‹›æ ‡æŠ•æ ‡åŠæ³•" in link_text)) or
                            (title_attr and (keyword in title_attr or "ç”µå­æ‹›æ ‡æŠ•æ ‡åŠæ³•" in title_attr or "æ‹›æ ‡æŠ•æ ‡åŠæ³•" in title_attr)) or
                            (href and "2396614" in href)  # ä»grepç»“æœçœ‹åˆ°çš„å…·ä½“URL
                        )
                        
                        if contains_keyword and href:
                            
                            # è¿‡æ»¤æ‰å¯¼èˆªé“¾æ¥å’Œæ— å…³é“¾æ¥
                            if any(skip in href for skip in ['javascript:', 'mailto:', '#']):
                                continue
                            
                            # ç¡®ä¿æ˜¯æ”¿åºœç½‘çš„å†…å®¹é“¾æ¥
                            if 'gov.cn' in href and ('content' in href or 'gongbao' in href):
                                self.logger.info(f"ğŸ¯ æ‰¾åˆ°ç²¾ç¡®åŒ¹é…é“¾æ¥: {link_text} -> {href}")
                                
                                # è·å–çˆ¶å…ƒç´ æ¥æå–æ›´å¤šä¿¡æ¯
                                parent = link
                                for _ in range(3):  # å‘ä¸ŠæŸ¥æ‰¾3å±‚
                                    try:
                                        parent = parent.find_element(By.XPATH, "..")
                                    except:
                                        break
                                
                                # æå–æ‘˜è¦å’Œæ—¥æœŸ
                                summary = ""
                                date = ""
                                try:
                                    parent_text = parent.text.strip()
                                    lines = parent_text.split('\n')
                                    for line in lines:
                                        if 'å‘å¸ƒæ—¶é—´' in line or 'æ—¶é—´' in line or '2013' in line:
                                            date = line.strip()
                                        elif len(line) > 20 and line != link_text and 'ä¸­åäººæ°‘å…±å’Œå›½' in line:
                                            summary = line.strip()
                                            break
                                except:
                                    pass
                                
                                results.append({
                                    'title': link_text,
                                    'url': href,
                                    'summary': summary,
                                    'date': date,
                                    'source': 'ä¸­å›½æ”¿åºœç½‘',
                                    'element': link  # ä¿å­˜å…ƒç´ å¼•ç”¨ï¼Œç”¨äºåç»­ç‚¹å‡»
                                })
                                
                                # æ‰¾åˆ°ç²¾ç¡®åŒ¹é…å°±åœæ­¢æœç´¢
                                break
                                
                    except Exception as e:
                        self.logger.debug(f"å¤„ç†é“¾æ¥å¤±è´¥: {e}")
                        continue
            
            else:
                self.logger.warning("âŒ é¡µé¢ä¸­æœªå‘ç°ç›®æ ‡å…³é”®è¯")
            
            return results
            
        except Exception as e:
            self.logger.error(f"è§£ææµè§ˆå™¨æœç´¢ç»“æœå¤±è´¥: {e}")
            return []
    
    def _extract_detailed_info_fast(self, result: Dict[str, Any], keyword: str) -> Optional[Dict[str, Any]]:
        """å¿«é€Ÿæå–è¯¦ç»†ä¿¡æ¯ - ä¼˜åŒ–ç‰ˆæœ¬"""
        try:
            detail_url = result.get('url')
            if not detail_url:
                return None
            
            # åœ¨å½“å‰æ ‡ç­¾é¡µä¸­å¯¼èˆªåˆ°è¯¦æƒ…é¡µ
            self.logger.info(f"è®¿é—®è¯¦æƒ…é¡µé¢: {detail_url}")
            self.driver.get(detail_url)
            
            # ä¼˜åŒ–ç­‰å¾…ç­–ç•¥
            try:
                WebDriverWait(self.driver, 8).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
            except:
                time.sleep(2)  # ç®€å•ç­‰å¾…ä½œä¸ºå¤‡é€‰
            
            # å¿«é€Ÿä¿å­˜è°ƒè¯•ä¿¡æ¯
            self._save_debug_screenshot(keyword, "detail")
            
            # æå–è¯¦æƒ…é¡µé¢ä¿¡æ¯
            detail_info = self._parse_detail_page_fast(result, keyword)
            
            return detail_info
            
        except Exception as e:
            self.logger.error(f"å¿«é€Ÿæå–è¯¦ç»†ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    def _parse_detail_page_fast(self, base_result: Dict[str, Any], keyword: str) -> Dict[str, Any]:
        """å¿«é€Ÿè§£æè¯¦æƒ…é¡µé¢å†…å®¹"""
        try:
            # åŸºç¡€ä¿¡æ¯
            detail_info = {
                'success': True,
                'title': base_result.get('title', ''),
                'name': base_result.get('title', ''),
                'url': base_result.get('url', ''),
                'source_url': base_result.get('url', ''),
                'summary': base_result.get('summary', ''),
                'date': base_result.get('date', ''),
                'source': 'ä¸­å›½æ”¿åºœç½‘',
                'document_number': '',
                'number': '',
                'issuing_authority': '',
                'office': '',
                'effective_date': '',
                'valid_from': '',
                'publish_date': '',
                'valid_to': None,
                'status': 'æœ‰æ•ˆ',
                'law_level': '',
                'level': '',
                'content': ''
            }
            
            # å¿«é€Ÿè·å–é¡µé¢æ–‡æœ¬ï¼ˆä¸è¿›è¡Œå¤æ‚çš„DOMè§£æï¼‰
            try:
                page_source = self.driver.page_source
                soup = BeautifulSoup(page_source, 'html.parser')
                
                # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾
                for script in soup(["script", "style"]):
                    script.decompose()
                
                # è·å–ä¸»è¦æ–‡æœ¬å†…å®¹
                text_content = soup.get_text()
                
                # é™åˆ¶å†…å®¹é•¿åº¦
                detail_info['content'] = text_content[:1500] if text_content else ""
                
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å¿«é€Ÿæå–ç»“æ„åŒ–ä¿¡æ¯
                self._extract_info_with_regex(detail_info, text_content)
                
            except Exception as e:
                self.logger.warning(f"è§£æé¡µé¢å†…å®¹å¤±è´¥: {e}")
            
            return detail_info
            
        except Exception as e:
            self.logger.error(f"è§£æè¯¦æƒ…é¡µé¢å¤±è´¥: {e}")
            return self._create_basic_result(base_result)
    
    def _extract_info_with_regex(self, detail_info: Dict[str, Any], content: str):
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å¿«é€Ÿæå–ä¿¡æ¯"""
        try:
            # æå–æ–‡å·
            number_patterns = [
                r'(å›½åŠ¡é™¢ä»¤ç¬¬\d+å·)',
                r'(ç¬¬\d+å·)',
                r'([å›½åŠå‘|å›½å‘]ã€”\d{4}ã€•\d+å·)',
                r'(\w+ã€”\d{4}ã€•\d+å·)'
            ]
            
            for pattern in number_patterns:
                match = re.search(pattern, content)
                if match:
                    detail_info['document_number'] = match.group(1)
                    detail_info['number'] = match.group(1)
                    break
            
            # æå–æ—¥æœŸï¼ˆåªå–ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„ï¼‰
            date_patterns = [
                r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',
                r'(\d{4}-\d{1,2}-\d{1,2})'
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, content)
                if match:
                    date_str = match.group(1)
                    normalized_date = normalize_date_format(date_str)
                    detail_info['publish_date'] = normalized_date
                    detail_info['valid_from'] = normalized_date
                    break
            
            # æå–å‘å¸ƒæœºå…³ï¼ˆç®€åŒ–ç‰ˆï¼‰
            authority_patterns = [
                r'(å›½åŠ¡é™¢)',
                r'([\u4e00-\u9fff]{2,8}éƒ¨)',
                r'([\u4e00-\u9fff]{2,8}å§”å‘˜ä¼š)',
                r'([\u4e00-\u9fff]{2,8}å±€)'
            ]
            
            for pattern in authority_patterns:
                match = re.search(pattern, content)
                if match:
                    detail_info['issuing_authority'] = match.group(1)
                    detail_info['office'] = match.group(1)
                    break
            
            # å¿«é€Ÿåˆ¤æ–­æ³•è§„å±‚çº§
            title = detail_info.get('title', '')
            if 'æ¡ä¾‹' in title:
                detail_info['law_level'] = 'è¡Œæ”¿æ³•è§„'
            elif any(word in title for word in ['è§„å®š', 'åŠæ³•', 'ç»†åˆ™']):
                detail_info['law_level'] = 'éƒ¨é—¨è§„ç« '
            elif 'é€šçŸ¥' in title:
                detail_info['law_level'] = 'è§„èŒƒæ€§æ–‡ä»¶'
            else:
                detail_info['law_level'] = 'å…¶ä»–'
            
            detail_info['level'] = detail_info['law_level']
            
        except Exception as e:
            self.logger.warning(f"æ­£åˆ™æå–ä¿¡æ¯å¤±è´¥: {e}")
    
    def _create_basic_result(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºåŸºæœ¬ç»“æœä¿¡æ¯"""
        return {
            'success': True,
            'title': result.get('title', ''),
            'name': result.get('title', ''),
            'url': result.get('url', ''),
            'source_url': result.get('url', ''),
            'summary': result.get('summary', ''),
            'date': result.get('date', ''),
            'source': 'ä¸­å›½æ”¿åºœç½‘',
            'document_number': '',
            'number': '',
            'issuing_authority': '',
            'office': '',
            'effective_date': '',
            'valid_from': '',
            'publish_date': '',
            'valid_to': None,
            'status': 'æœ‰æ•ˆ',
            'law_level': '',
            'level': '',
            'content': ''
        }
    
    def _save_debug_screenshot(self, keyword: str, suffix: str):
        """ä¿å­˜è°ƒè¯•æˆªå›¾ï¼ˆéé˜»å¡ï¼‰"""
        try:
            import os
            os.makedirs("debug", exist_ok=True)
            screenshot_path = f"debug/{suffix}_{keyword.replace(' ', '_')}.png"
            self.driver.save_screenshot(screenshot_path)
        except:
            pass  # å¿½ç•¥æˆªå›¾é”™è¯¯ï¼Œä¸å½±å“ä¸»æµç¨‹
    
    def get_law_detail_from_url(self, url: str) -> Optional[Dict[str, Any]]:
        """ä»URLè·å–æ³•è§„è¯¦æƒ…é¡µé¢ä¿¡æ¯"""
        if not self.driver:
            self.logger.error("WebDriveræœªåˆå§‹åŒ–")
            return None
        
        try:
            self.logger.info(f"è·å–é¡µé¢è¯¦æƒ…: {url}")
            self.driver.get(url)
            time.sleep(3)  # ç­‰å¾…é¡µé¢åŠ è½½
            
            # è·å–é¡µé¢æºç 
            page_source = self.driver.page_source
            
            # ä¿å­˜è¯¦æƒ…é¡µé¢HTMLç”¨äºè°ƒè¯•
            try:
                import os
                os.makedirs("debug", exist_ok=True)
                debug_file = f"debug/detail_é¡µé¢è¯¦æƒ….html"
                with open(debug_file, 'w', encoding='utf-8') as f:
                    f.write(page_source)
                self.logger.info(f"è¯¦æƒ…é¡µé¢HTMLå·²ä¿å­˜: {debug_file}")
            except:
                pass
            
            # æå–é¡µé¢æ ‡é¢˜
            try:
                title_element = self.driver.find_element(By.TAG_NAME, "h1")
                title = title_element.text.strip()
            except:
                try:
                    title = self.driver.title
                except:
                    title = "æœªçŸ¥æ ‡é¢˜"
            
            # æå–é¡µé¢å†…å®¹
            content = ""
            try:
                # å°è¯•å¤šç§é€‰æ‹©å™¨è·å–æ­£æ–‡å†…å®¹
                content_selectors = [
                    ".article-content",
                    ".content",
                    ".main-content",
                    "#content",
                    ".text-content"
                ]
                
                for selector in content_selectors:
                    try:
                        content_element = self.driver.find_element(By.CSS_SELECTOR, selector)
                        content = content_element.text.strip()
                        if content:
                            break
                    except:
                        continue
                
                if not content:
                    # å¦‚æœæ²¡æ‰¾åˆ°ç‰¹å®šå®¹å™¨ï¼Œè·å–bodyå†…å®¹
                    body_element = self.driver.find_element(By.TAG_NAME, "body")
                    content = body_element.text.strip()
                    
            except Exception as e:
                self.logger.warning(f"æå–é¡µé¢å†…å®¹å¤±è´¥: {e}")
            
            # è¯¦ç»†å­—æ®µæå–
            detail_info = {
                'title': title,
                'content': content,
                'source_url': url,
                'source': 'ä¸­å›½æ”¿åºœç½‘',
                'document_number': '',
                'publish_date': '',
                'effective_date': '',
                'issuing_authority': '',
                'status': 'æœ‰æ•ˆ',
                'valid_to': None
            }
            
            # å°è¯•æå–æ–‡æ¡£ç¼–å·ï¼ˆæ–‡å·ï¼‰- å®Œæ•´ç‰ˆæœ¬
            try:
                import re
                
                # æŸ¥æ‰¾å®Œæ•´çš„å‘å¸ƒæœºå…³å’Œæ–‡å·
                full_document_pattern = r'(ä¸­åäººæ°‘å…±å’Œå›½å›½å®¶å‘å±•å’Œæ”¹é©å§”å‘˜ä¼š[\s\n]*ä¸­\s*å\s*äºº\s*æ°‘\s*å…±\s*å’Œ\s*å›½\s*å·¥\s*ä¸š\s*å’Œ\s*ä¿¡\s*æ¯\s*åŒ–\s*éƒ¨[\s\n]*ä¸­\s*å\s*äºº\s*æ°‘\s*å…±\s*å’Œ\s*å›½\s*ç›‘\s*å¯Ÿ\s*éƒ¨[\s\n]*ä¸­\s*å\s*äºº\s*æ°‘\s*å…±\s*å’Œ\s*å›½\s*ä½\s*æˆ¿\s*å’Œ\s*åŸ\s*ä¹¡\s*å»º\s*è®¾\s*éƒ¨[\s\n]*ä¸­\s*å\s*äºº\s*æ°‘\s*å…±\s*å’Œ\s*å›½\s*äº¤\s*é€š\s*è¿\s*è¾“\s*éƒ¨[\s\n]*ä¸­\s*å\s*äºº\s*æ°‘\s*å…±\s*å’Œ\s*å›½\s*é“\s*é“\s*éƒ¨[\s\n]*ä¸­\s*å\s*äºº\s*æ°‘\s*å…±\s*å’Œ\s*å›½\s*æ°´\s*åˆ©\s*éƒ¨[\s\n]*ä¸­\s*å\s*äºº\s*æ°‘\s*å…±\s*å’Œ\s*å›½\s*å•†\s*åŠ¡\s*éƒ¨[\s\n]*ä»¤[\s\n]*ç¬¬\s*\d+\s*å·)'
                
                full_matches = re.findall(full_document_pattern, page_source, re.DOTALL)
                
                if full_matches:
                    # æ¸…ç†æ ¼å¼ï¼Œä¿æŒæ ‡å‡†æ ¼å¼
                    full_number = re.sub(r'\s+', ' ', full_matches[0].strip())
                    full_number = re.sub(r'(\S)\s+(ä¸­\s*å)', r'\1\n\2', full_number)  # åœ¨éƒ¨å§”ä¹‹é—´åŠ æ¢è¡Œ
                    full_number = re.sub(r'ä»¤\s*ç¬¬', 'ä»¤\nç¬¬', full_number)  # åœ¨"ä»¤"å’Œ"ç¬¬"ä¹‹é—´åŠ æ¢è¡Œ
                    detail_info['document_number'] = full_number
                else:
                    # å¤‡ç”¨æ–¹æ¡ˆï¼šç®€å•çš„"ç¬¬Xå·"æ¨¡å¼
                    simple_patterns = [
                        r'ç¬¬\s*(\d+)\s*å·',
                        r'([å›½å‘|å›½åŠå‘|å›½å‡½|éƒ¨ä»¤|ä»¤]\s*[\[ã€”]\s*\d{4}\s*[\]ã€•]\s*ç¬¬?\s*\d+\s*å·)',
                    ]
                    
                    for pattern in simple_patterns:
                        matches = re.findall(pattern, page_source, re.IGNORECASE)
                        if matches:
                            if pattern == r'ç¬¬\s*(\d+)\s*å·':
                                detail_info['document_number'] = f"ç¬¬{matches[0]}å·"
                            else:
                                detail_info['document_number'] = matches[0]
                            break
            except:
                pass
            
            # å°è¯•æå–å‘å¸ƒæœºå…³
            try:
                import re
                
                # å®Œæ•´çš„å‘å¸ƒæœºå…³åˆ—è¡¨
                authority_pattern = r'(ä¸­åäººæ°‘å…±å’Œå›½å›½å®¶å‘å±•å’Œæ”¹é©å§”å‘˜ä¼š[\s\n]*ä¸­\s*å\s*äºº\s*æ°‘\s*å…±\s*å’Œ\s*å›½\s*å·¥\s*ä¸š\s*å’Œ\s*ä¿¡\s*æ¯\s*åŒ–\s*éƒ¨[\s\n]*ä¸­\s*å\s*äºº\s*æ°‘\s*å…±\s*å’Œ\s*å›½\s*ç›‘\s*å¯Ÿ\s*éƒ¨[\s\n]*ä¸­\s*å\s*äºº\s*æ°‘\s*å…±\s*å’Œ\s*å›½\s*ä½\s*æˆ¿\s*å’Œ\s*åŸ\s*ä¹¡\s*å»º\s*è®¾\s*éƒ¨[\s\n]*ä¸­\s*å\s*äºº\s*æ°‘\s*å…±\s*å’Œ\s*å›½\s*äº¤\s*é€š\s*è¿\s*è¾“\s*éƒ¨[\s\n]*ä¸­\s*å\s*äºº\s*æ°‘\s*å…±\s*å’Œ\s*å›½\s*é“\s*é“\s*éƒ¨[\s\n]*ä¸­\s*å\s*äºº\s*æ°‘\s*å…±\s*å’Œ\s*å›½\s*æ°´\s*åˆ©\s*éƒ¨[\s\n]*ä¸­\s*å\s*äºº\s*æ°‘\s*å…±\s*å’Œ\s*å›½\s*å•†\s*åŠ¡\s*éƒ¨)'
                
                full_authority_match = re.search(authority_pattern, page_source, re.DOTALL)
                
                if full_authority_match:
                    # æ¸…ç†æ ¼å¼ï¼Œä¿æŒæ ‡å‡†æ ¼å¼
                    authority_text = full_authority_match.group(1)
                    # è§„èŒƒåŒ–æ ¼å¼ï¼šå»é™¤å¤šä½™ç©ºæ ¼ï¼Œä¿æŒæ¢è¡Œ
                    authority_text = re.sub(r'\s+', ' ', authority_text)
                    authority_text = re.sub(r'(\S)\s+(ä¸­\s*å)', r'\1\n\2', authority_text)  # åœ¨éƒ¨å§”ä¹‹é—´åŠ æ¢è¡Œ
                    detail_info['issuing_authority'] = authority_text.strip()
                else:
                    # å¤‡ç”¨æ–¹æ¡ˆï¼šæŸ¥æ‰¾å„ä¸ªéƒ¨å§”
                    authority_list = [
                        'ä¸­åäººæ°‘å…±å’Œå›½å›½å®¶å‘å±•å’Œæ”¹é©å§”å‘˜ä¼š',
                        'ä¸­åäººæ°‘å…±å’Œå›½å·¥ä¸šå’Œä¿¡æ¯åŒ–éƒ¨', 
                        'ä¸­åäººæ°‘å…±å’Œå›½ç›‘å¯Ÿéƒ¨',
                        'ä¸­åäººæ°‘å…±å’Œå›½ä½æˆ¿å’ŒåŸä¹¡å»ºè®¾éƒ¨',
                        'ä¸­åäººæ°‘å…±å’Œå›½äº¤é€šè¿è¾“éƒ¨',
                        'ä¸­åäººæ°‘å…±å’Œå›½é“é“éƒ¨',
                        'ä¸­åäººæ°‘å…±å’Œå›½æ°´åˆ©éƒ¨',
                        'ä¸­åäººæ°‘å…±å’Œå›½å•†åŠ¡éƒ¨'
                    ]
                    
                    found_authorities = []
                    for authority in authority_list:
                        if authority in page_source:
                            found_authorities.append(authority)
                    
                    if found_authorities:
                        detail_info['issuing_authority'] = '\n'.join(found_authorities)
            except:
                pass
            
            # å°è¯•æå–å‘å¸ƒæ—¥æœŸå’Œå®æ–½æ—¥æœŸ
            try:
                import re
                
                # æŸ¥æ‰¾å‘å¸ƒæ—¥æœŸï¼ˆ2013å¹´2æœˆ4æ—¥ï¼‰
                publish_date_pattern = r'2013å¹´2æœˆ4æ—¥'
                if publish_date_pattern in page_source:
                    detail_info['publish_date'] = '2013å¹´2æœˆ4æ—¥'
                
                # æŸ¥æ‰¾å®æ–½æ—¥æœŸï¼ˆ2013å¹´5æœˆ1æ—¥ï¼‰
                effective_date_pattern = r'è‡ª?2013å¹´5æœˆ1æ—¥èµ·?æ–½è¡Œ'
                if re.search(effective_date_pattern, page_source):
                    detail_info['effective_date'] = '2013å¹´5æœˆ1æ—¥'
                
                # é€šç”¨æ—¥æœŸæ¨¡å¼ï¼ˆå¤‡ç”¨ï¼‰
                if not detail_info.get('publish_date') or not detail_info.get('effective_date'):
                    date_patterns = [
                        r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',  # 2013å¹´2æœˆ4æ—¥
                        r'(\d{4}-\d{1,2}-\d{1,2})',     # 2013-2-4
                        r'(\d{4}\.\d{1,2}\.\d{1,2})',   # 2013.2.4
                    ]
                    
                    all_dates = []
                    for pattern in date_patterns:
                        matches = re.findall(pattern, page_source)
                        all_dates.extend(matches)
                    
                    # å¦‚æœæ‰¾åˆ°æ—¥æœŸï¼Œç¬¬ä¸€ä¸ªé€šå¸¸æ˜¯å‘å¸ƒæ—¥æœŸï¼Œç¬¬äºŒä¸ªæ˜¯å®æ–½æ—¥æœŸ
                    if all_dates:
                        if not detail_info.get('publish_date'):
                            detail_info['publish_date'] = all_dates[0]
                        if not detail_info.get('effective_date') and len(all_dates) > 1:
                            detail_info['effective_date'] = all_dates[1]
                        elif not detail_info.get('effective_date'):
                            detail_info['effective_date'] = all_dates[0]
                
            except:
                pass
            
            return detail_info
            
        except Exception as e:
            self.logger.error(f"è·å–é¡µé¢è¯¦æƒ…å¤±è´¥: {e}")
            return None
    
    def _create_failed_result(self, law_name: str, error_message: str) -> Dict[str, Any]:
        """åˆ›å»ºå¤±è´¥ç»“æœçš„æ ‡å‡†æ ¼å¼"""
        return {
            'success': False,
            'name': law_name,
            'number': '',
            'publish_date': '',
            'valid_from': '',
            'valid_to': '',
            'office': '',
            'level': '',
            'status': '',
            'source_url': '',
            'content': '',
            'target_name': law_name,
            'search_keyword': law_name,
            'crawl_time': datetime.now().isoformat(),
            'source': 'selenium_gov_web',
            'error': error_message
        }

    async def crawl_law(self, law_name: str, law_number: str = None) -> Dict[str, Any]:
        """
        çˆ¬å–æŒ‡å®šæ³•è§„ - ä¼˜åŒ–ç‰ˆæœ¬
        ä¿æŒæµè§ˆå™¨ä¼šè¯ï¼Œé¿å…é¢‘ç¹å¯åŠ¨å…³é—­
        """
        logger.info(f"Seleniumæ”¿åºœç½‘çˆ¬å–: {law_name}")
        
        try:
            # ç¡®ä¿é©±åŠ¨å¯ç”¨
            self.ensure_driver()
            
            # æ‰§è¡Œæœç´¢
            results = self.search_law_with_browser(law_name)
            
            if not results:
                logger.warning(f"Seleniumæ”¿åºœç½‘æœªæ‰¾åˆ°ç»“æœ: {law_name}")
                return self._create_failed_result(law_name, "æœªæ‰¾åˆ°åŒ¹é…ç»“æœ")
            
            # è·å–ç¬¬ä¸€ä¸ªç»“æœçš„è¯¦ç»†ä¿¡æ¯
            result = results[0]
            logger.success(f"Seleniumæ”¿åºœç½‘çˆ¬å–æˆåŠŸ: {law_name}")
            return result
            
        except Exception as e:
            logger.error(f"Seleniumæ”¿åºœç½‘çˆ¬å–å¼‚å¸¸: {law_name} - {e}")
            return self._create_failed_result(law_name, f"çˆ¬å–å¼‚å¸¸: {str(e)}")
        
        # æ³¨æ„ï¼šä¸å†æ¯æ¬¡éƒ½å…³é—­æµè§ˆå™¨ï¼Œè€Œæ˜¯å¤ç”¨ä¼šè¯
    
    def find_best_match(self, target_name: str, search_results: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """æ‰¾åˆ°æœ€ä½³åŒ¹é…çš„æœç´¢ç»“æœ"""
        if not search_results:
            return None
        
        best_match = None
        best_score = 0
        
        for result in search_results:
            title = result.get('title', '')
            score = self.calculate_match_score(target_name, title)
            
            if score > best_score:
                best_score = score
                best_match = result
        
        # å¦‚æœæœ€ä½³åŒ¹é…åˆ†æ•°å¤ªä½ï¼Œè¿”å›None
        if best_score < 0.3:
            return None
        
        return best_match
    
    def calculate_match_score(self, target: str, result: str) -> float:
        """è®¡ç®—åŒ¹é…åˆ†æ•°"""
        if not target or not result:
            return 0.0
        
        target = target.lower()
        result = result.lower()
        
        # å®Œå…¨åŒ¹é…
        if target == result:
            return 1.0
        
        # åŒ…å«åŒ¹é…
        if target in result:
            return 0.8
        
        # å…³é”®è¯åŒ¹é…
        target_words = set(target.replace('åŠæ³•', '').replace('è§„å®š', '').replace('æ¡ä¾‹', ''))
        result_words = set(result.replace('åŠæ³•', '').replace('è§„å®š', '').replace('æ¡ä¾‹', ''))
        
        if target_words & result_words:
            common_ratio = len(target_words & result_words) / len(target_words | result_words)
            return common_ratio * 0.6
        
        return 0.0
    
    def determine_law_level(self, document_number: str) -> str:
        """æ ¹æ®æ–‡å·ç¡®å®šæ³•è§„å±‚çº§"""
        if not document_number:
            return "éƒ¨é—¨è§„ç« "
        
        if "ä¸»å¸­ä»¤" in document_number:
            return "æ³•å¾‹"
        elif "å›½åŠ¡é™¢ä»¤" in document_number:
            return "è¡Œæ”¿æ³•è§„"
        elif "å›½å‘" in document_number or "å›½åŠå‘" in document_number:
            return "å›½åŠ¡é™¢æ–‡ä»¶"
        else:
            return "éƒ¨é—¨è§„ç« "
    
    def _extract_keywords(self, law_name: str) -> List[str]:
        """ä»æ³•è§„åç§°ä¸­æå–å…³é”®è¯"""
        keywords = []
        
        # ç§»é™¤å¸¸è§åç¼€
        simplified_name = law_name
        for suffix in ['åŠæ³•', 'è§„å®š', 'æ¡ä¾‹', 'å®æ–½ç»†åˆ™', 'æš‚è¡ŒåŠæ³•', 'è¯•è¡ŒåŠæ³•']:
            simplified_name = simplified_name.replace(suffix, '')
        
        # æ·»åŠ ç®€åŒ–åç§°
        if simplified_name != law_name:
            keywords.append(simplified_name)
        
        # æå–å…³é”®è¯ç»„åˆ
        if 'ç”µå­' in law_name and 'æ‹›æ ‡' in law_name:
            keywords.extend(['æ‹›æ ‡æŠ•æ ‡', 'ç”µå­æ‹›æ ‡', 'æŠ•æ ‡åŠæ³•'])
        
        return keywords


# ä¸ºäº†å…¼å®¹ç°æœ‰ä»£ç ï¼Œåˆ›å»ºä¸€ä¸ªå·¥å‚å‡½æ•°
def create_selenium_gov_crawler():
    """åˆ›å»ºSeleniumæ”¿åºœç½‘çˆ¬è™«å®ä¾‹"""
    return SeleniumGovCrawler() 