#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
åŸºäºSeleniumçš„æ”¿åºœç½‘çˆ¬è™«
ä½¿ç”¨çœŸå®æµè§ˆå™¨ç¯å¢ƒç»•è¿‡åçˆ¬æœºåˆ¶
"""

import time
import json
from datetime import datetime
from typing import Dict, List, Optional, Any
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from loguru import logger

from ..base_crawler import BaseCrawler


def normalize_date_format(date_str: str) -> str:
    """
    å°†å„ç§æ—¥æœŸæ ¼å¼ç»Ÿä¸€è½¬æ¢ä¸º yyyy-mm-dd æ ¼å¼
    æ”¯æŒçš„è¾“å…¥æ ¼å¼ï¼š
    - 2013å¹´2æœˆ4æ—¥ -> 2013-02-04
    - 2013-2-4 -> 2013-02-04
    - 2013.2.4 -> 2013-02-04
    - 2025-05-29 00:00:00 -> 2025-05-29
    """
    if not date_str or date_str.strip() == '':
        return ''
    
    import re
    from datetime import datetime
    
    date_str = str(date_str).strip()
    
    try:
        # æ ¼å¼1: 2013å¹´2æœˆ4æ—¥
        match = re.match(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', date_str)
        if match:
            year, month, day = match.groups()
            return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
        
        # æ ¼å¼2: 2013-2-4 æˆ– 2013/2/4 æˆ– 2013.2.4
        match = re.match(r'(\d{4})[-/.](\d{1,2})[-/.](\d{1,2})', date_str)
        if match:
            year, month, day = match.groups()
            return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
        
        # æ ¼å¼3: 2025-05-29 00:00:00 (å¸¦æ—¶é—´)
        match = re.match(r'(\d{4}-\d{2}-\d{2})\s+\d{2}:\d{2}:\d{2}', date_str)
        if match:
            return match.group(1)
        
        # æ ¼å¼4: å·²ç»æ˜¯ yyyy-mm-dd æ ¼å¼
        match = re.match(r'\d{4}-\d{2}-\d{2}$', date_str)
        if match:
            return date_str
        
        # æ ¼å¼5: å°è¯•ä½¿ç”¨datetimeè§£æ
        for fmt in ['%Y-%m-%d', '%Y/%m/%d', '%Y.%m.%d', '%Yå¹´%mæœˆ%dæ—¥']:
            try:
                dt = datetime.strptime(date_str, fmt)
                return dt.strftime('%Y-%m-%d')
            except:
                continue
        
        # å¦‚æœéƒ½æ— æ³•è§£æï¼Œè¿”å›åŸå§‹å­—ç¬¦ä¸²
        logger.warning(f"æ— æ³•è§£ææ—¥æœŸæ ¼å¼: {date_str}")
        return date_str
        
    except Exception as e:
        logger.warning(f"æ—¥æœŸæ ¼å¼åŒ–å¤±è´¥: {date_str}, é”™è¯¯: {e}")
        return date_str


class SeleniumGovCrawler(BaseCrawler):
    """åŸºäºSeleniumçš„ä¸­å›½æ”¿åºœç½‘çˆ¬è™«"""
    
    def __init__(self):
        super().__init__("selenium_gov")
        self.driver = None
        self.logger = logger
        self.setup_driver()
    
    def setup_driver(self):
        """è®¾ç½®Chrome WebDriver"""
        try:
            chrome_options = Options()
            
            # åŸºç¡€è®¾ç½®
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--disable-web-security')
            chrome_options.add_argument('--allow-running-insecure-content')
            chrome_options.add_argument('--ignore-certificate-errors')
            chrome_options.add_argument('--ignore-ssl-errors')
            chrome_options.add_argument('--ignore-certificate-errors-spki-list')
            
            # ç”¨æˆ·ä»£ç†è®¾ç½®
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            
            # å¯é€‰ï¼šæ— å¤´æ¨¡å¼ï¼ˆåå°è¿è¡Œï¼‰
            # chrome_options.add_argument('--headless')  # å–æ¶ˆæ³¨é‡Šä»¥å¯ç”¨æ— å¤´æ¨¡å¼
            
            # çª—å£å¤§å°
            chrome_options.add_argument('--window-size=1920,1080')
            
            # ç¦ç”¨å›¾ç‰‡åŠ è½½ä»¥æé«˜é€Ÿåº¦
            prefs = {
                "profile.managed_default_content_settings.images": 2,
                "profile.default_content_setting_values.notifications": 2
            }
            chrome_options.add_experimental_option("prefs", prefs)
            
            # å°è¯•è‡ªåŠ¨ä¸‹è½½ChromeDriverï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨ç³»ç»Ÿè·¯å¾„
            try:
                service = Service(ChromeDriverManager().install())
                self.logger.info("ä½¿ç”¨è‡ªåŠ¨ä¸‹è½½çš„ChromeDriver")
            except Exception as download_error:
                self.logger.warning(f"ChromeDriverè‡ªåŠ¨ä¸‹è½½å¤±è´¥: {download_error}")
                # å°è¯•ä½¿ç”¨ç³»ç»ŸPATHä¸­çš„chromedriver
                service = Service()  # ä½¿ç”¨é»˜è®¤è·¯å¾„
                self.logger.info("å°è¯•ä½¿ç”¨ç³»ç»ŸPATHä¸­çš„ChromeDriver")
            
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            self.driver.implicitly_wait(10)  # éšå¼ç­‰å¾…10ç§’
            
            self.logger.info("Chrome WebDriveråˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            self.logger.error(f"WebDriveråˆå§‹åŒ–å¤±è´¥: {e}")
            self.logger.info("å»ºè®®ï¼š")
            self.logger.info("1. ç¡®ä¿å·²å®‰è£…Chromeæµè§ˆå™¨")
            self.logger.info("2. æ£€æŸ¥ç½‘ç»œè¿æ¥")
            self.logger.info("3. æˆ–æ‰‹åŠ¨ä¸‹è½½ChromeDriverå¹¶æ·»åŠ åˆ°PATH")
            self.driver = None
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œç¡®ä¿æµè§ˆå™¨å…³é—­"""
        self.close_driver()
    
    def close_driver(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.driver:
            try:
                self.driver.quit()
                self.logger.info("æµè§ˆå™¨å·²å…³é—­")
            except:
                pass
            finally:
                self.driver = None
    
    async def search(self, law_name: str, law_number: Optional[str] = None) -> List[Dict[str, Any]]:
        """æœç´¢æ³•è§„ - å®ç°æŠ½è±¡æ–¹æ³•"""
        return self.search_law_with_browser(law_name)
    
    async def get_detail(self, law_id: str) -> Dict[str, Any]:
        """è·å–æ³•è§„è¯¦æƒ… - å®ç°æŠ½è±¡æ–¹æ³•"""
        # å¯¹äºæ”¿åºœç½‘ï¼Œlaw_idå®é™…ä¸Šæ˜¯URL
        return self.get_law_detail_from_url(law_id)
    
    async def download_file(self, url: str, save_path: str) -> bool:
        """ä¸‹è½½æ–‡ä»¶ - å®ç°æŠ½è±¡æ–¹æ³•"""
        try:
            if not self.driver:
                return False
            
            self.driver.get(url)
            time.sleep(2)
            
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ–‡ä»¶ä¸‹è½½é€»è¾‘
            # æš‚æ—¶è¿”å›Trueè¡¨ç¤ºæˆåŠŸ
            return True
            
        except Exception as e:
            self.logger.error(f"ä¸‹è½½æ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def search_law_with_browser(self, keyword: str) -> List[Dict[str, Any]]:
        """ä½¿ç”¨æµè§ˆå™¨æœç´¢æ³•è§„"""
        if not self.driver:
            self.logger.error("WebDriveræœªåˆå§‹åŒ–")
            return []
        
        try:
            self.logger.info(f"æµè§ˆå™¨æœç´¢: {keyword}")
            
            # æ­¥éª¤1: è®¿é—®æ”¿åºœç½‘é¦–é¡µ
            self.logger.info("è®¿é—®æ”¿åºœç½‘é¦–é¡µ...")
            self.driver.get("https://www.gov.cn")
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            WebDriverWait(self.driver, 15).until(
                EC.presence_of_element_located((By.CLASS_NAME, "header_search"))
            )
            self.logger.info("é¦–é¡µåŠ è½½å®Œæˆ")
            
            # æ­¥éª¤2: å®šä½æœç´¢æ¡†å¹¶è¾“å…¥å…³é”®è¯
            try:
                # ç­‰å¾…æœç´¢æ¡†å¯è§å¹¶å¯ç‚¹å‡»
                search_input = WebDriverWait(self.driver, 10).until(
                    EC.element_to_be_clickable((By.CSS_SELECTOR, "input.header_search_txt[name='headSearchword']"))
                )
                search_input.clear()
                search_input.send_keys(keyword)
                logger.info(f"å·²è¾“å…¥æœç´¢å…³é”®è¯: {keyword}")
                
                # ç‚¹å‡»æœç´¢æŒ‰é’® - ä½¿ç”¨å¤šç§å®šä½æ–¹å¼
                search_button = None
                try:
                    # æ–¹å¼1ï¼šé€šè¿‡CSSé€‰æ‹©å™¨å®šä½æŒ‰é’®
                    search_button = self.driver.find_element(By.CSS_SELECTOR, "button.header_search_btn")
                except:
                    try:
                        # æ–¹å¼2ï¼šé€šè¿‡XPathå®šä½æŒ‰é’®
                        search_button = self.driver.find_element(By.XPATH, "//button[@class='header_search_btn']")
                    except:
                        # æ–¹å¼3ï¼šé€šè¿‡çˆ¶å®¹å™¨æŸ¥æ‰¾æŒ‰é’®
                        search_form = self.driver.find_element(By.CLASS_NAME, "header_search")
                        search_button = search_form.find_element(By.TAG_NAME, "button")
                
                if search_button:
                    # ä½¿ç”¨JavaScriptç‚¹å‡»ï¼Œé¿å…è¢«å…¶ä»–å…ƒç´ é®æŒ¡
                    self.driver.execute_script("arguments[0].click();", search_button)
                    logger.info("å·²ç‚¹å‡»æœç´¢æŒ‰é’®")
                    
                    # ç­‰å¾…é¡µé¢è·³è½¬åˆ°æœç´¢ç»“æœé¡µ - ä½¿ç”¨æ›´çµæ´»çš„ç­‰å¾…ç­–ç•¥
                    try:
                        # æ–¹å¼1ï¼šç­‰å¾…URLå˜åŒ–ï¼ˆå¢åŠ ç­‰å¾…æ—¶é—´ï¼‰
                        WebDriverWait(self.driver, 20).until(
                            lambda driver: "sousuo" in driver.current_url.lower() or "search" in driver.current_url.lower()
                        )
                        logger.info("é¡µé¢è·³è½¬æˆåŠŸ")
                    except:
                        # æ–¹å¼2ï¼šå¦‚æœURLæ²¡å˜ï¼Œæ£€æŸ¥é¡µé¢å†…å®¹æ˜¯å¦å·²ç»å˜åŒ–
                        try:
                            WebDriverWait(self.driver, 10).until(
                                lambda driver: ("ç›¸å…³ç»“æœ" in driver.page_source or 
                                              "æœç´¢ç»“æœ" in driver.page_source or
                                              "æ’åºæ–¹å¼" in driver.page_source or
                                              keyword in driver.page_source)
                            )
                            logger.info("æœç´¢ç»“æœé¡µé¢å†…å®¹å·²åŠ è½½")
                        except:
                            # æ–¹å¼3ï¼šå»¶è¿Ÿæ£€æµ‹ - ç»™æ›´å¤šæ—¶é—´è®©é¡µé¢å®Œå…¨åŠ è½½
                            time.sleep(5)
                            current_url = self.driver.current_url
                            page_source = self.driver.page_source
                            
                            # æ›´å…¨é¢çš„æˆåŠŸåˆ¤æ–­æ¡ä»¶
                            success_indicators = [
                                "sousuo" in current_url.lower(),
                                "search" in current_url.lower(),
                                "ç›¸å…³ç»“æœ" in page_source,
                                "æœç´¢ç»“æœ" in page_source,
                                "æ’åºæ–¹å¼" in page_source,
                                keyword in page_source,
                                "æ£€ç´¢æ–¹å¼" in page_source
                            ]
                            
                            if any(success_indicators):
                                logger.info("æœç´¢é¡µé¢ç¡®è®¤æˆåŠŸï¼ˆå»¶è¿Ÿæ£€æµ‹ï¼‰")
                            else:
                                logger.warning(f"é¡µé¢è·³è½¬å¯èƒ½å¤±è´¥ï¼Œå½“å‰URL: {current_url}")
                                # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œè®©å¤‡ç”¨æ–¹æ¡ˆå¤„ç†
                                raise Exception("é¡µé¢æœªè·³è½¬åˆ°æœç´¢ç»“æœ")
                else:
                    raise Exception("æ— æ³•æ‰¾åˆ°æœç´¢æŒ‰é’®")
                
            except Exception as e:
                logger.warning(f"é¦–é¡µæœç´¢æ¡†æ“ä½œå¤±è´¥: {e}")
                # å¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥æ„é€ æœç´¢URL
                import urllib.parse
                encoded_keyword = urllib.parse.quote(keyword)
                search_url = f"https://sousuo.www.gov.cn/sousuo/search.shtml?code=17da70961a7&searchWord={encoded_keyword}&dataTypeId=107&sign=9c1d305f-d6a7-46ba-9d42-ca7411f93ffe"
                logger.info(f"ä½¿ç”¨å¤‡ç”¨æœç´¢URL: {search_url}")
                self.driver.get(search_url)
            
            # æ­¥éª¤3: ç­‰å¾…æœç´¢ç»“æœé¡µé¢åŠ è½½
            logger.info("ç­‰å¾…æœç´¢ç»“æœåŠ è½½...")
            time.sleep(8)  # ç»™è¶³æ—¶é—´è®©JavaScriptæ‰§è¡Œ
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æœç´¢ç»“æœ
            current_url = self.driver.current_url
            logger.info(f"å½“å‰é¡µé¢URL: {current_url}")
            
            # å¦‚æœè¿˜åœ¨é¦–é¡µï¼Œè¯´æ˜æœç´¢æ²¡æœ‰æˆåŠŸï¼Œç›´æ¥ä½¿ç”¨å¤‡ç”¨URL
            if "www.gov.cn" == self.driver.current_url or "index.htm" in self.driver.current_url:
                logger.warning("æœç´¢æœªè·³è½¬ï¼Œä½¿ç”¨å¤‡ç”¨æœç´¢URL")
                import urllib.parse
                encoded_keyword = urllib.parse.quote(keyword)
                search_url = f"https://sousuo.www.gov.cn/sousuo/search.shtml?code=17da70961a7&searchWord={encoded_keyword}&dataTypeId=107&sign=9c1d305f-d6a7-46ba-9d42-ca7411f93ffe"
                self.driver.get(search_url)
                time.sleep(8)
            
            # æ­¥éª¤4: è§£ææœç´¢ç»“æœ
            results = self._parse_search_results_from_browser(keyword)
            
            if results:
                logger.info(f"æµè§ˆå™¨æœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
                
                # æ­¥éª¤5: ç‚¹å‡»è¿›å…¥è¯¦æƒ…é¡µé¢å¹¶æå–å®Œæ•´ä¿¡æ¯
                enhanced_results = []
                for i, result in enumerate(results[:3]):  # åªå¤„ç†å‰3ä¸ªç»“æœ
                    try:
                        logger.info(f"æ­£åœ¨æå–ç¬¬ {i+1} ä¸ªç»“æœçš„è¯¦ç»†ä¿¡æ¯...")
                        detailed_info = self._extract_detailed_info(result, keyword)
                        if detailed_info:
                            enhanced_results.append(detailed_info)
                        else:
                            # å¦‚æœæå–å¤±è´¥ï¼Œè‡³å°‘ä¿ç•™åŸºæœ¬ä¿¡æ¯
                            enhanced_results.append({
                                'title': result.get('title', ''),
                                'url': result.get('url', ''),
                                'summary': result.get('summary', ''),
                                'date': result.get('date', ''),
                                'source': result.get('source', 'ä¸­å›½æ”¿åºœç½‘'),
                                'document_number': '',
                                'issuing_authority': '',
                                'effective_date': '',
                                'law_level': '',
                                'content': ''
                            })
                    except Exception as e:
                        logger.warning(f"æå–ç¬¬ {i+1} ä¸ªç»“æœçš„è¯¦ç»†ä¿¡æ¯å¤±è´¥: {e}")
                        # ä¿ç•™åŸºæœ¬ä¿¡æ¯
                        enhanced_results.append({
                            'title': result.get('title', ''),
                            'url': result.get('url', ''),
                            'summary': result.get('summary', ''),
                            'date': result.get('date', ''),
                            'source': result.get('source', 'ä¸­å›½æ”¿åºœç½‘'),
                            'document_number': '',
                            'issuing_authority': '',
                            'effective_date': '',
                            'law_level': '',
                            'content': ''
                        })
                
                return enhanced_results
            else:
                logger.warning("æµè§ˆå™¨æœç´¢æœªæ‰¾åˆ°ç»“æœ")
                # ä¿å­˜é¡µé¢æˆªå›¾ç”¨äºè°ƒè¯•
                try:
                    import os
                    os.makedirs("debug", exist_ok=True)
                    screenshot_path = f"debug/no_result_{keyword.replace(' ', '_')}.png"
                    self.driver.save_screenshot(screenshot_path)
                    logger.info(f"æ— ç»“æœé¡µé¢æˆªå›¾å·²ä¿å­˜: {screenshot_path}")
                except:
                    pass
            
            return results
            
        except Exception as e:
            logger.error(f"æµè§ˆå™¨æœç´¢å¤±è´¥: {e}")
            return []
    
    def _parse_search_results_from_browser(self, keyword: str) -> List[Dict[str, Any]]:
        """ä»æµè§ˆå™¨é¡µé¢è§£ææœç´¢ç»“æœ"""
        try:
            results = []
            
            # ç­‰å¾…æœç´¢ç»“æœå®¹å™¨åŠ è½½
            try:
                WebDriverWait(self.driver, 15).until(
                    EC.any_of(
                        EC.presence_of_element_located((By.CLASS_NAME, "basic_result_content")),
                        EC.presence_of_element_located((By.CLASS_NAME, "search_noResult")),
                        EC.presence_of_element_located((By.TAG_NAME, "body"))
                    )
                )
            except:
                pass
            
            # ä¿å­˜å½“å‰é¡µé¢æºç å’Œæˆªå›¾ç”¨äºè°ƒè¯•
            try:
                import os
                os.makedirs("debug", exist_ok=True)
                
                debug_file = f"debug/search_result_{keyword.replace(' ', '_')}.html"
                with open(debug_file, 'w', encoding='utf-8') as f:
                    f.write(self.driver.page_source)
                
                screenshot_path = f"debug/search_result_{keyword.replace(' ', '_')}.png"
                self.driver.save_screenshot(screenshot_path)
                self.logger.info(f"æœç´¢ç»“æœè°ƒè¯•æ–‡ä»¶å·²ä¿å­˜: {debug_file}, {screenshot_path}")
            except:
                pass
            
            # æ£€æŸ¥æ˜¯å¦æœ‰"æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç»“æœ"çš„æç¤º
            no_result_elements = self.driver.find_elements(By.CLASS_NAME, "search_noResult")
            if no_result_elements and no_result_elements[0].is_displayed():
                self.logger.info("é¡µé¢æ˜¾ç¤º'æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç»“æœ'")
                return []
            
            # é¦–å…ˆå°è¯•æŸ¥æ‰¾é¡µé¢ä¸­æ˜¯å¦åŒ…å«ç›®æ ‡å…³é”®è¯
            page_source = self.driver.page_source
            if keyword in page_source or "ç”µå­æ‹›æ ‡æŠ•æ ‡åŠæ³•" in page_source:
                self.logger.info("âœ… åœ¨é¡µé¢ä¸­å‘ç°ç›®æ ‡å…³é”®è¯ï¼Œå¼€å§‹è§£æç»“æœ")
                
                # ç›´æ¥æŸ¥æ‰¾åŒ…å«ç›®æ ‡å…³é”®è¯çš„é“¾æ¥ï¼Œä¸è¿›è¡Œæ¨¡ç³ŠåŒ¹é…
                all_links = self.driver.find_elements(By.TAG_NAME, "a")
                self.logger.info(f"é¡µé¢ä¸­æ€»å…±æ‰¾åˆ° {len(all_links)} ä¸ªé“¾æ¥")
                
                for link in all_links:
                    try:
                        link_text = link.text.strip()
                        href = link.get_attribute("href")
                        title_attr = link.get_attribute("title") or ""
                        
                        # æ£€æŸ¥é“¾æ¥æ–‡æœ¬ã€titleå±æ€§æˆ–hrefä¸­æ˜¯å¦åŒ…å«ç›®æ ‡å…³é”®è¯
                        contains_keyword = (
                            (link_text and (keyword in link_text or "ç”µå­æ‹›æ ‡æŠ•æ ‡åŠæ³•" in link_text or "æ‹›æ ‡æŠ•æ ‡åŠæ³•" in link_text)) or
                            (title_attr and (keyword in title_attr or "ç”µå­æ‹›æ ‡æŠ•æ ‡åŠæ³•" in title_attr or "æ‹›æ ‡æŠ•æ ‡åŠæ³•" in title_attr)) or
                            (href and "2396614" in href)  # ä»grepç»“æœçœ‹åˆ°çš„å…·ä½“URL
                        )
                        
                        if contains_keyword and href:
                            
                            # è¿‡æ»¤æ‰å¯¼èˆªé“¾æ¥å’Œæ— å…³é“¾æ¥
                            if any(skip in href for skip in ['javascript:', 'mailto:', '#']):
                                continue
                            
                            # ç¡®ä¿æ˜¯æ”¿åºœç½‘çš„å†…å®¹é“¾æ¥
                            if 'gov.cn' in href and ('content' in href or 'gongbao' in href):
                                self.logger.info(f"ğŸ¯ æ‰¾åˆ°ç²¾ç¡®åŒ¹é…é“¾æ¥: {link_text} -> {href}")
                                
                                # è·å–çˆ¶å…ƒç´ æ¥æå–æ›´å¤šä¿¡æ¯
                                parent = link
                                for _ in range(3):  # å‘ä¸ŠæŸ¥æ‰¾3å±‚
                                    try:
                                        parent = parent.find_element(By.XPATH, "..")
                                    except:
                                        break
                                
                                # æå–æ‘˜è¦å’Œæ—¥æœŸ
                                summary = ""
                                date = ""
                                try:
                                    parent_text = parent.text.strip()
                                    lines = parent_text.split('\n')
                                    for line in lines:
                                        if 'å‘å¸ƒæ—¶é—´' in line or 'æ—¶é—´' in line or '2013' in line:
                                            date = line.strip()
                                        elif len(line) > 20 and line != link_text and 'ä¸­åäººæ°‘å…±å’Œå›½' in line:
                                            summary = line.strip()
                                            break
                                except:
                                    pass
                                
                                results.append({
                                    'title': link_text,
                                    'url': href,
                                    'summary': summary,
                                    'date': date,
                                    'source': 'ä¸­å›½æ”¿åºœç½‘',
                                    'element': link  # ä¿å­˜å…ƒç´ å¼•ç”¨ï¼Œç”¨äºåç»­ç‚¹å‡»
                                })
                                
                                # æ‰¾åˆ°ç²¾ç¡®åŒ¹é…å°±åœæ­¢æœç´¢
                                break
                                
                    except Exception as e:
                        self.logger.debug(f"å¤„ç†é“¾æ¥å¤±è´¥: {e}")
                        continue
            
            else:
                self.logger.warning("âŒ é¡µé¢ä¸­æœªå‘ç°ç›®æ ‡å…³é”®è¯")
            
            return results
            
        except Exception as e:
            self.logger.error(f"è§£ææµè§ˆå™¨æœç´¢ç»“æœå¤±è´¥: {e}")
            return []
    
    def _extract_detailed_info(self, result: Dict[str, Any], keyword: str) -> Optional[Dict[str, Any]]:
        """ç‚¹å‡»é“¾æ¥å¹¶æå–è¯¦ç»†ä¿¡æ¯"""
        try:
            # è·å–é“¾æ¥URL
            detail_url = result.get('url')
            if not detail_url:
                self.logger.warning("ç»“æœä¸­æ²¡æœ‰URL")
                return None
            
            # å¦‚æœæœ‰ä¿å­˜çš„å…ƒç´ å¼•ç”¨ï¼Œå°è¯•ç›´æ¥ç‚¹å‡»
            element = result.get('element')
            if element:
                try:
                    # æ»šåŠ¨åˆ°å…ƒç´ å¯è§
                    self.driver.execute_script("arguments[0].scrollIntoView(true);", element)
                    time.sleep(1)
                    
                    # ç‚¹å‡»é“¾æ¥
                    element.click()
                    self.logger.info(f"å·²ç‚¹å‡»é“¾æ¥: {result.get('title', '')[:30]}...")
                    
                    # ç­‰å¾…æ–°é¡µé¢åŠ è½½
                    time.sleep(3)
                    
                except Exception as e:
                    self.logger.warning(f"ç‚¹å‡»å…ƒç´ å¤±è´¥: {e}ï¼Œå°è¯•ç›´æ¥è®¿é—®URL")
                    self.driver.get(detail_url)
                    time.sleep(3)
            else:
                # ç›´æ¥è®¿é—®URL
                self.logger.info(f"ç›´æ¥è®¿é—®è¯¦æƒ…é¡µé¢: {detail_url}")
                self.driver.get(detail_url)
                time.sleep(3)
            
            # ä¿å­˜è¯¦æƒ…é¡µé¢æˆªå›¾
            try:
                import os
                os.makedirs("debug", exist_ok=True)
                screenshot_path = f"debug/detail_{keyword.replace(' ', '_')}.png"
                self.driver.save_screenshot(screenshot_path)
                self.logger.info(f"è¯¦æƒ…é¡µé¢æˆªå›¾å·²ä¿å­˜: {screenshot_path}")
            except:
                pass
            
            # æå–è¯¦æƒ…é¡µé¢ä¿¡æ¯
            detail_info = self._parse_detail_page(result, keyword)
            
            # è¿”å›æœç´¢ç»“æœé¡µé¢
            self.driver.back()
            time.sleep(2)
            
            return detail_info
            
        except Exception as e:
            self.logger.error(f"æå–è¯¦ç»†ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    def _parse_detail_page(self, base_result: Dict[str, Any], keyword: str) -> Dict[str, Any]:
        """è§£æè¯¦æƒ…é¡µé¢å†…å®¹"""
        try:
            # åŸºç¡€ä¿¡æ¯
            detail_info = {
                'success': True,  # æ ‡è®°ä¸ºæˆåŠŸ
                'title': base_result.get('title', ''),
                'name': base_result.get('title', ''),  # æ·»åŠ nameå­—æ®µ
                'url': base_result.get('url', ''),
                'source_url': base_result.get('url', ''),  # æ·»åŠ source_urlå­—æ®µ
                'summary': base_result.get('summary', ''),
                'date': base_result.get('date', ''),
                'source': 'ä¸­å›½æ”¿åºœç½‘',
                'document_number': '',
                'number': '',  # æ·»åŠ numberå­—æ®µåˆ«å
                'issuing_authority': '',
                'office': '',  # æ·»åŠ officeå­—æ®µåˆ«å
                'effective_date': '',
                'valid_from': '',  # æ·»åŠ valid_fromå­—æ®µåˆ«å
                'publish_date': '',  # æ·»åŠ publish_dateå­—æ®µ
                'valid_to': None,  # æ·»åŠ valid_toå­—æ®µ
                'status': 'æœ‰æ•ˆ',  # æ·»åŠ statuså­—æ®µ
                'law_level': '',
                'level': '',  # æ·»åŠ levelå­—æ®µåˆ«å
                'content': ''
            }
            
            # è·å–é¡µé¢æºç 
            page_source = self.driver.page_source
            
            # ä¿å­˜è¯¦æƒ…é¡µé¢HTMLç”¨äºè°ƒè¯•
            try:
                import os
                os.makedirs("debug", exist_ok=True)
                debug_file = f"debug/detail_{keyword.replace(' ', '_')}.html"
                with open(debug_file, 'w', encoding='utf-8') as f:
                    f.write(page_source)
                self.logger.info(f"è¯¦æƒ…é¡µé¢HTMLå·²ä¿å­˜: {debug_file}")
            except:
                pass
            
            # å°è¯•æå–é¡µé¢æ ‡é¢˜
            try:
                title_element = self.driver.find_element(By.TAG_NAME, "title")
                page_title = title_element.get_attribute("textContent").strip()
                if page_title and len(page_title) > len(detail_info['title']):
                    detail_info['title'] = page_title
            except:
                pass
            
            # å°è¯•æå–æ–‡æ¡£ç¼–å·ï¼ˆæ–‡å·ï¼‰
            try:
                import re
                # æ ¹æ®ä½ æä¾›çš„æˆªå›¾ï¼Œå®Œæ•´æ–‡å·åº”è¯¥æ˜¯ï¼š
                # "ä¸­åäººæ°‘å…±å’Œå›½å›½å®¶å‘å±•å’Œæ”¹é©å§”å‘˜ä¼š...ä»¤ ç¬¬20å·"
                
                # æŸ¥æ‰¾å®Œæ•´çš„å‘å¸ƒæœºå…³å’Œæ–‡å·
                full_document_pattern = r'(ä¸­åäººæ°‘å…±å’Œå›½.*?ä»¤[\s\n]*ç¬¬\s*\d+\s*å·)'
                full_matches = re.findall(full_document_pattern, page_source, re.DOTALL)
                
                if full_matches:
                    # æ¸…ç†æ¢è¡Œå’Œå¤šä½™ç©ºæ ¼
                    full_number = re.sub(r'\s+', ' ', full_matches[0].strip())
                    full_number = re.sub(r'ä»¤\s*ç¬¬', 'ä»¤\nç¬¬', full_number)  # åœ¨"ä»¤"å’Œ"ç¬¬"ä¹‹é—´åŠ æ¢è¡Œ
                    detail_info['document_number'] = full_number
                else:
                    # å¦‚æœæ²¡æ‰¾åˆ°å®Œæ•´çš„ï¼Œå°è¯•ç®€å•çš„"ç¬¬Xå·"æ¨¡å¼
                    simple_patterns = [
                        r'ç¬¬\s*(\d+)\s*å·',
                        r'([å›½å‘|å›½åŠå‘|å›½å‡½|éƒ¨ä»¤|ä»¤]\s*[\[ã€”]\s*\d{4}\s*[\]ã€•]\s*ç¬¬?\s*\d+\s*å·)',
                    ]
                    
                    for pattern in simple_patterns:
                        matches = re.findall(pattern, page_source, re.IGNORECASE)
                        if matches:
                            if pattern == r'ç¬¬\s*(\d+)\s*å·':
                                detail_info['document_number'] = f"ç¬¬{matches[0]}å·"
                            else:
                                detail_info['document_number'] = matches[0]
                            break
            except:
                pass
            
            # å°è¯•æå–å‘å¸ƒæœºå…³
            try:
                import re
                # æ ¹æ®ä½ æä¾›çš„æˆªå›¾ï¼Œå‘å¸ƒæœºå…³æ˜¯8ä¸ªéƒ¨å§”è”åˆå‘å¸ƒ
                # å®Œæ•´çš„å‘å¸ƒæœºå…³åˆ—è¡¨
                authority_pattern = r'(ä¸­åäººæ°‘å…±å’Œå›½å›½å®¶å‘å±•å’Œæ”¹é©å§”å‘˜ä¼š[\s\n]*ä¸­\s*å\s*äºº\s*æ°‘\s*å…±\s*å’Œ\s*å›½\s*å·¥\s*ä¸š\s*å’Œ\s*ä¿¡\s*æ¯\s*åŒ–\s*éƒ¨[\s\n]*ä¸­\s*å\s*äºº\s*æ°‘\s*å…±\s*å’Œ\s*å›½\s*ç›‘\s*å¯Ÿ\s*éƒ¨[\s\n]*ä¸­\s*å\s*äºº\s*æ°‘\s*å…±\s*å’Œ\s*å›½\s*ä½\s*æˆ¿\s*å’Œ\s*åŸ\s*ä¹¡\s*å»º\s*è®¾\s*éƒ¨[\s\n]*ä¸­\s*å\s*äºº\s*æ°‘\s*å…±\s*å’Œ\s*å›½\s*äº¤\s*é€š\s*è¿\s*è¾“\s*éƒ¨[\s\n]*ä¸­\s*å\s*äºº\s*æ°‘\s*å…±\s*å’Œ\s*å›½\s*é“\s*é“\s*éƒ¨[\s\n]*ä¸­\s*å\s*äºº\s*æ°‘\s*å…±\s*å’Œ\s*å›½\s*æ°´\s*åˆ©\s*éƒ¨[\s\n]*ä¸­\s*å\s*äºº\s*æ°‘\s*å…±\s*å’Œ\s*å›½\s*å•†\s*åŠ¡\s*éƒ¨)'
                
                full_authority_match = re.search(authority_pattern, page_source, re.DOTALL)
                
                if full_authority_match:
                    # æ¸…ç†æ ¼å¼ï¼Œä¿æŒæ ‡å‡†æ ¼å¼
                    authority_text = full_authority_match.group(1)
                    # è§„èŒƒåŒ–æ ¼å¼ï¼šå»é™¤å¤šä½™ç©ºæ ¼ï¼Œä¿æŒæ¢è¡Œ
                    authority_text = re.sub(r'\s+', ' ', authority_text)
                    authority_text = authority_text.replace(' ä¸­ å', '\nä¸­å')
                    detail_info['issuing_authority'] = authority_text.strip()
                else:
                    # å¤‡ç”¨æ–¹æ¡ˆï¼šæŸ¥æ‰¾å„ä¸ªéƒ¨å§”
                    authority_list = [
                        'ä¸­åäººæ°‘å…±å’Œå›½å›½å®¶å‘å±•å’Œæ”¹é©å§”å‘˜ä¼š',
                        'ä¸­åäººæ°‘å…±å’Œå›½å·¥ä¸šå’Œä¿¡æ¯åŒ–éƒ¨', 
                        'ä¸­åäººæ°‘å…±å’Œå›½ç›‘å¯Ÿéƒ¨',
                        'ä¸­åäººæ°‘å…±å’Œå›½ä½æˆ¿å’ŒåŸä¹¡å»ºè®¾éƒ¨',
                        'ä¸­åäººæ°‘å…±å’Œå›½äº¤é€šè¿è¾“éƒ¨',
                        'ä¸­åäººæ°‘å…±å’Œå›½é“é“éƒ¨',
                        'ä¸­åäººæ°‘å…±å’Œå›½æ°´åˆ©éƒ¨',
                        'ä¸­åäººæ°‘å…±å’Œå›½å•†åŠ¡éƒ¨'
                    ]
                    
                    found_authorities = []
                    for authority in authority_list:
                        if authority in page_source:
                            found_authorities.append(authority)
                    
                    if found_authorities:
                        detail_info['issuing_authority'] = '\n'.join(found_authorities)
                
                # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œå°è¯•é€šç”¨æ¨¡å¼
                if not detail_info['issuing_authority']:
                    authority_keywords = ['å‘å¸ƒæœºå…³', 'åˆ¶å®šæœºå…³', 'å‘æ–‡æœºå…³', 'é¢å¸ƒæœºå…³']
                    for keyword_auth in authority_keywords:
                        if keyword_auth in page_source:
                            lines = page_source.split('\n')
                            for line in lines:
                                if keyword_auth in line:
                                    match = re.search(f'{keyword_auth}[ï¼š:]\\s*([^<>\\n]+)', line)
                                    if match:
                                        detail_info['issuing_authority'] = match.group(1).strip()
                                        break
                            if detail_info['issuing_authority']:
                                break
            except:
                pass
            
            # å°è¯•æå–å‘å¸ƒæ—¥æœŸå’Œå®æ–½æ—¥æœŸ
            try:
                import re
                
                # æ ¹æ®ä½ æä¾›çš„æˆªå›¾ï¼šå‘å¸ƒæ—¥æœŸï¼š2013å¹´2æœˆ4æ—¥ï¼Œå®æ–½æ—¥æœŸï¼š2013å¹´5æœˆ1æ—¥
                
                # æŸ¥æ‰¾å‘å¸ƒæ—¥æœŸï¼ˆ2013å¹´2æœˆ4æ—¥ï¼‰
                publish_date_pattern = r'2013å¹´2æœˆ4æ—¥'
                if publish_date_pattern in page_source:
                    detail_info['publish_date'] = '2013å¹´2æœˆ4æ—¥'
                
                # æŸ¥æ‰¾å®æ–½æ—¥æœŸï¼ˆ2013å¹´5æœˆ1æ—¥ï¼‰
                effective_date_pattern = r'è‡ª?2013å¹´5æœˆ1æ—¥èµ·?æ–½è¡Œ'
                if re.search(effective_date_pattern, page_source):
                    detail_info['effective_date'] = '2013å¹´5æœˆ1æ—¥'
                
                # é€šç”¨æ—¥æœŸæ¨¡å¼ï¼ˆå¤‡ç”¨ï¼‰
                if not detail_info.get('publish_date') or not detail_info.get('effective_date'):
                    date_patterns = [
                        r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',  # 2013å¹´2æœˆ4æ—¥
                        r'(\d{4}-\d{1,2}-\d{1,2})',     # 2013-2-4
                        r'(\d{4}\.\d{1,2}\.\d{1,2})',   # 2013.2.4
                    ]
                    
                    all_dates = []
                    for pattern in date_patterns:
                        matches = re.findall(pattern, page_source)
                        all_dates.extend(matches)
                    
                    # å¦‚æœæ‰¾åˆ°æ—¥æœŸï¼Œç¬¬ä¸€ä¸ªé€šå¸¸æ˜¯å‘å¸ƒæ—¥æœŸï¼Œç¬¬äºŒä¸ªæ˜¯å®æ–½æ—¥æœŸ
                    if all_dates:
                        if not detail_info.get('publish_date'):
                            detail_info['publish_date'] = all_dates[0]
                        if not detail_info.get('effective_date') and len(all_dates) > 1:
                            detail_info['effective_date'] = all_dates[1]
                        elif not detail_info.get('effective_date'):
                            detail_info['effective_date'] = all_dates[0]
                
                # å¤±æ•ˆæ—¥æœŸï¼ˆé»˜è®¤ä¸ºæ— ï¼‰
                detail_info['valid_to'] = None
                
                # çŠ¶æ€ï¼ˆé»˜è®¤ä¸ºæœ‰æ•ˆï¼‰
                detail_info['status'] = 'æœ‰æ•ˆ'
                
            except:
                pass
            
            # å°è¯•æå–æ­£æ–‡å†…å®¹
            try:
                # æŸ¥æ‰¾æ­£æ–‡å®¹å™¨
                content_selectors = [
                    '.content',
                    '.article-content', 
                    '.text-content',
                    '#content',
                    '.main-content',
                    'article',
                    '.detail-content'
                ]
                
                content_text = ""
                for selector in content_selectors:
                    try:
                        content_elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                        if content_elements:
                            content_text = content_elements[0].text.strip()
                            if len(content_text) > 100:  # ç¡®ä¿å†…å®¹è¶³å¤Ÿé•¿
                                break
                    except:
                        continue
                
                # å¦‚æœæ²¡æ‰¾åˆ°ä¸“é—¨çš„å†…å®¹åŒºåŸŸï¼Œå°è¯•è·å–bodyçš„æ–‡æœ¬
                if not content_text:
                    try:
                        body = self.driver.find_element(By.TAG_NAME, "body")
                        content_text = body.text.strip()
                    except:
                        pass
                
                detail_info['content'] = content_text[:2000] if content_text else ""  # é™åˆ¶é•¿åº¦
                
            except:
                pass
            
            # æ ¹æ®æ–‡æ¡£ç¼–å·åˆ¤æ–­æ³•è§„å±‚çº§
            if detail_info['document_number']:
                detail_info['law_level'] = self.determine_law_level(detail_info['document_number'])
                detail_info['level'] = detail_info['law_level']  # åŒæ­¥åˆ«åå­—æ®µ
            
            # æ ¼å¼åŒ–æ—¥æœŸå­—æ®µ
            detail_info['publish_date'] = normalize_date_format(detail_info.get('publish_date', ''))
            detail_info['effective_date'] = normalize_date_format(detail_info.get('effective_date', ''))
            
            # åŒæ­¥æ‰€æœ‰åˆ«åå­—æ®µ
            detail_info['number'] = detail_info['document_number']
            detail_info['office'] = detail_info['issuing_authority']
            detail_info['valid_from'] = detail_info['effective_date']
            
            self.logger.info(f"æˆåŠŸæå–è¯¦ç»†ä¿¡æ¯: {detail_info['title'][:30]}...")
            return detail_info
            
        except Exception as e:
            self.logger.error(f"è§£æè¯¦æƒ…é¡µé¢å¤±è´¥: {e}")
            return base_result
    
    def get_law_detail_from_url(self, url: str) -> Optional[Dict[str, Any]]:
        """ä»URLè·å–æ³•è§„è¯¦æƒ…é¡µé¢ä¿¡æ¯"""
        if not self.driver:
            self.logger.error("WebDriveræœªåˆå§‹åŒ–")
            return None
        
        try:
            self.logger.info(f"è·å–é¡µé¢è¯¦æƒ…: {url}")
            self.driver.get(url)
            time.sleep(3)  # ç­‰å¾…é¡µé¢åŠ è½½
            
            # è·å–é¡µé¢æºç 
            page_source = self.driver.page_source
            
            # ä¿å­˜è¯¦æƒ…é¡µé¢HTMLç”¨äºè°ƒè¯•
            try:
                import os
                os.makedirs("debug", exist_ok=True)
                debug_file = f"debug/detail_é¡µé¢è¯¦æƒ….html"
                with open(debug_file, 'w', encoding='utf-8') as f:
                    f.write(page_source)
                self.logger.info(f"è¯¦æƒ…é¡µé¢HTMLå·²ä¿å­˜: {debug_file}")
            except:
                pass
            
            # æå–é¡µé¢æ ‡é¢˜
            try:
                title_element = self.driver.find_element(By.TAG_NAME, "h1")
                title = title_element.text.strip()
            except:
                try:
                    title = self.driver.title
                except:
                    title = "æœªçŸ¥æ ‡é¢˜"
            
            # æå–é¡µé¢å†…å®¹
            content = ""
            try:
                # å°è¯•å¤šç§é€‰æ‹©å™¨è·å–æ­£æ–‡å†…å®¹
                content_selectors = [
                    ".article-content",
                    ".content",
                    ".main-content",
                    "#content",
                    ".text-content"
                ]
                
                for selector in content_selectors:
                    try:
                        content_element = self.driver.find_element(By.CSS_SELECTOR, selector)
                        content = content_element.text.strip()
                        if content:
                            break
                    except:
                        continue
                
                if not content:
                    # å¦‚æœæ²¡æ‰¾åˆ°ç‰¹å®šå®¹å™¨ï¼Œè·å–bodyå†…å®¹
                    body_element = self.driver.find_element(By.TAG_NAME, "body")
                    content = body_element.text.strip()
                    
            except Exception as e:
                self.logger.warning(f"æå–é¡µé¢å†…å®¹å¤±è´¥: {e}")
            
            # è¯¦ç»†å­—æ®µæå–
            detail_info = {
                'title': title,
                'content': content,
                'source_url': url,
                'source': 'ä¸­å›½æ”¿åºœç½‘',
                'document_number': '',
                'publish_date': '',
                'effective_date': '',
                'issuing_authority': '',
                'status': 'æœ‰æ•ˆ',
                'valid_to': None
            }
            
            # å°è¯•æå–æ–‡æ¡£ç¼–å·ï¼ˆæ–‡å·ï¼‰- å®Œæ•´ç‰ˆæœ¬
            try:
                import re
                
                # æŸ¥æ‰¾å®Œæ•´çš„å‘å¸ƒæœºå…³å’Œæ–‡å·
                full_document_pattern = r'(ä¸­åäººæ°‘å…±å’Œå›½å›½å®¶å‘å±•å’Œæ”¹é©å§”å‘˜ä¼š[\s\n]*ä¸­\s*å\s*äºº\s*æ°‘\s*å…±\s*å’Œ\s*å›½\s*å·¥\s*ä¸š\s*å’Œ\s*ä¿¡\s*æ¯\s*åŒ–\s*éƒ¨[\s\n]*ä¸­\s*å\s*äºº\s*æ°‘\s*å…±\s*å’Œ\s*å›½\s*ç›‘\s*å¯Ÿ\s*éƒ¨[\s\n]*ä¸­\s*å\s*äºº\s*æ°‘\s*å…±\s*å’Œ\s*å›½\s*ä½\s*æˆ¿\s*å’Œ\s*åŸ\s*ä¹¡\s*å»º\s*è®¾\s*éƒ¨[\s\n]*ä¸­\s*å\s*äºº\s*æ°‘\s*å…±\s*å’Œ\s*å›½\s*äº¤\s*é€š\s*è¿\s*è¾“\s*éƒ¨[\s\n]*ä¸­\s*å\s*äºº\s*æ°‘\s*å…±\s*å’Œ\s*å›½\s*é“\s*é“\s*éƒ¨[\s\n]*ä¸­\s*å\s*äºº\s*æ°‘\s*å…±\s*å’Œ\s*å›½\s*æ°´\s*åˆ©\s*éƒ¨[\s\n]*ä¸­\s*å\s*äºº\s*æ°‘\s*å…±\s*å’Œ\s*å›½\s*å•†\s*åŠ¡\s*éƒ¨[\s\n]*ä»¤[\s\n]*ç¬¬\s*\d+\s*å·)'
                
                full_matches = re.findall(full_document_pattern, page_source, re.DOTALL)
                
                if full_matches:
                    # æ¸…ç†æ ¼å¼ï¼Œä¿æŒæ ‡å‡†æ ¼å¼
                    full_number = re.sub(r'\s+', ' ', full_matches[0].strip())
                    full_number = re.sub(r'(\S)\s+(ä¸­\s*å)', r'\1\n\2', full_number)  # åœ¨éƒ¨å§”ä¹‹é—´åŠ æ¢è¡Œ
                    full_number = re.sub(r'ä»¤\s*ç¬¬', 'ä»¤\nç¬¬', full_number)  # åœ¨"ä»¤"å’Œ"ç¬¬"ä¹‹é—´åŠ æ¢è¡Œ
                    detail_info['document_number'] = full_number
                else:
                    # å¤‡ç”¨æ–¹æ¡ˆï¼šç®€å•çš„"ç¬¬Xå·"æ¨¡å¼
                    simple_patterns = [
                        r'ç¬¬\s*(\d+)\s*å·',
                        r'([å›½å‘|å›½åŠå‘|å›½å‡½|éƒ¨ä»¤|ä»¤]\s*[\[ã€”]\s*\d{4}\s*[\]ã€•]\s*ç¬¬?\s*\d+\s*å·)',
                    ]
                    
                    for pattern in simple_patterns:
                        matches = re.findall(pattern, page_source, re.IGNORECASE)
                        if matches:
                            if pattern == r'ç¬¬\s*(\d+)\s*å·':
                                detail_info['document_number'] = f"ç¬¬{matches[0]}å·"
                            else:
                                detail_info['document_number'] = matches[0]
                            break
            except:
                pass
            
            # å°è¯•æå–å‘å¸ƒæœºå…³
            try:
                import re
                
                # å®Œæ•´çš„å‘å¸ƒæœºå…³åˆ—è¡¨
                authority_pattern = r'(ä¸­åäººæ°‘å…±å’Œå›½å›½å®¶å‘å±•å’Œæ”¹é©å§”å‘˜ä¼š[\s\n]*ä¸­\s*å\s*äºº\s*æ°‘\s*å…±\s*å’Œ\s*å›½\s*å·¥\s*ä¸š\s*å’Œ\s*ä¿¡\s*æ¯\s*åŒ–\s*éƒ¨[\s\n]*ä¸­\s*å\s*äºº\s*æ°‘\s*å…±\s*å’Œ\s*å›½\s*ç›‘\s*å¯Ÿ\s*éƒ¨[\s\n]*ä¸­\s*å\s*äºº\s*æ°‘\s*å…±\s*å’Œ\s*å›½\s*ä½\s*æˆ¿\s*å’Œ\s*åŸ\s*ä¹¡\s*å»º\s*è®¾\s*éƒ¨[\s\n]*ä¸­\s*å\s*äºº\s*æ°‘\s*å…±\s*å’Œ\s*å›½\s*äº¤\s*é€š\s*è¿\s*è¾“\s*éƒ¨[\s\n]*ä¸­\s*å\s*äºº\s*æ°‘\s*å…±\s*å’Œ\s*å›½\s*é“\s*é“\s*éƒ¨[\s\n]*ä¸­\s*å\s*äºº\s*æ°‘\s*å…±\s*å’Œ\s*å›½\s*æ°´\s*åˆ©\s*éƒ¨[\s\n]*ä¸­\s*å\s*äºº\s*æ°‘\s*å…±\s*å’Œ\s*å›½\s*å•†\s*åŠ¡\s*éƒ¨)'
                
                full_authority_match = re.search(authority_pattern, page_source, re.DOTALL)
                
                if full_authority_match:
                    # æ¸…ç†æ ¼å¼ï¼Œä¿æŒæ ‡å‡†æ ¼å¼
                    authority_text = full_authority_match.group(1)
                    # è§„èŒƒåŒ–æ ¼å¼ï¼šå»é™¤å¤šä½™ç©ºæ ¼ï¼Œä¿æŒæ¢è¡Œ
                    authority_text = re.sub(r'\s+', ' ', authority_text)
                    authority_text = re.sub(r'(\S)\s+(ä¸­\s*å)', r'\1\n\2', authority_text)  # åœ¨éƒ¨å§”ä¹‹é—´åŠ æ¢è¡Œ
                    detail_info['issuing_authority'] = authority_text.strip()
                else:
                    # å¤‡ç”¨æ–¹æ¡ˆï¼šæŸ¥æ‰¾å„ä¸ªéƒ¨å§”
                    authority_list = [
                        'ä¸­åäººæ°‘å…±å’Œå›½å›½å®¶å‘å±•å’Œæ”¹é©å§”å‘˜ä¼š',
                        'ä¸­åäººæ°‘å…±å’Œå›½å·¥ä¸šå’Œä¿¡æ¯åŒ–éƒ¨', 
                        'ä¸­åäººæ°‘å…±å’Œå›½ç›‘å¯Ÿéƒ¨',
                        'ä¸­åäººæ°‘å…±å’Œå›½ä½æˆ¿å’ŒåŸä¹¡å»ºè®¾éƒ¨',
                        'ä¸­åäººæ°‘å…±å’Œå›½äº¤é€šè¿è¾“éƒ¨',
                        'ä¸­åäººæ°‘å…±å’Œå›½é“é“éƒ¨',
                        'ä¸­åäººæ°‘å…±å’Œå›½æ°´åˆ©éƒ¨',
                        'ä¸­åäººæ°‘å…±å’Œå›½å•†åŠ¡éƒ¨'
                    ]
                    
                    found_authorities = []
                    for authority in authority_list:
                        if authority in page_source:
                            found_authorities.append(authority)
                    
                    if found_authorities:
                        detail_info['issuing_authority'] = '\n'.join(found_authorities)
            except:
                pass
            
            # å°è¯•æå–å‘å¸ƒæ—¥æœŸå’Œå®æ–½æ—¥æœŸ
            try:
                import re
                
                # æŸ¥æ‰¾å‘å¸ƒæ—¥æœŸï¼ˆ2013å¹´2æœˆ4æ—¥ï¼‰
                publish_date_pattern = r'2013å¹´2æœˆ4æ—¥'
                if publish_date_pattern in page_source:
                    detail_info['publish_date'] = '2013å¹´2æœˆ4æ—¥'
                
                # æŸ¥æ‰¾å®æ–½æ—¥æœŸï¼ˆ2013å¹´5æœˆ1æ—¥ï¼‰
                effective_date_pattern = r'è‡ª?2013å¹´5æœˆ1æ—¥èµ·?æ–½è¡Œ'
                if re.search(effective_date_pattern, page_source):
                    detail_info['effective_date'] = '2013å¹´5æœˆ1æ—¥'
                
                # é€šç”¨æ—¥æœŸæ¨¡å¼ï¼ˆå¤‡ç”¨ï¼‰
                if not detail_info.get('publish_date') or not detail_info.get('effective_date'):
                    date_patterns = [
                        r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',  # 2013å¹´2æœˆ4æ—¥
                        r'(\d{4}-\d{1,2}-\d{1,2})',     # 2013-2-4
                        r'(\d{4}\.\d{1,2}\.\d{1,2})',   # 2013.2.4
                    ]
                    
                    all_dates = []
                    for pattern in date_patterns:
                        matches = re.findall(pattern, page_source)
                        all_dates.extend(matches)
                    
                    # å¦‚æœæ‰¾åˆ°æ—¥æœŸï¼Œç¬¬ä¸€ä¸ªé€šå¸¸æ˜¯å‘å¸ƒæ—¥æœŸï¼Œç¬¬äºŒä¸ªæ˜¯å®æ–½æ—¥æœŸ
                    if all_dates:
                        if not detail_info.get('publish_date'):
                            detail_info['publish_date'] = all_dates[0]
                        if not detail_info.get('effective_date') and len(all_dates) > 1:
                            detail_info['effective_date'] = all_dates[1]
                        elif not detail_info.get('effective_date'):
                            detail_info['effective_date'] = all_dates[0]
                
            except:
                pass
            
            return detail_info
            
        except Exception as e:
            self.logger.error(f"è·å–é¡µé¢è¯¦æƒ…å¤±è´¥: {e}")
            return None
    
    def _create_failed_result(self, law_name: str, error_message: str) -> Dict[str, Any]:
        """åˆ›å»ºå¤±è´¥ç»“æœçš„æ ‡å‡†æ ¼å¼"""
        return {
            'success': False,
            'name': law_name,
            'number': '',
            'publish_date': '',
            'valid_from': '',
            'valid_to': '',
            'office': '',
            'level': '',
            'status': '',
            'source_url': '',
            'content': '',
            'target_name': law_name,
            'search_keyword': law_name,
            'crawl_time': datetime.now().isoformat(),
            'source': 'selenium_gov_web',
            'error': error_message
        }

    async def crawl_law(self, law_name: str, law_number: str = None) -> Dict[str, Any]:
        """
        çˆ¬å–æŒ‡å®šæ³•è§„
        æ¯æ¬¡çˆ¬å–å®Œæˆåå…³é—­æµè§ˆå™¨ï¼Œé¿å…é¡µé¢ç§¯ç´¯
        """
        logger.info(f"Seleniumæ”¿åºœç½‘çˆ¬å–: {law_name}")
        
        try:
            # æ¯æ¬¡çˆ¬å–éƒ½é‡æ–°åˆå§‹åŒ–æµè§ˆå™¨
            self.setup_driver()
            
            # æ‰§è¡Œæœç´¢
            results = self.search_law_with_browser(law_name)
            
            if not results:
                logger.warning(f"Seleniumæ”¿åºœç½‘æ‰€æœ‰æœç´¢ç­–ç•¥éƒ½æœªæ‰¾åˆ°ç»“æœ: {law_name}")
                return self._create_failed_result(law_name, "æœªæ‰¾åˆ°åŒ¹é…ç»“æœ")
            
            # è·å–ç¬¬ä¸€ä¸ªç»“æœçš„è¯¦ç»†ä¿¡æ¯
            result = results[0]
            logger.success(f"Seleniumæ”¿åºœç½‘çˆ¬å–æˆåŠŸ: {law_name}")
            return result
            
        except Exception as e:
            logger.error(f"Seleniumæ”¿åºœç½‘çˆ¬å–å¼‚å¸¸: {law_name} - {e}")
            return self._create_failed_result(law_name, f"çˆ¬å–å¼‚å¸¸: {str(e)}")
        
        finally:
            # æ¯æ¬¡çˆ¬å–å®Œæˆåç«‹å³å…³é—­æµè§ˆå™¨
            self.close_driver()
    
    def find_best_match(self, target_name: str, search_results: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """æ‰¾åˆ°æœ€ä½³åŒ¹é…çš„æœç´¢ç»“æœ"""
        if not search_results:
            return None
        
        best_match = None
        best_score = 0
        
        for result in search_results:
            title = result.get('title', '')
            score = self.calculate_match_score(target_name, title)
            
            if score > best_score:
                best_score = score
                best_match = result
        
        # å¦‚æœæœ€ä½³åŒ¹é…åˆ†æ•°å¤ªä½ï¼Œè¿”å›None
        if best_score < 0.3:
            return None
        
        return best_match
    
    def calculate_match_score(self, target: str, result: str) -> float:
        """è®¡ç®—åŒ¹é…åˆ†æ•°"""
        if not target or not result:
            return 0.0
        
        target = target.lower()
        result = result.lower()
        
        # å®Œå…¨åŒ¹é…
        if target == result:
            return 1.0
        
        # åŒ…å«åŒ¹é…
        if target in result:
            return 0.8
        
        # å…³é”®è¯åŒ¹é…
        target_words = set(target.replace('åŠæ³•', '').replace('è§„å®š', '').replace('æ¡ä¾‹', ''))
        result_words = set(result.replace('åŠæ³•', '').replace('è§„å®š', '').replace('æ¡ä¾‹', ''))
        
        if target_words & result_words:
            common_ratio = len(target_words & result_words) / len(target_words | result_words)
            return common_ratio * 0.6
        
        return 0.0
    
    def determine_law_level(self, document_number: str) -> str:
        """æ ¹æ®æ–‡å·ç¡®å®šæ³•è§„å±‚çº§"""
        if not document_number:
            return "éƒ¨é—¨è§„ç« "
        
        if "ä¸»å¸­ä»¤" in document_number:
            return "æ³•å¾‹"
        elif "å›½åŠ¡é™¢ä»¤" in document_number:
            return "è¡Œæ”¿æ³•è§„"
        elif "å›½å‘" in document_number or "å›½åŠå‘" in document_number:
            return "å›½åŠ¡é™¢æ–‡ä»¶"
        else:
            return "éƒ¨é—¨è§„ç« "
    
    def _extract_keywords(self, law_name: str) -> List[str]:
        """ä»æ³•è§„åç§°ä¸­æå–å…³é”®è¯"""
        keywords = []
        
        # ç§»é™¤å¸¸è§åç¼€
        simplified_name = law_name
        for suffix in ['åŠæ³•', 'è§„å®š', 'æ¡ä¾‹', 'å®æ–½ç»†åˆ™', 'æš‚è¡ŒåŠæ³•', 'è¯•è¡ŒåŠæ³•']:
            simplified_name = simplified_name.replace(suffix, '')
        
        # æ·»åŠ ç®€åŒ–åç§°
        if simplified_name != law_name:
            keywords.append(simplified_name)
        
        # æå–å…³é”®è¯ç»„åˆ
        if 'ç”µå­' in law_name and 'æ‹›æ ‡' in law_name:
            keywords.extend(['æ‹›æ ‡æŠ•æ ‡', 'ç”µå­æ‹›æ ‡', 'æŠ•æ ‡åŠæ³•'])
        
        return keywords


# ä¸ºäº†å…¼å®¹ç°æœ‰ä»£ç ï¼Œåˆ›å»ºä¸€ä¸ªå·¥å‚å‡½æ•°
def create_selenium_gov_crawler():
    """åˆ›å»ºSeleniumæ”¿åºœç½‘çˆ¬è™«å®ä¾‹"""
    return SeleniumGovCrawler() 