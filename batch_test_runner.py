#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Strategy1 æ‰¹é‡æµ‹è¯•è¿è¡Œå™¨
è‡ªåŠ¨åŒ–æµ‹è¯•ä¸åŒå‚æ•°ç»„åˆï¼Œæ‰¾å‡ºåçˆ¬ä¸´ç•Œç‚¹

åŠŸèƒ½ï¼š
- æµ‹è¯•ä¸åŒè¯·æ±‚é—´éš”å’Œæ•°é‡çš„ç»„åˆ
- è‡ªåŠ¨åˆ†ææœ€ä½³å‚æ•°èŒƒå›´
- ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š
- æä¾›åçˆ¬ç­–ç•¥å»ºè®®

ä½¿ç”¨æ–¹æ³•ï¼š
python batch_test_runner.py --config batch_test_config.json
"""

import asyncio
import json
import logging
import argparse
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Tuple
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from test_strategy1_anti_crawler import Strategy1AntiCrawlerTester

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'batch_test_logs/batch_test_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class BatchTestRunner:
    """æ‰¹é‡æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self, config_file: str = None):
        self.config = self._load_config(config_file)
        self.output_dir = Path("batch_test_logs")
        self.output_dir.mkdir(exist_ok=True)
        
        # æµ‹è¯•ç»“æœ
        self.batch_results: List[Dict[str, Any]] = []
        
        logger.info("ğŸš€ æ‰¹é‡æµ‹è¯•è¿è¡Œå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ“‹ é…ç½®: {self.config}")
    
    def _load_config(self, config_file: str) -> Dict[str, Any]:
        """åŠ è½½æµ‹è¯•é…ç½®"""
        default_config = {
            "test_scenarios": [
                {
                    "name": "å¿«é€Ÿæµ‹è¯•",
                    "request_count": 50,
                    "interval": 0.1,
                    "description": "é«˜é¢‘ç‡çŸ­æ—¶é—´æµ‹è¯•ï¼Œå¿«é€Ÿè§¦å‘åçˆ¬"
                },
                {
                    "name": "æ­£å¸¸æµ‹è¯•",
                    "request_count": 100,
                    "interval": 0.5,
                    "description": "ä¸­ç­‰é¢‘ç‡æµ‹è¯•ï¼Œæ¨¡æ‹Ÿæ­£å¸¸ä½¿ç”¨"
                },
                {
                    "name": "ä¿å®ˆæµ‹è¯•",
                    "request_count": 200,
                    "interval": 1.0,
                    "description": "ä½é¢‘ç‡é•¿æ—¶é—´æµ‹è¯•ï¼Œé¿å…è§¦å‘åçˆ¬"
                },
                {
                    "name": "æé™æµ‹è¯•",
                    "request_count": 500,
                    "interval": 0.05,
                    "description": "æé«˜é¢‘ç‡æµ‹è¯•ï¼Œæµ‹è¯•ç³»ç»Ÿæé™"
                }
            ],
            "analysis_settings": {
                "success_rate_threshold": 80.0,
                "waf_trigger_threshold": 5,
                "response_time_threshold": 2.0
            }
        }
        
        if config_file and Path(config_file).exists():
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                logger.info(f"ğŸ“ åŠ è½½é…ç½®æ–‡ä»¶: {config_file}")
                return config
            except Exception as e:
                logger.warning(f"âš ï¸ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        
        # ä¿å­˜é»˜è®¤é…ç½®
        default_config_file = "batch_test_config.json"
        with open(default_config_file, 'w', encoding='utf-8') as f:
            json.dump(default_config, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ“ åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶: {default_config_file}")
        
        return default_config
    
    async def run_single_scenario(self, scenario: Dict[str, Any]) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•åœºæ™¯"""
        logger.info(f"ğŸ¯ å¼€å§‹æµ‹è¯•åœºæ™¯: {scenario['name']}")
        logger.info(f"ğŸ“Š å‚æ•°: è¯·æ±‚æ•°={scenario['request_count']}, é—´éš”={scenario['interval']}ç§’")
        
        # åˆ›å»ºæµ‹è¯•å™¨
        tester = Strategy1AntiCrawlerTester(
            max_requests=scenario['request_count'],
            interval=scenario['interval'],
            output_dir=self.output_dir / scenario['name']
        )
        
        # è¿è¡Œæµ‹è¯•
        start_time = datetime.now()
        await tester.run_stress_test()
        end_time = datetime.now()
        
        # åˆ†æç»“æœ
        result = self._analyze_scenario_result(tester, scenario, start_time, end_time)
        
        logger.info(f"âœ… åœºæ™¯ '{scenario['name']}' æµ‹è¯•å®Œæˆ")
        logger.info(f"ğŸ“Š æˆåŠŸç‡: {result['success_rate']:.1f}%, WAFè§¦å‘: {result['waf_triggered']}")
        
        return result
    
    def _analyze_scenario_result(self, tester: Strategy1AntiCrawlerTester, 
                               scenario: Dict[str, Any], 
                               start_time: datetime, 
                               end_time: datetime) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªåœºæ™¯çš„æµ‹è¯•ç»“æœ"""
        duration = (end_time - start_time).total_seconds()
        
        # åŸºç¡€ç»Ÿè®¡
        success_rate = (tester.successful_requests / tester.total_requests) * 100 if tester.total_requests > 0 else 0
        avg_response_time = sum(r.response_time for r in tester.test_results) / len(tester.test_results) if tester.test_results else 0
        
        # è®¡ç®—è¯·æ±‚é€Ÿç‡
        actual_rps = tester.total_requests / duration if duration > 0 else 0
        
        # æ‰¾åˆ°WAFè§¦å‘ç‚¹
        waf_trigger_point = None
        for i, result in enumerate(tester.test_results):
            if result.is_waf_blocked:
                waf_trigger_point = i + 1
                break
        
        # è®¡ç®—æˆåŠŸç‡ä¸‹é™ç‚¹
        success_decline_point = None
        if len(tester.test_results) > 10:
            window_size = 10
            for i in range(window_size, len(tester.test_results)):
                window_success = sum(1 for r in tester.test_results[i-window_size:i] if r.success)
                window_success_rate = (window_success / window_size) * 100
                if window_success_rate < 50:  # æˆåŠŸç‡ä½äº50%
                    success_decline_point = i
                    break
        
        # è¯„ä¼°ç­‰çº§
        evaluation = self._evaluate_scenario(success_rate, tester.waf_triggered, avg_response_time)
        
        return {
            "scenario_name": scenario['name'],
            "scenario_config": scenario,
            "start_time": start_time.isoformat(),
            "end_time": end_time.isoformat(),
            "duration_seconds": duration,
            "total_requests": tester.total_requests,
            "successful_requests": tester.successful_requests,
            "failed_requests": tester.failed_requests,
            "waf_blocked_requests": tester.waf_blocked_requests,
            "success_rate": success_rate,
            "avg_response_time": avg_response_time,
            "actual_rps": actual_rps,
            "waf_triggered": tester.waf_triggered,
            "ip_blocked": tester.ip_blocked,
            "rate_limited": tester.rate_limited,
            "waf_trigger_point": waf_trigger_point,
            "success_decline_point": success_decline_point,
            "max_consecutive_failures": tester.max_consecutive_failures,
            "evaluation": evaluation
        }
    
    def _evaluate_scenario(self, success_rate: float, waf_triggered: bool, avg_response_time: float) -> Dict[str, Any]:
        """è¯„ä¼°åœºæ™¯ç»“æœ"""
        thresholds = self.config['analysis_settings']
        
        # è¯„åˆ†é€»è¾‘
        score = 0
        
        # æˆåŠŸç‡è¯„åˆ† (40åˆ†)
        if success_rate >= thresholds['success_rate_threshold']:
            score += 40
        elif success_rate >= 60:
            score += 30
        elif success_rate >= 40:
            score += 20
        else:
            score += 10
        
        # WAFè§¦å‘è¯„åˆ† (30åˆ†)
        if not waf_triggered:
            score += 30
        else:
            score += 10
        
        # å“åº”æ—¶é—´è¯„åˆ† (30åˆ†)
        if avg_response_time <= thresholds['response_time_threshold']:
            score += 30
        elif avg_response_time <= 5.0:
            score += 20
        else:
            score += 10
        
        # ç¡®å®šç­‰çº§
        if score >= 90:
            level = "ä¼˜ç§€"
            color = "green"
        elif score >= 70:
            level = "è‰¯å¥½"
            color = "blue"
        elif score >= 50:
            level = "ä¸€èˆ¬"
            color = "orange"
        else:
            level = "å·®"
            color = "red"
        
        return {
            "score": score,
            "level": level,
            "color": color,
            "recommendations": self._get_recommendations(success_rate, waf_triggered, avg_response_time)
        }
    
    def _get_recommendations(self, success_rate: float, waf_triggered: bool, avg_response_time: float) -> List[str]:
        """è·å–æ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if success_rate < 80:
            recommendations.append("å»ºè®®é™ä½è¯·æ±‚é¢‘ç‡ï¼Œå¢åŠ é—´éš”æ—¶é—´")
        
        if waf_triggered:
            recommendations.append("å»ºè®®ä½¿ç”¨ä»£ç†IPæˆ–æ›´æ¢User-Agent")
            recommendations.append("è€ƒè™‘æ·»åŠ æ›´å¤šéšæœºå»¶è¿Ÿ")
        
        if avg_response_time > 2.0:
            recommendations.append("å“åº”æ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®ä¼˜åŒ–ç½‘ç»œè¿æ¥")
        
        if not recommendations:
            recommendations.append("å½“å‰å‚æ•°é…ç½®è‰¯å¥½ï¼Œå¯ä»¥ç»§ç»­ä½¿ç”¨")
        
        return recommendations
    
    async def run_all_scenarios(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•åœºæ™¯"""
        logger.info("ğŸš€ å¼€å§‹æ‰¹é‡æµ‹è¯•...")
        
        total_scenarios = len(self.config['test_scenarios'])
        
        for i, scenario in enumerate(self.config['test_scenarios']):
            logger.info(f"ğŸ“‹ [{i+1}/{total_scenarios}] å‡†å¤‡æµ‹è¯•åœºæ™¯: {scenario['name']}")
            
            try:
                result = await self.run_single_scenario(scenario)
                self.batch_results.append(result)
                
                # æ·»åŠ é—´éš”ï¼Œé¿å…å¯¹æœåŠ¡å™¨é€ æˆè¿‡å¤§å‹åŠ›
                if i < total_scenarios - 1:
                    logger.info("â±ï¸ ç­‰å¾…10ç§’åç»§ç»­ä¸‹ä¸€ä¸ªåœºæ™¯...")
                    await asyncio.sleep(10)
                    
            except Exception as e:
                logger.error(f"âŒ åœºæ™¯ '{scenario['name']}' æµ‹è¯•å¤±è´¥: {e}")
                # è®°å½•å¤±è´¥ç»“æœ
                self.batch_results.append({
                    "scenario_name": scenario['name'],
                    "error": str(e),
                    "success_rate": 0,
                    "evaluation": {"level": "é”™è¯¯", "score": 0}
                })
        
        logger.info("ğŸ æ‰€æœ‰åœºæ™¯æµ‹è¯•å®Œæˆ")
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        self.generate_comprehensive_report()
    
    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š"""
        report_time = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # åˆ›å»ºç»¼åˆæŠ¥å‘Š
        report = {
            "æŠ¥å‘Šæ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "æµ‹è¯•é…ç½®": self.config,
            "åœºæ™¯ç»“æœ": self.batch_results,
            "ç»¼åˆåˆ†æ": self._generate_comprehensive_analysis(),
            "å»ºè®®å‚æ•°": self._suggest_optimal_parameters()
        }
        
        # ä¿å­˜JSONæŠ¥å‘Š
        json_file = self.output_dir / f"comprehensive_report_{report_time}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # ç”ŸæˆCSVæ€»ç»“
        csv_data = []
        for result in self.batch_results:
            if 'error' not in result:
                csv_data.append({
                    "åœºæ™¯åç§°": result['scenario_name'],
                    "è¯·æ±‚æ•°": result['total_requests'],
                    "é—´éš”(ç§’)": result['scenario_config']['interval'],
                    "æˆåŠŸç‡(%)": f"{result['success_rate']:.1f}",
                    "WAFè§¦å‘": result['waf_triggered'],
                    "å¹³å‡å“åº”æ—¶é—´(ç§’)": f"{result['avg_response_time']:.3f}",
                    "å®é™…RPS": f"{result['actual_rps']:.2f}",
                    "è¯„ä¼°ç­‰çº§": result['evaluation']['level'],
                    "è¯„åˆ†": result['evaluation']['score']
                })
        
        if csv_data:
            csv_file = self.output_dir / f"comprehensive_summary_{report_time}.csv"
            df = pd.DataFrame(csv_data)
            df.to_csv(csv_file, index=False, encoding='utf-8')
        
        # ç”Ÿæˆå›¾è¡¨
        self.generate_comprehensive_charts(report_time)
        
        # æ‰“å°æŠ¥å‘Š
        self.print_comprehensive_report(report)
        
        logger.info(f"ğŸ“ ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜:")
        logger.info(f"   JSONæŠ¥å‘Š: {json_file}")
        if csv_data:
            logger.info(f"   CSVæ€»ç»“: {csv_file}")
    
    def _generate_comprehensive_analysis(self) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆåˆ†æ"""
        if not self.batch_results:
            return {}
        
        # è¿‡æ»¤æ‰å‡ºé”™çš„ç»“æœ
        valid_results = [r for r in self.batch_results if 'error' not in r]
        
        if not valid_results:
            return {"error": "æ²¡æœ‰æœ‰æ•ˆçš„æµ‹è¯•ç»“æœ"}
        
        # ç»Ÿè®¡åˆ†æ
        success_rates = [r['success_rate'] for r in valid_results]
        response_times = [r['avg_response_time'] for r in valid_results]
        waf_triggered_count = sum(1 for r in valid_results if r['waf_triggered'])
        
        # æ‰¾å‡ºæœ€ä½³å’Œæœ€å·®åœºæ™¯
        best_scenario = max(valid_results, key=lambda x: x['evaluation']['score'])
        worst_scenario = min(valid_results, key=lambda x: x['evaluation']['score'])
        
        # åçˆ¬ä¸´ç•Œç‚¹åˆ†æ
        waf_trigger_points = [r['waf_trigger_point'] for r in valid_results if r['waf_trigger_point']]
        
        return {
            "åœºæ™¯æ€»æ•°": len(valid_results),
            "å¹³å‡æˆåŠŸç‡": np.mean(success_rates),
            "æˆåŠŸç‡æ ‡å‡†å·®": np.std(success_rates),
            "å¹³å‡å“åº”æ—¶é—´": np.mean(response_times),
            "WAFè§¦å‘åœºæ™¯æ•°": waf_triggered_count,
            "WAFè§¦å‘ç‡": (waf_triggered_count / len(valid_results)) * 100,
            "æœ€ä½³åœºæ™¯": {
                "åç§°": best_scenario['scenario_name'],
                "æˆåŠŸç‡": best_scenario['success_rate'],
                "è¯„åˆ†": best_scenario['evaluation']['score']
            },
            "æœ€å·®åœºæ™¯": {
                "åç§°": worst_scenario['scenario_name'],
                "æˆåŠŸç‡": worst_scenario['success_rate'],
                "è¯„åˆ†": worst_scenario['evaluation']['score']
            },
            "åçˆ¬ä¸´ç•Œç‚¹": {
                "å¹³å‡è§¦å‘ç‚¹": np.mean(waf_trigger_points) if waf_trigger_points else None,
                "æœ€æ—©è§¦å‘ç‚¹": min(waf_trigger_points) if waf_trigger_points else None,
                "æœ€æ™šè§¦å‘ç‚¹": max(waf_trigger_points) if waf_trigger_points else None
            }
        }
    
    def _suggest_optimal_parameters(self) -> Dict[str, Any]:
        """å»ºè®®æœ€ä½³å‚æ•°"""
        valid_results = [r for r in self.batch_results if 'error' not in r]
        
        if not valid_results:
            return {"error": "æ²¡æœ‰æœ‰æ•ˆçš„æµ‹è¯•ç»“æœ"}
        
        # ç­›é€‰å‡ºæˆåŠŸç‡é«˜ä¸”æœªè§¦å‘WAFçš„åœºæ™¯
        good_scenarios = [r for r in valid_results if r['success_rate'] >= 80 and not r['waf_triggered']]
        
        if good_scenarios:
            # æŒ‰è¯„åˆ†æ’åº
            good_scenarios.sort(key=lambda x: x['evaluation']['score'], reverse=True)
            best_scenario = good_scenarios[0]
            
            return {
                "æ¨èåœºæ™¯": best_scenario['scenario_name'],
                "æ¨èå‚æ•°": {
                    "è¯·æ±‚é—´éš”": best_scenario['scenario_config']['interval'],
                    "æœ€å¤§è¯·æ±‚æ•°": best_scenario['scenario_config']['request_count'],
                    "é¢„æœŸæˆåŠŸç‡": f"{best_scenario['success_rate']:.1f}%",
                    "é¢„æœŸRPS": f"{best_scenario['actual_rps']:.2f}"
                },
                "ä½¿ç”¨å»ºè®®": [
                    f"å»ºè®®ä½¿ç”¨ {best_scenario['scenario_config']['interval']} ç§’é—´éš”",
                    f"å•æ¬¡æ‰¹é‡ä¸è¶…è¿‡ {best_scenario['scenario_config']['request_count']} ä¸ªè¯·æ±‚",
                    "å»ºè®®é…åˆä»£ç†IPä½¿ç”¨ä»¥æé«˜ç¨³å®šæ€§",
                    "å»ºè®®æ·»åŠ éšæœºå»¶è¿Ÿé¿å…è§„å¾‹æ€§æ£€æµ‹"
                ]
            }
        else:
            # å¦‚æœæ²¡æœ‰å®Œå…¨ç†æƒ³çš„åœºæ™¯ï¼Œé€‰æ‹©ç›¸å¯¹æœ€å¥½çš„
            best_scenario = max(valid_results, key=lambda x: x['evaluation']['score'])
            return {
                "æ¨èåœºæ™¯": best_scenario['scenario_name'],
                "æ¨èå‚æ•°": best_scenario['scenario_config'],
                "è­¦å‘Š": "æ‰€æœ‰åœºæ™¯éƒ½å­˜åœ¨é—®é¢˜ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–",
                "ä½¿ç”¨å»ºè®®": [
                    "å»ºè®®è¿›ä¸€æ­¥é™ä½è¯·æ±‚é¢‘ç‡",
                    "ä½¿ç”¨ä»£ç†IPæ± ",
                    "å¢åŠ æ›´å¤šåæ£€æµ‹æœºåˆ¶",
                    "è€ƒè™‘åˆ†æ—¶æ®µè¯·æ±‚"
                ]
            }
    
    def generate_comprehensive_charts(self, report_time: str):
        """ç”Ÿæˆç»¼åˆå›¾è¡¨"""
        valid_results = [r for r in self.batch_results if 'error' not in r]
        
        if not valid_results:
            return
        
        # åˆ›å»ºå›¾è¡¨
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. æˆåŠŸç‡å¯¹æ¯”
        scenarios = [r['scenario_name'] for r in valid_results]
        success_rates = [r['success_rate'] for r in valid_results]
        colors = [r['evaluation']['color'] for r in valid_results]
        
        bars1 = ax1.bar(scenarios, success_rates, color=colors, alpha=0.7)
        ax1.set_title('å„åœºæ™¯æˆåŠŸç‡å¯¹æ¯”')
        ax1.set_ylabel('æˆåŠŸç‡ (%)')
        ax1.set_ylim(0, 100)
        ax1.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, rate in zip(bars1, success_rates):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                    f'{rate:.1f}%', ha='center', va='bottom')
        
        # 2. å“åº”æ—¶é—´å¯¹æ¯”
        response_times = [r['avg_response_time'] for r in valid_results]
        bars2 = ax2.bar(scenarios, response_times, color='skyblue', alpha=0.7)
        ax2.set_title('å¹³å‡å“åº”æ—¶é—´å¯¹æ¯”')
        ax2.set_ylabel('å“åº”æ—¶é—´ (ç§’)')
        ax2.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, time in zip(bars2, response_times):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,
                    f'{time:.2f}s', ha='center', va='bottom')
        
        # 3. WAFè§¦å‘æƒ…å†µ
        waf_triggered = [1 if r['waf_triggered'] else 0 for r in valid_results]
        bars3 = ax3.bar(scenarios, waf_triggered, color=['red' if w else 'green' for w in waf_triggered], alpha=0.7)
        ax3.set_title('WAFè§¦å‘æƒ…å†µ')
        ax3.set_ylabel('WAFè§¦å‘ (1=æ˜¯, 0=å¦)')
        ax3.set_ylim(0, 1.2)
        ax3.grid(True, alpha=0.3)
        
        # 4. è¯„åˆ†å¯¹æ¯”
        scores = [r['evaluation']['score'] for r in valid_results]
        bars4 = ax4.bar(scenarios, scores, color=colors, alpha=0.7)
        ax4.set_title('ç»¼åˆè¯„åˆ†å¯¹æ¯”')
        ax4.set_ylabel('è¯„åˆ†')
        ax4.set_ylim(0, 100)
        ax4.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, score in zip(bars4, scores):
            ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                    f'{score}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.xticks(rotation=45)
        
        # ä¿å­˜å›¾è¡¨
        chart_file = self.output_dir / f"comprehensive_charts_{report_time}.png"
        plt.savefig(chart_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"   å›¾è¡¨æ–‡ä»¶: {chart_file}")
    
    def print_comprehensive_report(self, report: Dict[str, Any]):
        """æ‰“å°ç»¼åˆæŠ¥å‘Š"""
        print(f"\n{'='*80}")
        print(f"ğŸ¯ Strategy1 æ‰¹é‡æµ‹è¯•ç»¼åˆæŠ¥å‘Š")
        print(f"{'='*80}")
        
        # ç»¼åˆåˆ†æ
        analysis = report.get('ç»¼åˆåˆ†æ', {})
        if analysis and 'error' not in analysis:
            print(f"ğŸ“Š ç»¼åˆåˆ†æ:")
            print(f"   åœºæ™¯æ€»æ•°: {analysis['åœºæ™¯æ€»æ•°']}")
            print(f"   å¹³å‡æˆåŠŸç‡: {analysis['å¹³å‡æˆåŠŸç‡']:.1f}%")
            print(f"   WAFè§¦å‘ç‡: {analysis['WAFè§¦å‘ç‡']:.1f}%")
            print(f"   æœ€ä½³åœºæ™¯: {analysis['æœ€ä½³åœºæ™¯']['åç§°']} (æˆåŠŸç‡: {analysis['æœ€ä½³åœºæ™¯']['æˆåŠŸç‡']:.1f}%)")
            print(f"   æœ€å·®åœºæ™¯: {analysis['æœ€å·®åœºæ™¯']['åç§°']} (æˆåŠŸç‡: {analysis['æœ€å·®åœºæ™¯']['æˆåŠŸç‡']:.1f}%)")
            
            # åçˆ¬ä¸´ç•Œç‚¹
            threshold = analysis.get('åçˆ¬ä¸´ç•Œç‚¹', {})
            if threshold.get('å¹³å‡è§¦å‘ç‚¹'):
                print(f"   åçˆ¬å¹³å‡è§¦å‘ç‚¹: {threshold['å¹³å‡è§¦å‘ç‚¹']:.0f} ä¸ªè¯·æ±‚")
        
        # å»ºè®®å‚æ•°
        suggestions = report.get('å»ºè®®å‚æ•°', {})
        if suggestions and 'error' not in suggestions:
            print(f"\nğŸ’¡ å»ºè®®å‚æ•°:")
            print(f"   æ¨èåœºæ™¯: {suggestions['æ¨èåœºæ™¯']}")
            if 'æ¨èå‚æ•°' in suggestions:
                params = suggestions['æ¨èå‚æ•°']
                print(f"   è¯·æ±‚é—´éš”: {params['è¯·æ±‚é—´éš”']} ç§’")
                print(f"   æœ€å¤§è¯·æ±‚æ•°: {params['æœ€å¤§è¯·æ±‚æ•°']}")
                print(f"   é¢„æœŸæˆåŠŸç‡: {params['é¢„æœŸæˆåŠŸç‡']}")
            
            print(f"   ä½¿ç”¨å»ºè®®:")
            for suggestion in suggestions.get('ä½¿ç”¨å»ºè®®', []):
                print(f"     â€¢ {suggestion}")
        
        print(f"{'='*80}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Strategy1 æ‰¹é‡æµ‹è¯•è¿è¡Œå™¨')
    parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path("batch_test_logs").mkdir(exist_ok=True)
    
    # åˆ›å»ºæ‰¹é‡æµ‹è¯•è¿è¡Œå™¨
    runner = BatchTestRunner(args.config)
    
    # è¿è¡Œæ‰¹é‡æµ‹è¯•
    asyncio.run(runner.run_all_scenarios())

if __name__ == "__main__":
    main() 