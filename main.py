#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ³•å¾‹æ³•è§„é‡‡é›†ç³»ç»Ÿä¸»ç¨‹åº - ä½¿ç”¨åŒæ•°æ®æºç­–ç•¥
æ”¯æŒï¼šå›½å®¶æ³•å¾‹æ³•è§„æ•°æ®åº“ + ä¸­å›½æ”¿åºœç½‘

ä½¿ç”¨æ–¹æ³•ï¼š
  python main.py                           # æ‰¹é‡çˆ¬å–ï¼ˆæŒ‰é…ç½®é™åˆ¶ï¼‰
  python main.py --law "ç”µå­æ‹›æ ‡æŠ•æ ‡åŠæ³•"    # å•ç‹¬æœç´¢æŒ‡å®šæ³•è§„
  python main.py --law "ç”µå­æ‹›æ ‡æŠ•æ ‡åŠæ³•" -v # è¯¦ç»†æ¨¡å¼æ˜¾ç¤ºæœç´¢è¿‡ç¨‹
"""

import asyncio
import json
import os
import pandas as pd
import argparse
import time
from datetime import datetime
from typing import List, Dict, Any

from config.settings import settings
from src.crawler.crawler_manager import CrawlerManager

def normalize_date_format(date_str: str) -> str:
    """
    å°†å„ç§æ—¥æœŸæ ¼å¼ç»Ÿä¸€è½¬æ¢ä¸º yyyy-mm-dd æ ¼å¼
    æ”¯æŒçš„è¾“å…¥æ ¼å¼ï¼š
    - 2013å¹´2æœˆ4æ—¥ -> 2013-02-04
    - 2013-2-4 -> 2013-02-04
    - 2013.2.4 -> 2013-02-04
    - 2025-05-29 00:00:00 -> 2025-05-29
    """
    if not date_str or date_str.strip() == '':
        return ''
    
    import re
    from datetime import datetime
    
    date_str = str(date_str).strip()
    
    # è½¬æ¢å…¨è§’å­—ç¬¦ä¸ºåŠè§’ï¼ˆä¿®å¤å…¨è§’æ—¥æœŸé—®é¢˜ï¼‰
    full_to_half = {
        'ï¼': '0', 'ï¼‘': '1', 'ï¼’': '2', 'ï¼“': '3', 'ï¼”': '4',
        'ï¼•': '5', 'ï¼–': '6', 'ï¼—': '7', 'ï¼˜': '8', 'ï¼™': '9',
        'ï¼': '-', 'â€”': '-', 'â€“': '-'
    }
    for full, half in full_to_half.items():
        date_str = date_str.replace(full, half)
    
    try:
        # æ ¼å¼1: 2013å¹´2æœˆ4æ—¥
        match = re.match(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', date_str)
        if match:
            year, month, day = match.groups()
            return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
        
        # æ ¼å¼2: 2013-2-4 æˆ– 2013/2/4 æˆ– 2013.2.4
        match = re.match(r'(\d{4})[-/.](\d{1,2})[-/.](\d{1,2})', date_str)
        if match:
            year, month, day = match.groups()
            return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
        
        # æ ¼å¼3: 2025-05-29 00:00:00 (å¸¦æ—¶é—´)
        match = re.match(r'(\d{4}-\d{2}-\d{2})\s+\d{2}:\d{2}:\d{2}', date_str)
        if match:
            return match.group(1)
        
        # æ ¼å¼4: å·²ç»æ˜¯ yyyy-mm-dd æ ¼å¼
        match = re.match(r'\d{4}-\d{2}-\d{2}$', date_str)
        if match:
            return date_str
        
        # æ ¼å¼5: å°è¯•ä½¿ç”¨datetimeè§£æ
        for fmt in ['%Y-%m-%d', '%Y/%m/%d', '%Y.%m.%d', '%Yå¹´%mæœˆ%dæ—¥']:
            try:
                dt = datetime.strptime(date_str, fmt)
                return dt.strftime('%Y-%m-%d')
            except:
                continue
        
        # å¦‚æœéƒ½æ— æ³•è§£æï¼Œè¿”å›åŸå§‹å­—ç¬¦ä¸²
        print(f"æ— æ³•è§£ææ—¥æœŸæ ¼å¼: {date_str}")
        return date_str
        
    except Exception as e:
        print(f"æ—¥æœŸæ ¼å¼åŒ–å¤±è´¥: {date_str}, é”™è¯¯: {e}")
        return date_str


def normalize_datetime_format(datetime_str: str) -> str:
    """ç»Ÿä¸€æ—¶é—´æ ¼å¼ä¸ºYYYY-MM-DD HH:MM:SSï¼Œé¿å…Excelè¯†åˆ«ä¸ºè¶…é“¾æ¥"""
    if not datetime_str or datetime_str.strip() == "":
        return ""
    
    import re
    datetime_str = datetime_str.strip()
    
    # å¦‚æœæ˜¯ISOæ ¼å¼ï¼Œè½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
    if 'T' in datetime_str:
        try:
            # è§£æISOæ ¼å¼ï¼š2025-06-23T16:25:10.215903
            dt = datetime.fromisoformat(datetime_str.replace('Z', '+00:00'))
            return dt.strftime("%Y-%m-%d %H:%M:%S")
        except:
            pass
    
    # å¦‚æœå·²ç»æ˜¯æ ‡å‡†æ ¼å¼ï¼Œç›´æ¥è¿”å›
    if re.match(r'^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}$', datetime_str):
        return datetime_str
    
    # å°è¯•å…¶ä»–æ ¼å¼
    try:
        # å°è¯•è§£æå¸¸è§æ ¼å¼
        for fmt in ["%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S", "%Y-%m-%d", "%Y/%m/%d"]:
            try:
                dt = datetime.strptime(datetime_str, fmt)
                return dt.strftime("%Y-%m-%d %H:%M:%S")
            except:
                continue
    except:
        pass
    
    # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œè¿”å›å½“å‰æ—¶é—´æ ¼å¼
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

def load_target_laws_from_excel(excel_path: str) -> List[str]:
    """ä»Excelæ–‡ä»¶åŠ è½½ç›®æ ‡æ³•è§„åˆ—è¡¨"""
    try:
        # æ­£ç¡®è¯»å–Excelï¼Œä½¿ç”¨ç¬¬ä¸€è¡Œä½œä¸ºæ ‡é¢˜
        df = pd.read_excel(excel_path)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰"åç§°"åˆ—
        if "åç§°" in df.columns:
            laws = df["åç§°"].dropna().astype(str).tolist()
        else:
            # å¦‚æœæ²¡æœ‰"åç§°"åˆ—ï¼Œä½¿ç”¨ç¬¬ä¸€åˆ—
            print("è­¦å‘Šï¼šExcelæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°'åç§°'åˆ—ï¼Œä½¿ç”¨ç¬¬ä¸€åˆ—")
            laws = df.iloc[:, 0].dropna().astype(str).tolist()
        
        # æ¸…ç†æ•°æ®ï¼šå»é™¤ç©ºç™½å’Œæ— æ•ˆæ¡ç›®
        laws = [law.strip() for law in laws if law.strip() and law.strip() != 'nan']
        return laws
    except Exception as e:
        print(f"è¯»å–Excelæ–‡ä»¶å¤±è´¥: {e}")
        return []

async def save_results(results: List[Dict[str, Any]], target_laws: List[str], output_dir: str = "data"):
    """ä¿å­˜é‡‡é›†ç»“æœåˆ°å„ç§æ ¼å¼çš„æ–‡ä»¶"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(f"{output_dir}/raw/json", exist_ok=True)
    os.makedirs(f"{output_dir}/raw/detailed", exist_ok=True)
    os.makedirs(f"{output_dir}/ledgers", exist_ok=True)
    
    # å»ºç«‹ç»“æœæ˜ å°„ï¼ˆæŒ‰åç§°åŒ¹é…ï¼‰
    results_map = {}
    for result in results:
        target_name = result.get('target_name', result.get('name'))
        if target_name:
            results_map[target_name] = result
    
    excel_results = []
    detailed_results = []
    
    # å¤„ç†æ¯ä¸ªç›®æ ‡æ³•è§„
    for i, target_law in enumerate(target_laws):
        if target_law in results_map:
            law_data = results_map[target_law]
            
            # ç¡®å®šæ¥æºæ¸ é“ - ä¿®å¤ç‰ˆ
            source = law_data.get('source', 'unknown')
            source_url = law_data.get('source_url', '')
            source_channel = ""
            
            # ä¼˜å…ˆé€šè¿‡sourceå­—æ®µåˆ¤æ–­
            if source == "search_api":
                source_channel = "å›½å®¶æ³•å¾‹æ³•è§„æ•°æ®åº“"
            elif source == "selenium_gov_web":
                source_channel = "ä¸­å›½æ”¿åºœç½‘(www.gov.cn)"
            elif source == "gov_web":
                source_channel = "ä¸­å›½æ”¿åºœç½‘"
            elif source in ["æœç´¢å¼•æ“(æ”¿åºœç½‘)", "DuckDuckGo", "Bing"]:
                source_channel = "æœç´¢å¼•æ“(æ”¿åºœç½‘)"
            else:
                # é€šè¿‡URLåˆ¤æ–­æ¥æº
                if "flk.npc.gov.cn" in source_url:
                    source_channel = "å›½å®¶æ³•å¾‹æ³•è§„æ•°æ®åº“"
                elif "gov.cn" in source_url:
                    source_channel = "æœç´¢å¼•æ“(æ”¿åºœç½‘)"
                elif source_url:
                    source_channel = "å…¶ä»–æ”¿åºœç½‘ç«™"
                else:
                    source_channel = "æœªçŸ¥æ¥æº"
            
            # Excelè¡¨æ ¼æ•°æ®ï¼ˆç®€åŒ–ç‰ˆï¼‰
            excel_data = {
                "åºå·": i + 1,
                "ç›®æ ‡æ³•è§„": target_law,
                "æœç´¢å…³é”®è¯": law_data.get('search_keyword', target_law),
                "æ³•è§„åç§°": law_data.get('name', ''),
                "æ–‡å·": law_data.get('number', ''),
                "å‘å¸ƒæ—¥æœŸ": normalize_date_format(law_data.get('publish_date', '')),
                "å®æ–½æ—¥æœŸ": normalize_date_format(law_data.get('valid_from', '')),
                "å¤±æ•ˆæ—¥æœŸ": normalize_date_format(law_data.get('valid_to', '')),
                "å‘å¸ƒæœºå…³": law_data.get('office', ''),
                "æ³•è§„çº§åˆ«": law_data.get('level', ''),
                "çŠ¶æ€": law_data.get('status', ''),
                "æ¥æºæ¸ é“": source_channel,  # æ–°å¢çš„æ¥æºæ¸ é“åˆ—
                "æ¥æºé“¾æ¥": law_data.get('source_url', ''),
                "é‡‡é›†æ—¶é—´": normalize_datetime_format(law_data.get('crawl_time', datetime.now().isoformat())),
                "é‡‡é›†çŠ¶æ€": "æˆåŠŸ"
            }
            
            # è¯¦ç»†æ•°æ®ï¼ˆåŒ…å«æ‰€æœ‰æ‰©å±•ä¿¡æ¯ï¼‰
            detailed_data = {
                "åºå·": i + 1,
                "é‡‡é›†çŠ¶æ€": "æˆåŠŸ",
                "æ¥æºæ¸ é“": source_channel,  # æ–°å¢çš„æ¥æºæ¸ é“åˆ—
                **law_data  # åŒ…å«æ‰€æœ‰åŸå§‹é‡‡é›†æ•°æ®
            }
        else:
            # æœªæ‰¾åˆ°åŒ¹é…çš„æ³•è§„ï¼Œä¿ç•™å ä½
            excel_data = {
                "åºå·": i + 1,
                "ç›®æ ‡æ³•è§„": target_law,
                "æœç´¢å…³é”®è¯": "",
                "æ³•è§„åç§°": "",
                "æ–‡å·": "",
                "å‘å¸ƒæ—¥æœŸ": "",
                "å®æ–½æ—¥æœŸ": "",
                "å¤±æ•ˆæ—¥æœŸ": "",
                "å‘å¸ƒæœºå…³": "",
                "æ³•è§„çº§åˆ«": "",
                "çŠ¶æ€": "",
                "æ¥æºæ¸ é“": "",  # æ–°å¢çš„æ¥æºæ¸ é“åˆ—
                "æ¥æºé“¾æ¥": "",
                "é‡‡é›†æ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "é‡‡é›†çŠ¶æ€": "æœªæ‰¾åˆ°"
            }
            
            detailed_data = {
                "åºå·": i + 1,
                "target_name": target_law,
                "é‡‡é›†çŠ¶æ€": "æœªæ‰¾åˆ°",
                "æ¥æºæ¸ é“": "",  # æ–°å¢çš„æ¥æºæ¸ é“åˆ—
                "é‡‡é›†æ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            
        excel_results.append(excel_data)
        detailed_results.append(detailed_data)
    
    # ä¿å­˜ç®€åŒ–ç‰ˆJSONï¼ˆä¸Excelä¸€è‡´ï¼‰
    json_file = f"{output_dir}/raw/json/search_crawl_{timestamp}.json"
    with open(json_file, "w", encoding="utf-8") as f:
        json.dump(excel_results, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜è¯¦ç»†ç‰ˆJSONï¼ˆåŒ…å«å®Œæ•´APIå“åº”å’Œæ‰©å±•æ•°æ®ï¼‰
    detailed_json_file = f"{output_dir}/raw/detailed/search_crawl_detailed_{timestamp}.json"
    with open(detailed_json_file, "w", encoding="utf-8") as f:
        json.dump(detailed_results, f, ensure_ascii=False, indent=2)
    
    # ç”ŸæˆExcel
    excel_file = f"{output_dir}/ledgers/search_crawl_{timestamp}.xlsx"
    
    df = pd.DataFrame(excel_results)
    
    with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
        df.to_excel(writer, sheet_name='æ³•è§„é‡‡é›†ç»“æœ', index=False)
        
        worksheet = writer.sheets['æ³•è§„é‡‡é›†ç»“æœ']
        
        # è®¾ç½®è¶…é“¾æ¥ï¼ˆä»…å¯¹æˆåŠŸé‡‡é›†çš„æ³•è§„ï¼‰
        # æ³¨æ„ï¼šæ¥æºé“¾æ¥åˆ—çš„ä½ç½®ä»13å˜ä¸º14ï¼ˆå› ä¸ºæ·»åŠ äº†æ¥æºæ¸ é“åˆ—ï¼‰
        for idx, row in enumerate(df.iterrows(), start=2):
            url = row[1]['æ¥æºé“¾æ¥']
            if url and row[1]['é‡‡é›†çŠ¶æ€'] == 'æˆåŠŸ':
                worksheet.cell(row=idx, column=14).hyperlink = url
                worksheet.cell(row=idx, column=14).value = "ç‚¹å‡»æŸ¥çœ‹"
    
    print(f"ç»“æœå·²ä¿å­˜:")
    print(f"  ç®€åŒ–JSON: {json_file}")
    print(f"  è¯¦ç»†JSON: {detailed_json_file} (åŒ…å«å®Œæ•´APIå“åº”)")
    print(f"  Excel: {excel_file}")
    
    # ç»Ÿè®¡æ¥æºæ¸ é“ä¿¡æ¯
    successful_results = [item for item in excel_results if item.get('é‡‡é›†çŠ¶æ€') == 'æˆåŠŸ']
    if successful_results:
        source_stats = {}
        for result in successful_results:
            channel = result.get('æ¥æºæ¸ é“', 'æœªçŸ¥')
            source_stats[channel] = source_stats.get(channel, 0) + 1
        
        print(f"\nğŸ“Š æ•°æ®æ¥æºç»Ÿè®¡:")
        for channel, count in source_stats.items():
            print(f"  - {channel}: {count} æ¡")


def get_default_law_list() -> List[str]:
    """è·å–é»˜è®¤çš„æ³•è§„åˆ—è¡¨"""
    return [
        "ä¸­åäººæ°‘å…±å’Œå›½åæ´—é’±æ³•",
        "ä¸­åäººæ°‘å…±å’Œå›½å…³ç¨æ³•", 
        "ä¸­åäººæ°‘å…±å’Œå›½ç»Ÿè®¡æ³•",
        "ä¸­åäººæ°‘å…±å’Œå›½ä¼šè®¡æ³•",
        "å›½å®¶ç§‘å­¦æŠ€æœ¯å¥–åŠ±æ¡ä¾‹",
        "å›ºå®šèµ„äº§æŠ•èµ„é¡¹ç›®èŠ‚èƒ½å®¡æŸ¥åŠæ³•",
        "ä¸­åäººæ°‘å…±å’Œå›½å†œäº§å“è´¨é‡å®‰å…¨æ³•",
        "æœ€é«˜äººæ°‘æ³•é™¢ã€æœ€é«˜äººæ°‘æ£€å¯Ÿé™¢å…³äºåŠç†å±å®³ç”Ÿäº§å®‰å…¨åˆ‘äº‹æ¡ˆä»¶é€‚ç”¨æ³•å¾‹è‹¥å¹²é—®é¢˜çš„è§£é‡Š",
        "ä¸­åäººæ°‘å…±å’Œå›½ç§‘å­¦æŠ€æœ¯è¿›æ­¥æ³•",
        "ä¸­åäººæ°‘å…±å’Œå›½å®‰å…¨ç”Ÿäº§æ³•",
        "ä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸",
        "ä¸­åäººæ°‘å…±å’Œå›½ç–«è‹—ç®¡ç†æ³•",
        "ä¸­åäººæ°‘å…±å’Œå›½è¯å“ç®¡ç†æ³•",
        "ä¸­åäººæ°‘å…±å’Œå›½å»ºç­‘æ³•",
        "æˆ¿å±‹å»ºç­‘å’Œå¸‚æ”¿åŸºç¡€è®¾æ–½å·¥ç¨‹æ–½å·¥æ‹›æ ‡æŠ•æ ‡ç®¡ç†åŠæ³•",
        "ä¸­åäººæ°‘å…±å’Œå›½äº§å“è´¨é‡æ³•",
        "ä¸­åäººæ°‘å…±å’Œå›½è®¡é‡æ³•",
        "é‡ç‚¹ç”¨èƒ½å•ä½èŠ‚èƒ½ç®¡ç†åŠæ³•",
        "å»ºç­‘å·¥ç¨‹è®¾è®¡æ‹›æ ‡æŠ•æ ‡ç®¡ç†åŠæ³•",
        "ä¸­åäººæ°‘å…±å’Œå›½ç‰¹ç§è®¾å¤‡å®‰å…¨æ³•"
    ]


async def search_single_law(law_name: str, verbose: bool = False, strategy: int = None):
    """å•ç‹¬æœç´¢æŒ‡å®šæ³•è§„"""
    print("=== å•æ³•è§„æœç´¢æ¨¡å¼ ===")
    print(f"ç›®æ ‡æ³•è§„: {law_name}")
    print(f"è¯¦ç»†æ¨¡å¼: {'å¼€å¯' if verbose else 'å…³é—­'}")
    
    if strategy:
        strategy_names = {
            1: "å›½å®¶æ³•å¾‹æ³•è§„æ•°æ®åº“",
            2: "HTTPæœç´¢å¼•æ“",
            3: "Seleniumæœç´¢å¼•æ“", 
            4: "Seleniumæ”¿åºœç½‘",
            5: "ç›´æ¥URLè®¿é—®"
        }
        print(f"æŒ‡å®šç­–ç•¥: {strategy} - {strategy_names.get(strategy, 'æœªçŸ¥ç­–ç•¥')}")
    else:
        print("æ•°æ®æº: å›½å®¶æ³•å¾‹æ³•è§„æ•°æ®åº“ + ä¸­å›½æ”¿åºœç½‘")
    print()
    
    # åˆ›å»ºé‡‡é›†ç®¡ç†å™¨ï¼ˆåŒæ•°æ®æºï¼‰
    crawler_manager = CrawlerManager()
    
    print("å¼€å§‹æœç´¢...")
    result = await crawler_manager.crawl_law(law_name, strategy=strategy)
    
    if result:
        print(f"âœ… æœç´¢æˆåŠŸï¼")
        print(f"   æ¥æº: {result.get('source', 'unknown')}")
        print(f"   åç§°: {result.get('name', 'æœªçŸ¥')}")
        print(f"   æ–‡å·: {result.get('number', 'æ— ')}")
        print(f"   çº§åˆ«: {result.get('level', 'æœªçŸ¥')}")
        print(f"   å‘å¸ƒæ—¥æœŸ: {result.get('publish_date', 'æœªçŸ¥')}")
        print(f"   æ¥æºé“¾æ¥: {result.get('source_url', 'æ— ')}")
        
        if verbose:
            print(f"\nğŸ“‹ è¯¦ç»†ä¿¡æ¯:")
            for key, value in result.items():
                if key not in ['raw_data']:  # è·³è¿‡è¿‡é•¿çš„åŸå§‹æ•°æ®
                    print(f"   {key}: {value}")
        
        # ä¿å­˜å•ä¸ªç»“æœ
        result['target_name'] = law_name
        await save_results([result], [law_name])
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°dataç›®å½•")
        
    else:
        print(f"âŒ æœç´¢å¤±è´¥")
        print(f"   åœ¨æ‰€æœ‰æ•°æ®æºä¸­éƒ½æœªæ‰¾åˆ° '{law_name}'")
        print(f"   å»ºè®®æ£€æŸ¥æ³•è§„åç§°æ˜¯å¦æ­£ç¡®ï¼Œæˆ–å°è¯•ç®€åŒ–æœç´¢å…³é”®è¯")


async def batch_crawl_optimized(limit: int = None, strategy: int = None):
    """æ‰¹é‡çˆ¬å–æ¨¡å¼ - ç»ˆæä¼˜åŒ–ç‰ˆæœ¬"""
    print("=== æ‰¹é‡é‡‡é›†æ¨¡å¼ (ç»ˆæä¼˜åŒ–ç‰ˆ) ===")
    print(f"ç‰ˆæœ¬: {settings.version} | è°ƒè¯•æ¨¡å¼: {'å¼€å¯' if settings.debug else 'å…³é—­'}")
    
    if strategy:
        strategy_names = {
            1: "å›½å®¶æ³•å¾‹æ³•è§„æ•°æ®åº“",
            2: "HTTPæœç´¢å¼•æ“",
            3: "Seleniumæœç´¢å¼•æ“", 
            4: "Seleniumæ”¿åºœç½‘",
            5: "ç›´æ¥URLè®¿é—®"
        }
        print(f"æŒ‡å®šç­–ç•¥: {strategy} - {strategy_names.get(strategy, 'æœªçŸ¥ç­–ç•¥')}")
    else:
        print("ç­–ç•¥: æœç´¢å¼•æ“â†’æ³•è§„åº“â†’ä¼˜åŒ–Selenium (å¤šå±‚å¹¶è¡Œ)")
    print()
    
    # è·å–ç›®æ ‡æ³•è§„åˆ—è¡¨
    excel_path = "Background info/law list.xls"
    if os.path.exists(excel_path):
        print(f"ä»Excelæ–‡ä»¶åŠ è½½æ³•è§„åˆ—è¡¨: {excel_path}")
        target_laws = load_target_laws_from_excel(excel_path)
        if not target_laws:
            print("Excelæ–‡ä»¶ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯ï¼Œç¨‹åºé€€å‡º")
            return
    else:
        print("Excelæ–‡ä»¶ä¸å­˜åœ¨ï¼Œç¨‹åºé€€å‡º")
        return
    
    # æ ¹æ®å‚æ•°æˆ–é…ç½®å†³å®šçˆ¬å–æ•°é‡
    if limit is not None:
        crawl_limit = limit
        print(f"æ ¹æ®å‘½ä»¤è¡Œå‚æ•°é™åˆ¶ï¼Œæœ¬æ¬¡ä»…é‡‡é›†å‰ {crawl_limit} æ¡æ³•è§„")
    else:
        crawl_limit = settings.crawler.crawl_limit
        if crawl_limit > 0:
            print(f"æ ¹æ®é…ç½®é™åˆ¶ï¼Œæœ¬æ¬¡ä»…é‡‡é›†å‰ {crawl_limit} æ¡æ³•è§„")
        else:
            print(f"æ— çˆ¬å–æ•°é‡é™åˆ¶ï¼Œå°†é‡‡é›†å…¨éƒ¨ {len(target_laws)} æ¡æ³•è§„")
    
    if crawl_limit > 0:
        target_laws = target_laws[:crawl_limit]
    
    print(f"ç›®æ ‡æ³•è§„æ•°: {len(target_laws)}")
    if target_laws:
        print("å‰5ä¸ªæ³•è§„:", target_laws[:5])
    print()
    
    # å‡†å¤‡æ³•è§„ä¿¡æ¯åˆ—è¡¨
    law_list = [{'åç§°': law_name} for law_name in target_laws]
    
    # åˆ›å»ºé‡‡é›†ç®¡ç†å™¨
    crawler_manager = CrawlerManager()
    
    try:
        # ä½¿ç”¨ç»ˆæä¼˜åŒ–æ‰¹é‡çˆ¬å–
        print("ğŸš€ å¼€å§‹æ‰¹é‡é‡‡é›†ï¼ˆç»ˆæä¼˜åŒ–æ¨¡å¼ï¼‰...")
        start_time = time.time()
        
        results = await crawler_manager.crawl_laws_batch(law_list, limit=crawl_limit, strategy=strategy)
        
        total_time = time.time() - start_time
        
        # ä¿å­˜ç»“æœ
        if results or target_laws:
            await save_results(results, target_laws)
            
            # ç»Ÿè®¡ä¿¡æ¯
            success_count = len([r for r in results if r and r.get('success', False)])
            total_count = len(target_laws)
            failed_count = total_count - success_count
            success_rate = success_count / total_count * 100 if total_count > 0 else 0
            avg_time = total_time / total_count if total_count > 0 else 0
            
            print(f"\n=== ğŸ‰ é‡‡é›†å®Œæˆï¼ˆç»ˆæä¼˜åŒ–ç‰ˆï¼‰===")
            print(f"ç›®æ ‡æ³•è§„æ•°: {total_count}")
            print(f"æˆåŠŸé‡‡é›†: {success_count}")
            print(f"æœªæ‰¾åˆ°: {failed_count}")
            print(f"æˆåŠŸç‡: {success_rate:.1f}%")
            print(f"æ€»è€—æ—¶: {total_time:.1f}ç§’")
            print(f"å¹³å‡è€—æ—¶: {avg_time:.2f}ç§’/æ³•è§„")
            
            # æ•ˆç‡å¯¹æ¯”æ˜¾ç¤º
            original_estimated_time = total_count * 24  # åŸç‰ˆä¼°è®¡24ç§’/æ³•è§„
            efficiency_improvement = ((original_estimated_time - total_time) / original_estimated_time) * 100
            print(f"ğŸš€ æ•ˆç‡æå‡: {efficiency_improvement:.1f}% (ç›¸æ¯”åŸç‰ˆé¢„ä¼°)")
            
            # æ˜¾ç¤ºæˆåŠŸé‡‡é›†çš„æ³•è§„æŒ‰ç­–ç•¥åˆ†ç±»
            if success_count > 0:
                print(f"\nâœ… æˆåŠŸé‡‡é›†çš„æ³•è§„ï¼ˆæŒ‰ç­–ç•¥åˆ†ç±»ï¼‰:")
                
                strategy_groups = {}
                for result in results:
                    if result and result.get('success'):
                        strategy = result.get('crawler_strategy', 'unknown')
                        if strategy not in strategy_groups:
                            strategy_groups[strategy] = []
                        strategy_groups[strategy].append(result)
                
                strategy_names = {
                    'search_engine': 'ğŸ¯ æœç´¢å¼•æ“',
                    'search_based': 'ğŸ“š å›½å®¶æ³•å¾‹æ³•è§„æ•°æ®åº“',
                    'optimized_selenium': 'âš¡ ä¼˜åŒ–ç‰ˆSelenium',
                    'selenium_gov': 'ğŸ”§ æ ‡å‡†Selenium',
                    'direct_url': 'ğŸ”— ç›´æ¥URLè®¿é—®'
                }
                
                for strategy, laws in strategy_groups.items():
                    strategy_name = strategy_names.get(strategy, f"ğŸ”§ {strategy}")
                    print(f"  {strategy_name} ({len(laws)}æ¡):")
                    for i, law in enumerate(laws, 1):
                        print(f"    {i}. {law.get('name', law.get('target_name'))} ({law.get('level', 'æœªçŸ¥çº§åˆ«')})")
            
            # æ˜¾ç¤ºæœªæ‰¾åˆ°çš„æ³•è§„
            if failed_count > 0:
                print(f"\nâŒ æœªæ‰¾åˆ°çš„æ³•è§„ ({failed_count}æ¡):")
                successful_names = {r.get('name', r.get('target_name')) for r in results if r and r.get('success')}
                unfound_count = 0
                for target_law in target_laws:
                    if target_law not in successful_names:
                        unfound_count += 1
                        print(f"  - {target_law}")
        else:
            print("âŒ æ²¡æœ‰ç›®æ ‡æ³•è§„ï¼Œä¹Ÿæœªé‡‡é›†åˆ°ä»»ä½•ä¿¡æ¯")
    
    finally:
        # æ¸…ç†èµ„æº
        try:
            await crawler_manager.async_cleanup()
            print("ğŸ§¹ èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            print(f"æ¸…ç†èµ„æºæ—¶å‘ç”Ÿé”™è¯¯: {e}")


async def batch_crawl(limit: int = None, strategy: int = None):
    """æ‰¹é‡çˆ¬å–æ¨¡å¼ï¼ˆåŸæœ‰åŠŸèƒ½ï¼Œä¿æŒå‘åå…¼å®¹ï¼‰"""
    print("=== æ‰¹é‡é‡‡é›†æ¨¡å¼ ===")
    print(f"ç‰ˆæœ¬: {settings.version} | è°ƒè¯•æ¨¡å¼: {'å¼€å¯' if settings.debug else 'å…³é—­'}")
    
    if strategy:
        strategy_names = {
            1: "å›½å®¶æ³•å¾‹æ³•è§„æ•°æ®åº“",
            2: "HTTPæœç´¢å¼•æ“",
            3: "Seleniumæœç´¢å¼•æ“", 
            4: "Seleniumæ”¿åºœç½‘",
            5: "ç›´æ¥URLè®¿é—®"
        }
        print(f"æŒ‡å®šç­–ç•¥: {strategy} - {strategy_names.get(strategy, 'æœªçŸ¥ç­–ç•¥')}")
    else:
        print("æ•°æ®æº: å›½å®¶æ³•å¾‹æ³•è§„æ•°æ®åº“ + ä¸­å›½æ”¿åºœç½‘")
    print()
    
    # è·å–ç›®æ ‡æ³•è§„åˆ—è¡¨
    excel_path = "Background info/law list.xls"
    if os.path.exists(excel_path):
        print(f"ä»Excelæ–‡ä»¶åŠ è½½æ³•è§„åˆ—è¡¨: {excel_path}")
        target_laws = load_target_laws_from_excel(excel_path)
        if not target_laws:
            print("Excelæ–‡ä»¶ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯ï¼Œç¨‹åºé€€å‡º")
            return
    else:
        print("Excelæ–‡ä»¶ä¸å­˜åœ¨ï¼Œç¨‹åºé€€å‡º")
        return
    
    # æ ¹æ®å‚æ•°æˆ–é…ç½®å†³å®šçˆ¬å–æ•°é‡
    if limit is not None:
        crawl_limit = limit
        print(f"æ ¹æ®å‘½ä»¤è¡Œå‚æ•°é™åˆ¶ï¼Œæœ¬æ¬¡ä»…é‡‡é›†å‰ {crawl_limit} æ¡æ³•è§„")
    else:
        crawl_limit = settings.crawler.crawl_limit
        if crawl_limit > 0:
            print(f"æ ¹æ®é…ç½®é™åˆ¶ï¼Œæœ¬æ¬¡ä»…é‡‡é›†å‰ {crawl_limit} æ¡æ³•è§„")
        else:
            print(f"æ— çˆ¬å–æ•°é‡é™åˆ¶ï¼Œå°†é‡‡é›†å…¨éƒ¨ {len(target_laws)} æ¡æ³•è§„")
    
    if crawl_limit > 0:
        target_laws = target_laws[:crawl_limit]
    
    print(f"ç›®æ ‡æ³•è§„æ•°: {len(target_laws)}")
    if target_laws:
        print("å‰5ä¸ªæ³•è§„:", target_laws[:5])
    print()
    
    # åˆ›å»ºé‡‡é›†ç®¡ç†å™¨ï¼ˆåŒæ•°æ®æºï¼‰
    crawler_manager = CrawlerManager()
    
    # æ‰¹é‡é‡‡é›†
    print("å¼€å§‹é‡‡é›†...")
    results = []
    
    for i, law_name in enumerate(target_laws, 1):
        print(f"[{i}/{len(target_laws)}] å¤„ç†: {law_name}")
        
        result = await crawler_manager.crawl_law(law_name, strategy=strategy)
        if result and result.get('success', False):
            # ç¡®ä¿åŒ…å«ç›®æ ‡æ³•è§„åç§°
            result['target_name'] = law_name
            results.append(result)
            print(f"  âœ… æˆåŠŸ - æ¥æº: {result.get('source', 'unknown')}")
        else:
            print(f"  âŒ æœªæ‰¾åˆ°")
    
    # ä¿å­˜ç»“æœ
    if results or target_laws: # å³ä½¿æ²¡æœ‰ç»“æœä¹Ÿè¦ä¿å­˜
        await save_results(results, target_laws)
        
        # ç»Ÿè®¡ä¿¡æ¯
        success_count = len(results)
        total_count = len(target_laws)
        failed_count = total_count - success_count
        success_rate = success_count / total_count * 100 if total_count > 0 else 0
        
        print(f"\n=== é‡‡é›†å®Œæˆ ===")
        print(f"ç›®æ ‡æ³•è§„æ•°: {total_count}")
        print(f"æˆåŠŸé‡‡é›†: {success_count}")
        print(f"æœªæ‰¾åˆ°: {failed_count}")
        print(f"æˆåŠŸç‡: {success_rate:.1f}%")
        
        # æ˜¾ç¤ºæˆåŠŸé‡‡é›†çš„æ³•è§„æŒ‰æ•°æ®æºåˆ†ç±»
        if success_count > 0:
            print(f"\nâœ… æˆåŠŸé‡‡é›†çš„æ³•è§„ï¼ˆæŒ‰æ•°æ®æºåˆ†ç±»ï¼‰:")
            
            # æŒ‰æ•°æ®æºåˆ†ç»„
            source_groups = {}
            for law in results:
                source = law.get('source', 'unknown')
                if source not in source_groups:
                    source_groups[source] = []
                source_groups[source].append(law)
            
            for source, laws in source_groups.items():
                if source == "search_api":
                    source_name = "å›½å®¶æ³•å¾‹æ³•è§„æ•°æ®åº“"
                elif source == "selenium_gov_web":
                    source_name = "ä¸­å›½æ”¿åºœç½‘(www.gov.cn)"
                elif source == "gov_web":
                    source_name = "ä¸­å›½æ”¿åºœç½‘"
                else:
                    source_name = source
                print(f"  ğŸ“š {source_name} ({len(laws)}æ¡):")
                for i, law in enumerate(laws, 1):
                    print(f"    {i}. {law.get('name', law.get('target_name'))} ({law.get('level', 'æœªçŸ¥çº§åˆ«')})")
        
        # æ˜¾ç¤ºæœªæ‰¾åˆ°çš„æ³•è§„
        if failed_count > 0:
            print(f"\nâŒ æœªæ‰¾åˆ°çš„æ³•è§„:")
            results_map = {law['target_name']: law for law in results}
            unfound_count = 0
            for target_law in target_laws:
                if target_law not in results_map:
                    unfound_count += 1
                    print(f"  - {target_law}")
    else:
        print("âŒ æ²¡æœ‰ç›®æ ‡æ³•è§„ï¼Œä¹Ÿæœªé‡‡é›†åˆ°ä»»ä½•ä¿¡æ¯")
    
    # æ¸…ç†èµ„æº
    try:
        await crawler_manager.async_cleanup()
    except Exception as e:
        print(f"æ¸…ç†èµ„æºæ—¶å‘ç”Ÿé”™è¯¯: {e}")


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="æ³•å¾‹æ³•è§„é‡‡é›†ç³»ç»Ÿ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python main.py                           # æ‰¹é‡çˆ¬å–ï¼ˆç»ˆæä¼˜åŒ–ç‰ˆï¼‰
  python main.py --limit 10                # æ‰¹é‡çˆ¬å–å‰10æ¡ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
  python main.py --legacy                  # ä½¿ç”¨åŸç‰ˆæ‰¹é‡çˆ¬å–
  python main.py --law "ç”µå­æ‹›æ ‡æŠ•æ ‡åŠæ³•"    # å•ç‹¬æœç´¢æŒ‡å®šæ³•è§„
  python main.py --law "ä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸" -v  # è¯¦ç»†æ¨¡å¼
  
ç­–ç•¥é€‰æ‹©ç¤ºä¾‹:
  python main.py --strategy 1              # ä»…ä½¿ç”¨å›½å®¶æ³•å¾‹æ³•è§„æ•°æ®åº“
  python main.py --strategy 2 --limit 10   # ä»…ä½¿ç”¨HTTPæœç´¢å¼•æ“
  python main.py --strategy 3              # ä»…ä½¿ç”¨Seleniumæœç´¢å¼•æ“
  python main.py --strategy 4              # ä»…ä½¿ç”¨Seleniumæ”¿åºœç½‘
  python main.py --strategy 5              # ä»…ä½¿ç”¨ç›´æ¥URLè®¿é—®
        """
    )
    
    parser.add_argument(
        '--law', '-l',
        type=str,
        help='æŒ‡å®šè¦æœç´¢çš„å•ä¸ªæ³•è§„åç§°'
    )
    
    parser.add_argument(
        '--limit',
        type=int,
        help='é™åˆ¶æ‰¹é‡çˆ¬å–çš„æ•°é‡ï¼Œè¦†ç›–é…ç½®æ–‡ä»¶è®¾ç½®'
    )
    
    parser.add_argument(
        '--legacy',
        action='store_true',
        help='ä½¿ç”¨åŸç‰ˆæ‰¹é‡çˆ¬å–æ¨¡å¼ï¼ˆé€ä¸ªå¤„ç†ï¼‰'
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='è¯¦ç»†æ¨¡å¼ï¼Œæ˜¾ç¤ºæ›´å¤šä¿¡æ¯'
    )
    
    parser.add_argument(
        '--strategy', '-s',
        type=int,
        choices=[1, 2, 3, 4, 5],
        help='''æŒ‡å®šçˆ¬è™«ç­–ç•¥ï¼ˆå•ä¸€ç­–ç•¥æ¨¡å¼ï¼‰:
        1 - å›½å®¶æ³•å¾‹æ³•è§„æ•°æ®åº“ï¼ˆæƒå¨æ•°æ®æºï¼‰
        2 - HTTPæœç´¢å¼•æ“ï¼ˆå¿«é€Ÿæœç´¢ï¼Œå…ˆç›´è¿åä»£ç†ï¼‰
        3 - Seleniumæœç´¢å¼•æ“ï¼ˆæµè§ˆå™¨æœç´¢å¼•æ“ï¼‰
        4 - Seleniumæ”¿åºœç½‘ï¼ˆé’ˆå¯¹æ”¿åºœç½‘ä¼˜åŒ–ï¼‰
        5 - ç›´æ¥URLè®¿é—®ï¼ˆæœ€åä¿éšœï¼‰
        ä¸æŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤å¤šå±‚ç­–ç•¥'''
    )
    
    return parser.parse_args()


async def main():
    """ä¸»å‡½æ•° - æ ¹æ®å‚æ•°é€‰æ‹©è¿è¡Œæ¨¡å¼"""
    args = parse_args()
    
    if args.law:
        # å•æ³•è§„æœç´¢æ¨¡å¼
        await search_single_law(args.law, args.verbose, args.strategy)
    else:
        # æ‰¹é‡çˆ¬å–æ¨¡å¼
        if args.legacy:
            # ä½¿ç”¨åŸç‰ˆæ‰¹é‡çˆ¬å–æ¨¡å¼
            await batch_crawl(args.limit, args.strategy)
        else:
            # ä½¿ç”¨ç»ˆæä¼˜åŒ–ç‰ˆæ‰¹é‡çˆ¬å–æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
            await batch_crawl_optimized(args.limit, args.strategy)


if __name__ == "__main__":
    # ç¡®ä¿æ—¥å¿—å’Œæ•°æ®ç›®å½•å­˜åœ¨
    os.makedirs("logs", exist_ok=True)
    os.makedirs("data", exist_ok=True)
    
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        import traceback
        print(f"\nç¨‹åºå‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        if settings.debug:
            print("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            traceback.print_exc()
    finally:
        # ç¡®ä¿æ¸…ç†æ‰€æœ‰WebDriverå®ä¾‹
        try:
            from src.crawler.utils.webdriver_manager import cleanup_webdrivers
            asyncio.run(cleanup_webdrivers())
            print("ğŸ§¹ WebDriveræ¸…ç†å®Œæˆ")
        except Exception as e:
            print(f"æ¸…ç†WebDriveræ—¶å‘ç”Ÿé”™è¯¯: {e}") 